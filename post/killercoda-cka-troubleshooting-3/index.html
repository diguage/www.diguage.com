<!doctype html><html class=no-js lang=zh-cn><head><meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=X-UA-Compatible content="IE=edge"><title>killercoda CKA：Troubleshooting - 3 - "地瓜哥"博客网</title>
<script>(function(e,t){e[t]=e[t].replace("no-js","js")})(document.documentElement,"className")</script><meta name=keywords content="Kubernetes,CKA,Kubernetes,Linux,系统架构"><meta name=description content><meta property="og:url" content="https://www.diguage.com/post/killercoda-cka-troubleshooting-3/"><meta property="og:site_name" content='"地瓜哥"博客网'><meta property="og:title" content="killercoda CKA：Troubleshooting - 3"><meta property="og:description" content=' 1. Troubleshooting - Service account, role, role binding Issue Troubleshooting - Service account, role, role binding Issue
You have a service account named dev-sa, a Role named dev-role-cka, and a RoleBinding named dev-role-binding-cka. we are trying to create list and get the pods and services. However, using dev-sa service account is not able to perform these operations. fix this issue.
# @author D瓜哥 · https://www.diguage.com $ kubectl get serviceaccounts dev-sa -o yaml apiVersion: v1 kind: ServiceAccount metadata: creationTimestamp: "2025-01-22T09:48:06Z" name: dev-sa namespace: default resourceVersion: "2270" uid: 48b68f34-8c19-4477-9631-4f368f6ecc66 $ kubectl get role dev-role-cka NAME CREATED AT dev-role-cka 2025-01-22T09:48:06Z $ kubectl get role dev-role-cka -o yaml apiVersion: rbac.authorization.k8s.io/v1 kind: Role metadata: creationTimestamp: "2025-01-22T09:48:06Z" name: dev-role-cka namespace: default resourceVersion: "2271" uid: 7a011481-8edd-4417-a1b8-8d15290d3e9f rules: - apiGroups: - "" resources: - secrets verbs: - get $ kubectl get rolebindings dev-role-binding-cka -o yaml apiVersion: rbac.authorization.k8s.io/v1 kind: RoleBinding metadata: creationTimestamp: "2025-01-22T09:48:07Z" name: dev-role-binding-cka namespace: default resourceVersion: "2272" uid: 888af489-86b6-4d38-a723-a8ff13656d2b roleRef: apiGroup: rbac.authorization.k8s.io kind: Role name: dev-role-cka subjects: - kind: ServiceAccount name: dev-sa namespace: default # 将 Role 删掉，重建即可 $ kubectl delete role dev-role-cka --force --grace-period 0 Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely. role.rbac.authorization.k8s.io "dev-role-cka" force deleted $ kubectl create role dev-role-cka --resource=pods,services --verb=create,list,get role.rbac.authorization.k8s.io/dev-role-cka created $ kubectl get role dev-role-cka -o yaml apiVersion: rbac.authorization.k8s.io/v1 kind: Role metadata: creationTimestamp: "2025-01-22T09:49:46Z" name: dev-role-cka namespace: default resourceVersion: "2414" uid: b3d7fc62-f029-4f4b-88a5-99ee9840af05 rules: - apiGroups: - "" resources: - pods - services verbs: - create - list - get '><meta property="og:locale" content="zh_cn"><meta property="og:type" content="article"><meta property="article:section" content="post"><meta property="article:published_time" content="2024-12-26T19:54:28+08:00"><meta property="article:modified_time" content="2025-02-12T22:16:29+08:00"><meta property="article:tag" content="Kubernetes"><meta property="article:tag" content="Linux"><meta name=twitter:card content="summary"><meta name=twitter:title content="killercoda CKA：Troubleshooting - 3"><meta name=twitter:description content=' 1. Troubleshooting - Service account, role, role binding Issue Troubleshooting - Service account, role, role binding Issue
You have a service account named dev-sa, a Role named dev-role-cka, and a RoleBinding named dev-role-binding-cka. we are trying to create list and get the pods and services. However, using dev-sa service account is not able to perform these operations. fix this issue.
# @author D瓜哥 · https://www.diguage.com $ kubectl get serviceaccounts dev-sa -o yaml apiVersion: v1 kind: ServiceAccount metadata: creationTimestamp: "2025-01-22T09:48:06Z" name: dev-sa namespace: default resourceVersion: "2270" uid: 48b68f34-8c19-4477-9631-4f368f6ecc66 $ kubectl get role dev-role-cka NAME CREATED AT dev-role-cka 2025-01-22T09:48:06Z $ kubectl get role dev-role-cka -o yaml apiVersion: rbac.authorization.k8s.io/v1 kind: Role metadata: creationTimestamp: "2025-01-22T09:48:06Z" name: dev-role-cka namespace: default resourceVersion: "2271" uid: 7a011481-8edd-4417-a1b8-8d15290d3e9f rules: - apiGroups: - "" resources: - secrets verbs: - get $ kubectl get rolebindings dev-role-binding-cka -o yaml apiVersion: rbac.authorization.k8s.io/v1 kind: RoleBinding metadata: creationTimestamp: "2025-01-22T09:48:07Z" name: dev-role-binding-cka namespace: default resourceVersion: "2272" uid: 888af489-86b6-4d38-a723-a8ff13656d2b roleRef: apiGroup: rbac.authorization.k8s.io kind: Role name: dev-role-cka subjects: - kind: ServiceAccount name: dev-sa namespace: default # 将 Role 删掉，重建即可 $ kubectl delete role dev-role-cka --force --grace-period 0 Warning: Immediate deletion does not wait for confirmation that the running resource has been terminated. The resource may continue to run on the cluster indefinitely. role.rbac.authorization.k8s.io "dev-role-cka" force deleted $ kubectl create role dev-role-cka --resource=pods,services --verb=create,list,get role.rbac.authorization.k8s.io/dev-role-cka created $ kubectl get role dev-role-cka -o yaml apiVersion: rbac.authorization.k8s.io/v1 kind: Role metadata: creationTimestamp: "2025-01-22T09:49:46Z" name: dev-role-cka namespace: default resourceVersion: "2414" uid: b3d7fc62-f029-4f4b-88a5-99ee9840af05 rules: - apiGroups: - "" resources: - pods - services verbs: - create - list - get '><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link rel=dns-prefetch href=//fonts.googleapis.com><link rel=dns-prefetch href=//fonts.gstatic.com><link rel=stylesheet href="https://fonts.googleapis.com/css?family=Open+Sans:400,400i,700"><link rel=stylesheet href=/css/style.min.css><link rel=stylesheet href=/css/asciidoctor.css><link rel=stylesheet href=/css/rouge-monokai.css><link rel="shortcut icon" href=/favicon.ico><script async src="https://www.googletagmanager.com/gtag/js?id=G-1MMT2NLEL4"></script><script>var dnt,doNotTrack=!1;if(!1&&(dnt=navigator.doNotTrack||window.doNotTrack||navigator.msDoNotTrack,doNotTrack=dnt=="1"||dnt=="yes"),!doNotTrack){window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-1MMT2NLEL4")}</script><script id=baidu_analytics>var _hmt=_hmt||[];(function(){var e,t=document.createElement("script");t.src="https://hm.baidu.com/hm.js?e56e7dd0a120b414f5741f4c5e5218ea",e=document.getElementsByTagName("script")[0],e.parentNode.insertBefore(t,e)})()</script><link rel=stylesheet href=https://cdn.jsdelivr.net/npm/font-awesome@4.7.0/css/font-awesome.min.css></head><body class=body><div class="container container--outer"><header class=header><div class="container header__container"><div class=logo><a class=logo__link href=/ title='"地瓜哥"博客网' rel=home><div class="logo__item logo__text"><div class=logo__title>"地瓜哥"博客网</div><div class=logo__tagline>分享技术带来的喜悦 — https://www.diguage.com/</div></div></a></div><nav class=menu><button class=menu__btn aria-haspopup=true aria-expanded=false tabindex=0>
<span class=menu__btn-title tabindex=-1>菜单</span></button><ul class=menu__list><li class=menu__item><a class=menu__link href=/><span class=menu__text>首页</span></a></li><li class=menu__item><a class=menu__link href=/categories/><span class=menu__text>分类</span></a></li><li class=menu__item><a class=menu__link href=/about/><span class=menu__text>关于</span></a></li><li class=menu__item><a class=menu__link href=/archives/><span class=menu__text>归档</span></a></li></ul></nav></div></header><div class="wrapper flex"><div class=primary><main class=main role=main><article class=post><header class=post__header><h1 class=post__title>killercoda CKA：Troubleshooting - 3</h1><div class="post__meta meta"><div class="meta__item-author meta__item"><svg class="meta__icon icon icon-author" width="16" height="16" viewBox="0 0 12 16"><path d="M6 1c2.2.0 3.5 2 3.5 4.5C9.5 7 8.9 8.2 8 9c2.9.8 4 2.5 4 5v1H0v-1c0-2.5 1.1-4.2 4-5-.9-.8-1.5-2-1.5-3.5C2.5 3 3.8 1 6 1z"/></svg><span class=meta__text>D瓜哥</span></div><div class="meta__item-datetime meta__item"><svg class="meta__icon icon icon-time" width="16" height="14" viewBox="0 0 30 28"><path d="M15 0C7 0 1 6 1 14s6 14 14 14 14-6 14-14S23 0 15 0zm0 25C9 25 4 20 4 14S9 3 15 3s11 5 11 11-5 11-11 11zm1-18h-2v8.4l6.8 4.4L22 18l-6-3.8V7z"/></svg><time class=meta__text datetime=2024-12-26T19:54:28+08:00>2024-12-26</time></div><div class="meta__item-categories meta__item"><svg class="meta__icon icon icon-category" width="16" height="16" viewBox="0 0 16 16"><path d="m7 2 1 2h8v11H0V2z"/></svg><span class=meta__text><a class=meta__link href=/categories/%E7%B3%BB%E7%BB%9F%E6%9E%B6%E6%9E%84/ rel=category>系统架构</a></span></div></div></header><figure class="post__thumbnail thumbnail"><img class=thumbnail__image src=/images/logos/kubernetes.svg alt="killercoda CKA：Troubleshooting - 3"></figure><div class="content post__content clearfix"><div class=sect1><h2 id=_troubleshooting_service_account_role_role_binding_issue>1. Troubleshooting - Service account, role, role binding Issue</h2><div class=sectionbody><div class=paragraph><p><a href=https://killercoda.com/sachin/course/CKA/sa-cr-crb-issue target=_blank rel=noopener>Troubleshooting - Service account, role, role binding Issue</a></p></div><div class=sidebarblock><div class=content><div class=paragraph><p>You have a service account named <code>dev-sa</code>, a Role named <code>dev-role-cka</code>, and a RoleBinding named <code>dev-role-binding-cka</code>. we are trying to <code>create</code> <code>list</code> and <code>get</code> the <code>pods</code> and <code>services</code>. However, using <code>dev-sa</code> service account is not able to perform these operations. fix this issue.</p></div></div></div><div class=listingblock><div class=content><pre class="rouge highlight nowrap"><code data-lang=bash><span class=c># @author D瓜哥 · <a href=https://www.diguage.com target=_blank>https://www.diguage.com</a></span>

<span class=nv>$ </span>kubectl get serviceaccounts dev-sa <span class=nt>-o</span> yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  creationTimestamp: <span class=s2>&#34;2025-01-22T09:48:06Z&#34;</span>
  name: dev-sa
  namespace: default
  resourceVersion: <span class=s2>&#34;2270&#34;</span>
  uid: 48b68f34-8c19-4477-9631-4f368f6ecc66

<span class=nv>$ </span>kubectl get  role dev-role-cka
NAME           CREATED AT
dev-role-cka   2025-01-22T09:48:06Z

<span class=nv>$ </span>kubectl get  role dev-role-cka <span class=nt>-o</span> yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  creationTimestamp: <span class=s2>&#34;2025-01-22T09:48:06Z&#34;</span>
  name: dev-role-cka
  namespace: default
  resourceVersion: <span class=s2>&#34;2271&#34;</span>
  uid: 7a011481-8edd-4417-a1b8-8d15290d3e9f
rules:
- apiGroups:
  - <span class=s2>&#34;&#34;</span>
  resources:
  - secrets
  verbs:
  - get

<span class=nv>$ </span>kubectl get rolebindings dev-role-binding-cka <span class=nt>-o</span> yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  creationTimestamp: <span class=s2>&#34;2025-01-22T09:48:07Z&#34;</span>
  name: dev-role-binding-cka
  namespace: default
  resourceVersion: <span class=s2>&#34;2272&#34;</span>
  uid: 888af489-86b6-4d38-a723-a8ff13656d2b
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: dev-role-cka
subjects:
- kind: ServiceAccount
  name: dev-sa
  namespace: default

<span class=c># 将 Role 删掉，重建即可</span>
<span class=nv>$ </span>kubectl delete role dev-role-cka <span class=nt>--force</span> <span class=nt>--grace-period</span> 0
Warning: Immediate deletion does not <span class=nb>wait </span><span class=k>for </span>confirmation that the running resource has been terminated. The resource may <span class=k>continue </span>to run on the cluster indefinitely.
role.rbac.authorization.k8s.io <span class=s2>&#34;dev-role-cka&#34;</span> force deleted

<span class=nv>$ </span>kubectl create  role dev-role-cka <span class=nt>--resource</span><span class=o>=</span>pods,services <span class=nt>--verb</span><span class=o>=</span>create,list,get
role.rbac.authorization.k8s.io/dev-role-cka created

<span class=nv>$ </span>kubectl get role dev-role-cka <span class=nt>-o</span> yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  creationTimestamp: <span class=s2>&#34;2025-01-22T09:49:46Z&#34;</span>
  name: dev-role-cka
  namespace: default
  resourceVersion: <span class=s2>&#34;2414&#34;</span>
  uid: b3d7fc62-f029-4f4b-88a5-99ee9840af05
rules:
- apiGroups:
  - <span class=s2>&#34;&#34;</span>
  resources:
  - pods
  - services
  verbs:
  - create
  - list
  - get</code></pre></div></div></div></div><div class=sect1><h2 id=_troubleshooting_service_account_role_role_binding_issue_2>2. Troubleshooting - Service account, role, role binding Issue</h2><div class=sectionbody><div class=paragraph><p><a href=https://killercoda.com/sachin/course/CKA/sa-cr-crb-issue-1 target=_blank rel=noopener>Troubleshooting - Service account, role, role binding Issue</a></p></div><div class=sidebarblock><div class=content><div class=paragraph><p>You have a service account named <code>prod-sa</code>, a Role named <code>prod-role-cka</code>, and a RoleBinding named <code>prod-role-binding-cka</code>. we are trying to <code>create</code> <code>list</code> and <code>get</code> the <code>services</code> . However, using <code>prod-sa</code> service account is not able to perform these operations. fix this issue.</p></div></div></div><div class=listingblock><div class=content><pre class="rouge highlight nowrap"><code data-lang=bash><span class=c># @author D瓜哥 · <a href=https://www.diguage.com target=_blank>https://www.diguage.com</a></span>

<span class=nv>$ </span>kubectl get serviceaccounts prod-sa <span class=nt>-o</span> yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  creationTimestamp: <span class=s2>&#34;2025-01-22T09:51:21Z&#34;</span>
  name: prod-sa
  namespace: default
  resourceVersion: <span class=s2>&#34;2069&#34;</span>
  uid: 0a915925-11ef-4530-bf11-78b874d0f4d3

<span class=nv>$ </span>kubectl get rolebindings prod-role-binding-cka <span class=nt>-o</span> yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  creationTimestamp: <span class=s2>&#34;2025-01-22T09:51:21Z&#34;</span>
  name: prod-role-binding-cka
  namespace: default
  resourceVersion: <span class=s2>&#34;2071&#34;</span>
  uid: 8502ca87-2511-4c0c-b275-6d21c5f470bf
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: prod-role-cka
subjects:
- kind: ServiceAccount
  name: prod-sa
  namespace: default

<span class=nv>$ </span>kubectl get role prod-role-cka <span class=nt>-o</span> yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  creationTimestamp: <span class=s2>&#34;2025-01-22T09:51:21Z&#34;</span>
  name: prod-role-cka
  namespace: default
  resourceVersion: <span class=s2>&#34;2070&#34;</span>
  uid: 2a5b77cd-81f0-41c7-b2f2-2d2961377e2f
rules:
- apiGroups:
  - <span class=s2>&#34;&#34;</span>
  resources:
  - pods
  verbs:
  - list

<span class=c># 删掉 Role，重建即可</span>
<span class=nv>$ </span>kubectl delete role prod-role-cka <span class=nt>--force</span> <span class=nt>--grace-period</span> 0
Warning: Immediate deletion does not <span class=nb>wait </span><span class=k>for </span>confirmation that the running resource has been terminated. The resource may <span class=k>continue </span>to run on the cluster indefinitely.
role.rbac.authorization.k8s.io <span class=s2>&#34;prod-role-cka&#34;</span> force deleted

<span class=nv>$ </span>kubectl create role prod-role-cka <span class=nt>--resource</span><span class=o>=</span>services <span class=nt>--verb</span><span class=o>=</span>create,list,get
role.rbac.authorization.k8s.io/prod-role-cka created

<span class=nv>$ </span>kubectl get role prod-role-cka <span class=nt>-o</span> yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  creationTimestamp: <span class=s2>&#34;2025-01-22T09:54:17Z&#34;</span>
  name: prod-role-cka
  namespace: default
  resourceVersion: <span class=s2>&#34;2319&#34;</span>
  uid: fdfe51b9-b31b-4f1d-ac01-bd4724bb5adf
rules:
- apiGroups:
  - <span class=s2>&#34;&#34;</span>
  resources:
  - services
  verbs:
  - create
  - list
  - get</code></pre></div></div></div></div><div class=sect1><h2 id=_troubleshooting_network_policy_issue>3. Troubleshooting - Network Policy Issue</h2><div class=sectionbody><div class=paragraph><p><a href=https://killercoda.com/sachin/course/CKA/network-policy-issue target=_blank rel=noopener>Troubleshooting - Network Policy Issue</a></p></div><div class=sidebarblock><div class=content><div class=paragraph><p><code>red-pod</code>, <code>green-pod</code>, <code>blue-pod</code> pods are running, and <code>red-pod</code> exposed within the cluster using <code>red-service</code> service. and network policy applied on <code>red-pod</code> pod. problem is now the pod <code>red-pod</code> is accessible from both <code>green-pod</code> and <code>blue-pod</code> pods. fix the issue that <code>green-pod</code> only can able access <code>red-pod</code> pod.</p></div></div></div><div class=listingblock><div class=content><pre class="rouge highlight nowrap"><code data-lang=bash><span class=c># @author D瓜哥 · <a href=https://www.diguage.com target=_blank>https://www.diguage.com</a></span>

<span class=nv>$ </span>kubectl get pod <span class=nt>--show-labels</span>
NAME        READY   STATUS    RESTARTS   AGE   LABELS
blue-pod    1/1     Running   0          76s   <span class=nv>run</span><span class=o>=</span>blue-pod
green-pod   1/1     Running   0          76s   <span class=nv>run</span><span class=o>=</span>green-pod
red-pod     1/1     Running   0          76s   <span class=nv>run</span><span class=o>=</span>red-pod

<span class=nv>$ </span>kubectl get service <span class=nt>-o</span> wide
NAME          TYPE        CLUSTER-IP    EXTERNAL-IP   PORT<span class=o>(</span>S<span class=o>)</span>   AGE   SELECTOR
kubernetes    ClusterIP   10.96.0.1     &lt;none&gt;        443/TCP   20d   &lt;none&gt;
red-service   ClusterIP   10.97.8.239   &lt;none&gt;        80/TCP    89s   <span class=nv>run</span><span class=o>=</span>red-pod

<span class=nv>$ </span>kubectl get networkpolicies
NAME                   POD-SELECTOR   AGE
allow-green-and-blue   <span class=nv>run</span><span class=o>=</span>red-pod    116s

<span class=nv>$ </span>kubectl get networkpolicies allow-green-and-blue <span class=nt>-o</span> yaml | <span class=nb>tee </span>np.yaml
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  annotations:
    kubectl.kubernetes.io/last-applied-configuration: |
      <span class=o>{</span><span class=s2>&#34;apiVersion&#34;</span>:<span class=s2>&#34;networking.k8s.io/v1&#34;</span>,<span class=s2>&#34;kind&#34;</span>:<span class=s2>&#34;NetworkPolicy&#34;</span>,<span class=s2>&#34;metadata&#34;</span>:<span class=o>{</span><span class=s2>&#34;annotations&#34;</span>:<span class=o>{}</span>,<span class=s2>&#34;name&#34;</span>:<span class=s2>&#34;allow-green-and-blue&#34;</span>,<span class=s2>&#34;namespace&#34;</span>:<span class=s2>&#34;default&#34;</span><span class=o>}</span>,<span class=s2>&#34;spec&#34;</span>:<span class=o>{</span><span class=s2>&#34;ingress&#34;</span>:[<span class=o>{</span><span class=s2>&#34;from&#34;</span>:[<span class=o>{</span><span class=s2>&#34;podSelector&#34;</span>:<span class=o>{</span><span class=s2>&#34;matchLabels&#34;</span>:<span class=o>{</span><span class=s2>&#34;run&#34;</span>:<span class=s2>&#34;green-pod&#34;</span><span class=o>}}}</span>,<span class=o>{</span><span class=s2>&#34;podSelector&#34;</span>:<span class=o>{</span><span class=s2>&#34;matchLabels&#34;</span>:<span class=o>{</span><span class=s2>&#34;run&#34;</span>:<span class=s2>&#34;blue-pod&#34;</span><span class=o>}}}]}]</span>,<span class=s2>&#34;podSelector&#34;</span>:<span class=o>{</span><span class=s2>&#34;matchLabels&#34;</span>:<span class=o>{</span><span class=s2>&#34;run&#34;</span>:<span class=s2>&#34;red-pod&#34;</span><span class=o>}}</span>,<span class=s2>&#34;policyTypes&#34;</span>:[<span class=s2>&#34;Ingress&#34;</span><span class=o>]}}</span>
  creationTimestamp: <span class=s2>&#34;2025-01-22T09:55:14Z&#34;</span>
  generation: 1
  name: allow-green-and-blue
  namespace: default
  resourceVersion: <span class=s2>&#34;2012&#34;</span>
  uid: 7b8ffc9d-c994-47dc-8cee-4f19e6e8edc6
spec:
  ingress:
  - from:
    - podSelector:
        matchLabels:
          run: green-pod
    - podSelector:
        matchLabels:
          run: blue-pod
  podSelector:
    matchLabels:
      run: red-pod
  policyTypes:
  - Ingress

<span class=nv>$ </span>vim np.yaml
<span class=c># 把 blue-pod 的 podSelector 过滤器删掉即可。</span>

<span class=nv>$ </span>kubectl delete <span class=nt>-f</span> np.yaml <span class=nt>--force</span> <span class=nt>--grace-period</span> 0
Warning: Immediate deletion does not <span class=nb>wait </span><span class=k>for </span>confirmation that the running resource has been terminated. The resource may <span class=k>continue </span>to run on the cluster indefinitely.
networkpolicy.networking.k8s.io <span class=s2>&#34;allow-green-and-blue&#34;</span> force deleted

<span class=nv>$ </span>kubectl apply <span class=nt>-f</span> np.yaml
networkpolicy.networking.k8s.io/allow-green-and-blue created

<span class=nv>$ </span><span class=nb>cat </span>np.yaml
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: allow-green-and-blue
  namespace: default
spec:
  ingress:
  - from:
    - podSelector:
        matchLabels:
          run: green-pod
  podSelector:
    matchLabels:
      run: red-pod
  policyTypes:
  - Ingress</code></pre></div></div></div></div><div class=sect1><h2 id=_troubleshooting_kubectl_config_issue>4. Troubleshooting - Kubectl - Config Issue</h2><div class=sectionbody><div class=paragraph><p><a href=https://killercoda.com/sachin/course/CKA/kubectl-issue target=_blank rel=noopener>Troubleshooting - Kubectl - Config Issue</a></p></div><div class=sidebarblock><div class=content><div class=paragraph><p>some issue on the <code>controlplane</code> unable to run kubectl commands (EX: <code>kubectl get node</code>)</p></div><div class=paragraph><p>kubernetes configuration file available at <code>.kube/config</code></p></div></div></div><div class=listingblock><div class=content><pre class="rouge highlight nowrap"><code data-lang=bash><span class=c># @author D瓜哥 · <a href=https://www.diguage.com target=_blank>https://www.diguage.com</a></span>

<span class=nv>$ </span>kubectl get node
E0122 10:10:11.737969    5576 memcache.go:265] <span class=s2>&#34;Unhandled Error&#34;</span> <span class=nv>err</span><span class=o>=</span><span class=s2>&#34;couldn&#39;t get current server API group list: Get </span><span class=se>\&#34;</span><span class=s2>https://172.30.1.2:644333/api?timeout=32s</span><span class=se>\&#34;</span><span class=s2>: dial tcp: address 644333: invalid port&#34;</span>
E0122 10:10:11.739752    5576 memcache.go:265] <span class=s2>&#34;Unhandled Error&#34;</span> <span class=nv>err</span><span class=o>=</span><span class=s2>&#34;couldn&#39;t get current server API group list: Get </span><span class=se>\&#34;</span><span class=s2>https://172.30.1.2:644333/api?timeout=32s</span><span class=se>\&#34;</span><span class=s2>: dial tcp: address 644333: invalid port&#34;</span>
E0122 10:10:11.741197    5576 memcache.go:265] <span class=s2>&#34;Unhandled Error&#34;</span> <span class=nv>err</span><span class=o>=</span><span class=s2>&#34;couldn&#39;t get current server API group list: Get </span><span class=se>\&#34;</span><span class=s2>https://172.30.1.2:644333/api?timeout=32s</span><span class=se>\&#34;</span><span class=s2>: dial tcp: address 644333: invalid port&#34;</span>
E0122 10:10:11.743244    5576 memcache.go:265] <span class=s2>&#34;Unhandled Error&#34;</span> <span class=nv>err</span><span class=o>=</span><span class=s2>&#34;couldn&#39;t get current server API group list: Get </span><span class=se>\&#34;</span><span class=s2>https://172.30.1.2:644333/api?timeout=32s</span><span class=se>\&#34;</span><span class=s2>: dial tcp: address 644333: invalid port&#34;</span>
E0122 10:10:11.744923    5576 memcache.go:265] <span class=s2>&#34;Unhandled Error&#34;</span> <span class=nv>err</span><span class=o>=</span><span class=s2>&#34;couldn&#39;t get current server API group list: Get </span><span class=se>\&#34;</span><span class=s2>https://172.30.1.2:644333/api?timeout=32s</span><span class=se>\&#34;</span><span class=s2>: dial tcp: address 644333: invalid port&#34;</span>
Unable to connect to the server: dial tcp: address 644333: invalid port

<span class=nv>$ </span><span class=nb>cat</span> ~/.kube/config
apiVersion: v1
clusters:
- cluster:
    certificate-authority-data: LS0tLS1C...
    server: https://172.30.1.2:644333
  name: kubernetes
contexts:
- context:
    cluster: kubernetes
    user: kubernetes-admin
  name: kubernetes-admin@kubernetes
current-context: kubernetes-admin@kubernetes
kind: Config
preferences: <span class=o>{}</span>
<span class=nb>users</span>:
- name: kubernetes-admin
  user:
    client-certificate-data: LS0tLS1C...
    client-key-data: LS0tLS1C...
<span class=c># 省略证书内容</span>

<span class=nv>$ </span>vim ~/.kube/config
<span class=c># 端口号错误，将端口号改为 6443 即可</span>

<span class=nv>$ </span><span class=nb>cat</span> ~/.kube/config
apiVersion: v1
clusters:
- cluster:
    certificate-authority-data: LS0tLS1C...
    server: https://172.30.1.2:6443
  name: kubernetes
contexts:
- context:
    cluster: kubernetes
    user: kubernetes-admin
  name: kubernetes-admin@kubernetes
current-context: kubernetes-admin@kubernetes
kind: Config
preferences: <span class=o>{}</span>
<span class=nb>users</span>:
- name: kubernetes-admin
  user:
    client-certificate-data: LS0tLS1C...
    client-key-data: LS0tLS1C...

<span class=nv>$ </span>kubectl get nod
error: the server doesn<span class=s1>&#39;t have a resource type &#34;nod&#34;

$ kubectl get node
NAME           STATUS   ROLES           AGE   VERSION
controlplane   Ready    control-plane   20d   v1.31.0
node01         Ready    &lt;none&gt;          20d   v1.31.0</span></code></pre></div></div></div></div><div class=sect1><h2 id=_troubleshooting_kubectl_port_issue>5. Troubleshooting - Kubectl - Port Issue</h2><div class=sectionbody><div class=paragraph><p><a href=https://killercoda.com/sachin/course/CKA/node-port-issue target=_blank rel=noopener>Troubleshooting - Kubectl - Port Issue</a></p></div><div class=sidebarblock><div class=content><div class=paragraph><p>when you run <code>kubectl get nodes</code> OR <code>kubectl get pod -A</code> threw: <code>- The connection to the server 172.30.1.2:6443 was refused - did you specify the right host or port?</code></p></div><div class=ulist><ul><li><p>need to wait for few seconds to make above command work again but above error will come again after few second</p></li><li><p>and also <code>kube-controller-manager-controlplane</code> pod continuously restarting Fix above issue</p></li></ul></div><div class=paragraph><p>Expectation: <code>kube-apiserver-controlplane</code> pods running in <code>kube-system</code> namespace</p></div><div class=paragraph><p>You can <code>ssh controlplane</code></p></div><div class=paragraph><p>Note: after debugged wait for sometime, pods will come up</p></div></div></div><div class=listingblock><div class=content><pre class="rouge highlight nowrap"><code data-lang=bash><span class=c># @author D瓜哥 · <a href=https://www.diguage.com target=_blank>https://www.diguage.com</a></span>

<span class=nv>$ </span>kubectl get nodes <span class=nt>-o</span> wide
NAME           STATUS   ROLES           AGE   VERSION   INTERNAL-IP   EXTERNAL-IP   OS-IMAGE             KERNEL-VERSION      CONTAINER-RUNTIME
controlplane   Ready    control-plane   20d   v1.31.0   172.30.1.2    &lt;none&gt;        Ubuntu 20.04.5 LTS   5.4.0-131-generic   containerd://1.7.13
node01         Ready    &lt;none&gt;          20d   v1.31.0   172.30.2.2    &lt;none&gt;        Ubuntu 20.04.5 LTS   5.4.0-131-generic   containerd://1.7.13

<span class=nv>$ </span>kubectl get pod <span class=nt>-A</span>
NAMESPACE            NAME                                      READY   STATUS             RESTARTS      AGE
kube-system          calico-kube-controllers-94fb6bc47-4wx95   1/1     Running            2 <span class=o>(</span>28m ago<span class=o>)</span>   20d
kube-system          canal-mfc56                               2/2     Running            2 <span class=o>(</span>28m ago<span class=o>)</span>   20d
kube-system          canal-zstf2                               2/2     Running            2 <span class=o>(</span>28m ago<span class=o>)</span>   20d
kube-system          coredns-57888bfdc7-6sqfr                  1/1     Running            1 <span class=o>(</span>28m ago<span class=o>)</span>   20d
kube-system          coredns-57888bfdc7-jnrx9                  1/1     Running            1 <span class=o>(</span>28m ago<span class=o>)</span>   20d
kube-system          etcd-controlplane                         1/1     Running            2 <span class=o>(</span>28m ago<span class=o>)</span>   20d
kube-system          kube-apiserver-controlplane               0/1     Running            4 <span class=o>(</span>38s ago<span class=o>)</span>   19m
kube-system          kube-controller-manager-controlplane      0/1     CrashLoopBackOff   6 <span class=o>(</span>57s ago<span class=o>)</span>   20d
kube-system          kube-proxy-sqc72                          1/1     Running            2 <span class=o>(</span>28m ago<span class=o>)</span>   20d
kube-system          kube-proxy-xknck                          1/1     Running            1 <span class=o>(</span>28m ago<span class=o>)</span>   20d
kube-system          kube-scheduler-controlplane               0/1     CrashLoopBackOff   6 <span class=o>(</span>58s ago<span class=o>)</span>   20d
local-path-storage   local-path-provisioner-6c5cff8948-tmf26   1/1     Running            2 <span class=o>(</span>28m ago<span class=o>)</span>   20d

<span class=nv>$ </span>kubectl <span class=nt>-n</span> kube-system describe pod kube-controller-manager-controlplane
Name:                 kube-controller-manager-controlplane
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 controlplane/172.30.1.2
Start Time:           Wed, 22 Jan 2025 10:06:42 +0000
Labels:               <span class=nv>component</span><span class=o>=</span>kube-controller-manager
                      <span class=nv>tier</span><span class=o>=</span>control-plane
Annotations:          kubernetes.io/config.hash: a55d9c391dc5f492555b54cdef44652d
                      kubernetes.io/config.mirror: a55d9c391dc5f492555b54cdef44652d
                      kubernetes.io/config.seen: 2025-01-02T09:49:15.953920980Z
                      kubernetes.io/config.source: file
Status:               Running
SeccompProfile:       RuntimeDefault
IP:                   172.30.1.2
IPs:
  IP:           172.30.1.2
Controlled By:  Node/controlplane
Containers:
  kube-controller-manager:
    Container ID:  containerd://7cf916a69f1ae9fdafff4c03e559c847279caf25d4ae4aafdb7099fd5adb63de
    Image:         registry.k8s.io/kube-controller-manager:v1.31.0
    Image ID:      registry.k8s.io/kube-controller-manager@sha256:f6f3c33dda209e8434b83dacf5244c03b59b0018d93325ff21296a142b68497d
    Port:          &lt;none&gt;
    Host Port:     &lt;none&gt;
    Command:
      kube-controller-manager
      <span class=nt>--allocate-node-cidrs</span><span class=o>=</span><span class=nb>true</span>
      <span class=nt>--authentication-kubeconfig</span><span class=o>=</span>/etc/kubernetes/controller-manager.conf
      <span class=nt>--authorization-kubeconfig</span><span class=o>=</span>/etc/kubernetes/controller-manager.conf
      <span class=nt>--bind-address</span><span class=o>=</span>127.0.0.1
      <span class=nt>--client-ca-file</span><span class=o>=</span>/etc/kubernetes/pki/ca.crt
      <span class=nt>--cluster-cidr</span><span class=o>=</span>192.168.0.0/16
      <span class=nt>--cluster-name</span><span class=o>=</span>kubernetes
      <span class=nt>--cluster-signing-cert-file</span><span class=o>=</span>/etc/kubernetes/pki/ca.crt
      <span class=nt>--cluster-signing-key-file</span><span class=o>=</span>/etc/kubernetes/pki/ca.key
      <span class=nt>--controllers</span><span class=o>=</span><span class=k>*</span>,bootstrapsigner,tokencleaner
      <span class=nt>--kubeconfig</span><span class=o>=</span>/etc/kubernetes/controller-manager.conf
      <span class=nt>--leader-elect</span><span class=o>=</span><span class=nb>true</span>
      <span class=nt>--requestheader-client-ca-file</span><span class=o>=</span>/etc/kubernetes/pki/front-proxy-ca.crt
      <span class=nt>--root-ca-file</span><span class=o>=</span>/etc/kubernetes/pki/ca.crt
      <span class=nt>--service-account-private-key-file</span><span class=o>=</span>/etc/kubernetes/pki/sa.key
      <span class=nt>--service-cluster-ip-range</span><span class=o>=</span>10.96.0.0/12
      <span class=nt>--use-service-account-credentials</span><span class=o>=</span><span class=nb>true
    </span>State:          Waiting
      Reason:       CrashLoopBackOff
    Last State:     Terminated
      Reason:       Error
      Exit Code:    1
      Started:      Wed, 22 Jan 2025 10:30:46 +0000
      Finished:     Wed, 22 Jan 2025 10:33:55 +0000
    Ready:          False
    Restart Count:  6
    Requests:
      cpu:        25m
    Liveness:     http-get https://127.0.0.1:10257/healthz <span class=nv>delay</span><span class=o>=</span>10s <span class=nb>timeout</span><span class=o>=</span>15s <span class=nv>period</span><span class=o>=</span>10s <span class=c>#success=1 #failure=8</span>
    Startup:      http-get https://127.0.0.1:10257/healthz <span class=nv>delay</span><span class=o>=</span>10s <span class=nb>timeout</span><span class=o>=</span>15s <span class=nv>period</span><span class=o>=</span>10s <span class=c>#success=1 #failure=24</span>
    Environment:  &lt;none&gt;
    Mounts:
      /etc/ca-certificates from etc-ca-certificates <span class=o>(</span>ro<span class=o>)</span>
      /etc/kubernetes/controller-manager.conf from kubeconfig <span class=o>(</span>ro<span class=o>)</span>
      /etc/kubernetes/pki from k8s-certs <span class=o>(</span>ro<span class=o>)</span>
      /etc/ssl/certs from ca-certs <span class=o>(</span>ro<span class=o>)</span>
      /usr/libexec/kubernetes/kubelet-plugins/volume/exec from flexvolume-dir <span class=o>(</span>rw<span class=o>)</span>
      /usr/local/share/ca-certificates from usr-local-share-ca-certificates <span class=o>(</span>ro<span class=o>)</span>
      /usr/share/ca-certificates from usr-share-ca-certificates <span class=o>(</span>ro<span class=o>)</span>
Conditions:
  Type                        Status
  PodReadyToStartContainers   True
  Initialized                 True
  Ready                       False
  ContainersReady             False
  PodScheduled                True
Volumes:
  ca-certs:
    Type:          HostPath <span class=o>(</span>bare host directory volume<span class=o>)</span>
    Path:          /etc/ssl/certs
    HostPathType:  DirectoryOrCreate
  etc-ca-certificates:
    Type:          HostPath <span class=o>(</span>bare host directory volume<span class=o>)</span>
    Path:          /etc/ca-certificates
    HostPathType:  DirectoryOrCreate
  flexvolume-dir:
    Type:          HostPath <span class=o>(</span>bare host directory volume<span class=o>)</span>
    Path:          /usr/libexec/kubernetes/kubelet-plugins/volume/exec
    HostPathType:  DirectoryOrCreate
  k8s-certs:
    Type:          HostPath <span class=o>(</span>bare host directory volume<span class=o>)</span>
    Path:          /etc/kubernetes/pki
    HostPathType:  DirectoryOrCreate
  kubeconfig:
    Type:          HostPath <span class=o>(</span>bare host directory volume<span class=o>)</span>
    Path:          /etc/kubernetes/controller-manager.conf
    HostPathType:  FileOrCreate
  usr-local-share-ca-certificates:
    Type:          HostPath <span class=o>(</span>bare host directory volume<span class=o>)</span>
    Path:          /usr/local/share/ca-certificates
    HostPathType:  DirectoryOrCreate
  usr-share-ca-certificates:
    Type:          HostPath <span class=o>(</span>bare host directory volume<span class=o>)</span>
    Path:          /usr/share/ca-certificates
    HostPathType:  DirectoryOrCreate
QoS Class:         Burstable
Node-Selectors:    &lt;none&gt;
Tolerations:       :NoExecute <span class=nv>op</span><span class=o>=</span>Exists
Events:
  Type     Reason          Age                  From     Message
  <span class=nt>----</span>     <span class=nt>------</span>          <span class=nt>----</span>                 <span class=nt>----</span>     <span class=nt>-------</span>
  Normal   Pulled          20d                  kubelet  Container image <span class=s2>&#34;registry.k8s.io/kube-controller-manager:v1.31.0&#34;</span> already present on machine
  Normal   Created         20d                  kubelet  Created container kube-controller-manager
  Normal   Started         20d                  kubelet  Started container kube-controller-manager
  Normal   SandboxChanged  20d                  kubelet  Pod sandbox changed, it will be killed and re-created.
  Normal   Pulled          20d                  kubelet  Container image <span class=s2>&#34;registry.k8s.io/kube-controller-manager:v1.31.0&#34;</span> already present on machine
  Normal   Created         20d                  kubelet  Created container kube-controller-manager
  Normal   Started         20d                  kubelet  Started container kube-controller-manager
  Normal   SandboxChanged  28m                  kubelet  Pod sandbox changed, it will be killed and re-created.
  Normal   Pulled          9m52s <span class=o>(</span>x4 over 28m<span class=o>)</span>  kubelet  Container image <span class=s2>&#34;registry.k8s.io/kube-controller-manager:v1.31.0&#34;</span> already present on machine
  Normal   Created         9m52s <span class=o>(</span>x4 over 28m<span class=o>)</span>  kubelet  Created container kube-controller-manager
  Normal   Started         9m51s <span class=o>(</span>x4 over 28m<span class=o>)</span>  kubelet  Started container kube-controller-manager
  Warning  BackOff         74s <span class=o>(</span>x21 over 19m<span class=o>)</span>   kubelet  Back-off restarting failed container kube-controller-manager <span class=k>in </span>pod kube-controller-manager-controlplane_kube-system<span class=o>(</span>a55d9c391dc5f492555b54cdef44652d<span class=o>)</span>

<span class=nv>$ </span>kubectl <span class=nt>-n</span> kube-system logs kube-controller-manager-controlplane
I0122 10:31:07.464715       1 actual_state_of_world.go:540] <span class=s2>&#34;Failed to update statusUpdateNeeded field in actual state of world&#34;</span> <span class=nv>logger</span><span class=o>=</span><span class=s2>&#34;persistentvolume-attach-detach-controller&#34;</span> <span class=nv>err</span><span class=o>=</span><span class=s2>&#34;Failed to set statusUpdateNeeded to needed true, because nodeName=</span><span class=se>\&#34;</span><span class=s2>controlplane</span><span class=se>\&#34;</span><span class=s2> does not exist&#34;</span>
I0122 10:31:07.471752       1 shared_informer.go:320] Caches are synced <span class=k>for </span>TTL
I0122 10:31:07.472665       1 actual_state_of_world.go:540] <span class=s2>&#34;Failed to update statusUpdateNeeded field in actual state of world&#34;</span> <span class=nv>logger</span><span class=o>=</span><span class=s2>&#34;persistentvolume-attach-detach-controller&#34;</span> <span class=nv>err</span><span class=o>=</span><span class=s2>&#34;Failed to set statusUpdateNeeded to needed true, because nodeName=</span><span class=se>\&#34;</span><span class=s2>node01</span><span class=se>\&#34;</span><span class=s2> does not exist&#34;</span>
I0122 10:31:07.477722       1 shared_informer.go:320] Caches are synced <span class=k>for </span>ClusterRoleAggregator
I0122 10:31:07.481305       1 shared_informer.go:320] Caches are synced <span class=k>for </span>crt configmap
I0122 10:31:07.483384       1 shared_informer.go:313] Waiting <span class=k>for </span>caches to <span class=nb>sync </span><span class=k>for </span>garbage collector
I0122 10:31:07.485070       1 shared_informer.go:320] Caches are synced <span class=k>for </span>certificate-csrapproving
I0122 10:31:07.496164       1 shared_informer.go:320] Caches are synced <span class=k>for </span>GC
I0122 10:31:07.498895       1 shared_informer.go:320] Caches are synced <span class=k>for </span>namespace
I0122 10:31:07.499211       1 shared_informer.go:320] Caches are synced <span class=k>for </span>endpoint
I0122 10:31:07.499658       1 shared_informer.go:320] Caches are synced <span class=k>for </span>node
I0122 10:31:07.499842       1 range_allocator.go:171] <span class=s2>&#34;Sending events to api server&#34;</span> <span class=nv>logger</span><span class=o>=</span><span class=s2>&#34;node-ipam-controller&#34;</span>
I0122 10:31:07.501394       1 range_allocator.go:177] <span class=s2>&#34;Starting range CIDR allocator&#34;</span> <span class=nv>logger</span><span class=o>=</span><span class=s2>&#34;node-ipam-controller&#34;</span>
I0122 10:31:07.501596       1 shared_informer.go:313] Waiting <span class=k>for </span>caches to <span class=nb>sync </span><span class=k>for </span>cidrallocator
I0122 10:31:07.501733       1 shared_informer.go:320] Caches are synced <span class=k>for </span>cidrallocator
I0122 10:31:07.501947       1 range_allocator.go:241] <span class=s2>&#34;Successfully synced&#34;</span> <span class=nv>logger</span><span class=o>=</span><span class=s2>&#34;node-ipam-controller&#34;</span> <span class=nv>key</span><span class=o>=</span><span class=s2>&#34;controlplane&#34;</span>
I0122 10:31:07.502041       1 range_allocator.go:241] <span class=s2>&#34;Successfully synced&#34;</span> <span class=nv>logger</span><span class=o>=</span><span class=s2>&#34;node-ipam-controller&#34;</span> <span class=nv>key</span><span class=o>=</span><span class=s2>&#34;node01&#34;</span>
I0122 10:31:07.502156       1 shared_informer.go:320] Caches are synced <span class=k>for </span>taint-eviction-controller
I0122 10:31:07.502643       1 shared_informer.go:320] Caches are synced <span class=k>for </span>disruption
I0122 10:31:07.507003       1 shared_informer.go:320] Caches are synced <span class=k>for </span><span class=nb>expand
</span>I0122 10:31:07.507577       1 shared_informer.go:320] Caches are synced <span class=k>for </span>ReplicationController
I0122 10:31:07.516863       1 shared_informer.go:320] Caches are synced <span class=k>for </span>deployment
I0122 10:31:07.522026       1 shared_informer.go:320] Caches are synced <span class=k>for </span>stateful <span class=nb>set
</span>I0122 10:31:07.526920       1 shared_informer.go:320] Caches are synced <span class=k>for </span>persistent volume
I0122 10:31:07.527760       1 shared_informer.go:320] Caches are synced <span class=k>for </span>certificate-csrsigning-legacy-unknown
I0122 10:31:07.528118       1 shared_informer.go:320] Caches are synced <span class=k>for </span>certificate-csrsigning-kubelet-serving
I0122 10:31:07.528309       1 shared_informer.go:320] Caches are synced <span class=k>for </span>certificate-csrsigning-kubelet-client
I0122 10:31:07.533341       1 shared_informer.go:320] Caches are synced <span class=k>for </span>ephemeral
I0122 10:31:07.549076       1 shared_informer.go:320] Caches are synced <span class=k>for </span>HPA
I0122 10:31:07.549368       1 shared_informer.go:320] Caches are synced <span class=k>for </span>certificate-csrsigning-kube-apiserver-client
I0122 10:31:07.556642       1 shared_informer.go:320] Caches are synced <span class=k>for </span>endpoint_slice
I0122 10:31:07.559824       1 shared_informer.go:320] Caches are synced <span class=k>for </span>taint
I0122 10:31:07.560120       1 node_lifecycle_controller.go:1232] <span class=s2>&#34;Initializing eviction metric for zone&#34;</span> <span class=nv>logger</span><span class=o>=</span><span class=s2>&#34;node-lifecycle-controller&#34;</span> <span class=nv>zone</span><span class=o>=</span><span class=s2>&#34;&#34;</span>
I0122 10:31:07.560312       1 node_lifecycle_controller.go:884] <span class=s2>&#34;Missing timestamp for Node. Assuming now as a timestamp&#34;</span> <span class=nv>logger</span><span class=o>=</span><span class=s2>&#34;node-lifecycle-controller&#34;</span> <span class=nv>node</span><span class=o>=</span><span class=s2>&#34;controlplane&#34;</span>
I0122 10:31:07.560466       1 node_lifecycle_controller.go:884] <span class=s2>&#34;Missing timestamp for Node. Assuming now as a timestamp&#34;</span> <span class=nv>logger</span><span class=o>=</span><span class=s2>&#34;node-lifecycle-controller&#34;</span> <span class=nv>node</span><span class=o>=</span><span class=s2>&#34;node01&#34;</span>
I0122 10:31:07.562891       1 shared_informer.go:320] Caches are synced <span class=k>for </span>service account
I0122 10:31:07.587036       1 shared_informer.go:320] Caches are synced <span class=k>for </span>job
I0122 10:31:07.587154       1 shared_informer.go:320] Caches are synced <span class=k>for </span>daemon sets
I0122 10:31:07.587430       1 shared_informer.go:320] Caches are synced <span class=k>for </span>legacy-service-account-token-cleaner
I0122 10:31:07.587754       1 shared_informer.go:320] Caches are synced <span class=k>for </span>cronjob
I0122 10:31:07.587788       1 shared_informer.go:320] Caches are synced <span class=k>for </span>endpoint_slice_mirroring
I0122 10:31:07.587913       1 shared_informer.go:320] Caches are synced <span class=k>for </span>PV protection
I0122 10:31:07.591715       1 shared_informer.go:320] Caches are synced <span class=k>for </span>ReplicaSet
I0122 10:31:07.599025       1 topologycache.go:237] <span class=s2>&#34;Can&#39;t get CPU or zone information for node&#34;</span> <span class=nv>logger</span><span class=o>=</span><span class=s2>&#34;endpointslice-controller&#34;</span> <span class=nv>node</span><span class=o>=</span><span class=s2>&#34;node01&#34;</span>
I0122 10:31:07.606049       1 node_lifecycle_controller.go:1078] <span class=s2>&#34;Controller detected that zone is now in new state&#34;</span> <span class=nv>logger</span><span class=o>=</span><span class=s2>&#34;node-lifecycle-controller&#34;</span> <span class=nv>zone</span><span class=o>=</span><span class=s2>&#34;&#34;</span> <span class=nv>newState</span><span class=o>=</span><span class=s2>&#34;Normal&#34;</span>
I0122 10:31:07.615048       1 shared_informer.go:320] Caches are synced <span class=k>for </span>TTL after finished
I0122 10:31:07.615414       1 shared_informer.go:320] Caches are synced <span class=k>for </span>PVC protection
I0122 10:31:07.643139       1 replica_set.go:679] <span class=s2>&#34;Finished syncing&#34;</span> <span class=nv>logger</span><span class=o>=</span><span class=s2>&#34;replicaset-controller&#34;</span> <span class=nv>kind</span><span class=o>=</span><span class=s2>&#34;ReplicaSet&#34;</span> <span class=nv>key</span><span class=o>=</span><span class=s2>&#34;kube-system/calico-kube-controllers-94fb6bc47&#34;</span> <span class=nv>duration</span><span class=o>=</span><span class=s2>&#34;86.348µs&#34;</span>
I0122 10:31:07.643557       1 replica_set.go:679] <span class=s2>&#34;Finished syncing&#34;</span> <span class=nv>logger</span><span class=o>=</span><span class=s2>&#34;replicaset-controller&#34;</span> <span class=nv>kind</span><span class=o>=</span><span class=s2>&#34;ReplicaSet&#34;</span> <span class=nv>key</span><span class=o>=</span><span class=s2>&#34;kube-system/coredns-57888bfdc7&#34;</span> <span class=nv>duration</span><span class=o>=</span><span class=s2>&#34;105.653µs&#34;</span>
I0122 10:31:07.643856       1 replica_set.go:679] <span class=s2>&#34;Finished syncing&#34;</span> <span class=nv>logger</span><span class=o>=</span><span class=s2>&#34;replicaset-controller&#34;</span> <span class=nv>kind</span><span class=o>=</span><span class=s2>&#34;ReplicaSet&#34;</span> <span class=nv>key</span><span class=o>=</span><span class=s2>&#34;kube-system/coredns-6f6b679f8f&#34;</span> <span class=nv>duration</span><span class=o>=</span><span class=s2>&#34;119.561µs&#34;</span>
I0122 10:31:07.644056       1 replica_set.go:679] <span class=s2>&#34;Finished syncing&#34;</span> <span class=nv>logger</span><span class=o>=</span><span class=s2>&#34;replicaset-controller&#34;</span> <span class=nv>kind</span><span class=o>=</span><span class=s2>&#34;ReplicaSet&#34;</span> <span class=nv>key</span><span class=o>=</span><span class=s2>&#34;local-path-storage/local-path-provisioner-6c5cff8948&#34;</span> <span class=nv>duration</span><span class=o>=</span><span class=s2>&#34;45.995µs&#34;</span>
I0122 10:31:07.648059       1 shared_informer.go:320] Caches are synced <span class=k>for </span>resource quota
I0122 10:31:07.651899       1 shared_informer.go:320] Caches are synced <span class=k>for </span>resource quota
I0122 10:31:07.659814       1 shared_informer.go:320] Caches are synced <span class=k>for </span>attach detach
I0122 10:31:07.690290       1 shared_informer.go:320] Caches are synced <span class=k>for </span>validatingadmissionpolicy-status
I0122 10:31:08.120299       1 shared_informer.go:320] Caches are synced <span class=k>for </span>garbage collector
I0122 10:31:08.120612       1 garbagecollector.go:157] <span class=s2>&#34;All resource monitors have synced. Proceeding to collect garbage&#34;</span> <span class=nv>logger</span><span class=o>=</span><span class=s2>&#34;garbage-collector-controller&#34;</span>
I0122 10:31:08.184489       1 shared_informer.go:320] Caches are synced <span class=k>for </span>garbage collector
I0122 10:32:26.669535       1 range_allocator.go:241] <span class=s2>&#34;Successfully synced&#34;</span> <span class=nv>logger</span><span class=o>=</span><span class=s2>&#34;node-ipam-controller&#34;</span> <span class=nv>key</span><span class=o>=</span><span class=s2>&#34;node01&#34;</span>
I0122 10:32:30.819664       1 range_allocator.go:241] <span class=s2>&#34;Successfully synced&#34;</span> <span class=nv>logger</span><span class=o>=</span><span class=s2>&#34;node-ipam-controller&#34;</span> <span class=nv>key</span><span class=o>=</span><span class=s2>&#34;controlplane&#34;</span>
E0122 10:33:45.075699       1 leaderelection.go:429] Failed to update lock optimitically: Put <span class=s2>&#34;https://172.30.1.2:6443/apis/coordination.k8s.io/v1/namespaces/kube-system/leases/kube-controller-manager?timeout=5s&#34;</span>: dial tcp 172.30.1.2:6443: connect: connection refused, falling back to slow path
E0122 10:33:45.075955       1 leaderelection.go:436] error retrieving resource lock kube-system/kube-controller-manager: Get <span class=s2>&#34;https://172.30.1.2:6443/apis/coordination.k8s.io/v1/namespaces/kube-system/leases/kube-controller-manager?timeout=5s&#34;</span>: dial tcp 172.30.1.2:6443: connect: connection refused
E0122 10:33:47.076955       1 leaderelection.go:429] Failed to update lock optimitically: Put <span class=s2>&#34;https://172.30.1.2:6443/apis/coordination.k8s.io/v1/namespaces/kube-system/leases/kube-controller-manager?timeout=5s&#34;</span>: dial tcp 172.30.1.2:6443: connect: connection refused, falling back to slow path
E0122 10:33:47.077696       1 leaderelection.go:436] error retrieving resource lock kube-system/kube-controller-manager: Get <span class=s2>&#34;https://172.30.1.2:6443/apis/coordination.k8s.io/v1/namespaces/kube-system/leases/kube-controller-manager?timeout=5s&#34;</span>: dial tcp 172.30.1.2:6443: connect: connection refused
E0122 10:33:49.076833       1 leaderelection.go:429] Failed to update lock optimitically: Put <span class=s2>&#34;https://172.30.1.2:6443/apis/coordination.k8s.io/v1/namespaces/kube-system/leases/kube-controller-manager?timeout=5s&#34;</span>: dial tcp 172.30.1.2:6443: connect: connection refused, falling back to slow path
E0122 10:33:49.077663       1 leaderelection.go:436] error retrieving resource lock kube-system/kube-controller-manager: Get <span class=s2>&#34;https://172.30.1.2:6443/apis/coordination.k8s.io/v1/namespaces/kube-system/leases/kube-controller-manager?timeout=5s&#34;</span>: dial tcp 172.30.1.2:6443: connect: connection refused
E0122 10:33:51.077092       1 leaderelection.go:429] Failed to update lock optimitically: Put <span class=s2>&#34;https://172.30.1.2:6443/apis/coordination.k8s.io/v1/namespaces/kube-system/leases/kube-controller-manager?timeout=5s&#34;</span>: dial tcp 172.30.1.2:6443: connect: connection refused, falling back to slow path
E0122 10:33:51.077677       1 leaderelection.go:436] error retrieving resource lock kube-system/kube-controller-manager: Get <span class=s2>&#34;https://172.30.1.2:6443/apis/coordination.k8s.io/v1/namespaces/kube-system/leases/kube-controller-manager?timeout=5s&#34;</span>: dial tcp 172.30.1.2:6443: connect: connection refused
E0122 10:33:53.076654       1 leaderelection.go:429] Failed to update lock optimitically: Put <span class=s2>&#34;https://172.30.1.2:6443/apis/coordination.k8s.io/v1/namespaces/kube-system/leases/kube-controller-manager?timeout=5s&#34;</span>: dial tcp 172.30.1.2:6443: connect: connection refused, falling back to slow path
E0122 10:33:53.077099       1 leaderelection.go:436] error retrieving resource lock kube-system/kube-controller-manager: Get <span class=s2>&#34;https://172.30.1.2:6443/apis/coordination.k8s.io/v1/namespaces/kube-system/leases/kube-controller-manager?timeout=5s&#34;</span>: dial tcp 172.30.1.2:6443: connect: connection refused
I0122 10:33:55.075456       1 leaderelection.go:297] failed to renew lease kube-system/kube-controller-manager: timed out waiting <span class=k>for </span>the condition
E0122 10:33:55.075566       1 controllermanager.go:340] <span class=s2>&#34;leaderelection lost&#34;</span></code></pre></div></div><div class="admonitionblock note"><table><tbody><tr><td class=icon><i class="fa icon-note" title=Note></i></td><td class=content>不知道该怎么下手了？</td></tr></tbody></table></div></div></div><div class=sect1><h2 id=_troubleshooting_kubelet_issue>6. Troubleshooting - Kubelet Issue</h2><div class=sectionbody><div class=paragraph><p><a href=https://killercoda.com/sachin/course/CKA/kubelet-issue target=_blank rel=noopener>Troubleshooting - Kubelet Issue</a></p></div><div class=sidebarblock><div class=content><div class=paragraph><p>In <code>controlplane</code> node, something problem with <code>kubelet</code> configuration files, fix that issue</p></div><div class=paragraph><p>You can <code>ssh controlplane</code></p></div><div class=paragraph><p>location: <code>/var/lib/kubelet/config.yaml</code> and <code>/etc/kubernetes/kubelet.conf</code></p></div></div></div><div class=listingblock><div class=content><pre class="rouge highlight nowrap"><code data-lang=bash><span class=c># @author D瓜哥 · <a href=https://www.diguage.com target=_blank>https://www.diguage.com</a></span>

<span class=nv>$ </span>kubectl get nodes <span class=nt>-o</span> wide
NAME           STATUS     ROLES           AGE   VERSION   INTERNAL-IP   EXTERNAL-IP   OS-IMAGE             KERNEL-VERSION      CONTAINER-RUNTIME
controlplane   NotReady   control-plane   13d   v1.31.0   172.30.1.2    &lt;none&gt;        Ubuntu 20.04.5 LTS   5.4.0-131-generic   containerd://1.7.22
node01         Ready      &lt;none&gt;          13d   v1.31.0   172.30.2.2    &lt;none&gt;        Ubuntu 20.04.5 LTS   5.4.0-131-generic   containerd://1.7.22

<span class=nv>$ </span>kubectl get nodes
NAME           STATUS     ROLES           AGE   VERSION
controlplane   NotReady   control-plane   13d   v1.31.0
node01         Ready      &lt;none&gt;          13d   v1.31.0

<span class=nv>$ </span>kubectl describe nodes controlplane
Name:               controlplane
Roles:              control-plane
Labels:             beta.kubernetes.io/arch<span class=o>=</span>amd64
                    beta.kubernetes.io/os<span class=o>=</span>linux
                    kubernetes.io/arch<span class=o>=</span>amd64
                    kubernetes.io/hostname<span class=o>=</span>controlplane
                    kubernetes.io/os<span class=o>=</span>linux
                    node-role.kubernetes.io/control-plane<span class=o>=</span>
                    node.kubernetes.io/exclude-from-external-load-balancers<span class=o>=</span>
Annotations:        flannel.alpha.coreos.com/backend-data: <span class=o>{</span><span class=s2>&#34;VNI&#34;</span>:1,<span class=s2>&#34;VtepMAC&#34;</span>:<span class=s2>&#34;76:af:a0:b0:4b:41&#34;</span><span class=o>}</span>
                    flannel.alpha.coreos.com/backend-type: vxlan
                    flannel.alpha.coreos.com/kube-subnet-manager: <span class=nb>true
                    </span>flannel.alpha.coreos.com/public-ip: 172.30.1.2
                    kubeadm.alpha.kubernetes.io/cri-socket: unix:///var/run/containerd/containerd.sock
                    node.alpha.kubernetes.io/ttl: 0
                    projectcalico.org/IPv4Address: 172.30.1.2/24
                    projectcalico.org/IPv4IPIPTunnelAddr: 192.168.0.1
                    volumes.kubernetes.io/controller-managed-attach-detach: <span class=nb>true
</span>CreationTimestamp:  Tue, 28 Jan 2025 16:04:13 +0000
Taints:             node.kubernetes.io/unreachable:NoExecute
                    node-role.kubernetes.io/control-plane:NoSchedule
                    node.kubernetes.io/unreachable:NoSchedule
Unschedulable:      <span class=nb>false
</span>Lease:
  HolderIdentity:  controlplane
  AcquireTime:     &lt;<span class=nb>unset</span><span class=o>&gt;</span>
  RenewTime:       Tue, 11 Feb 2025 09:06:15 +0000
Conditions:
  Type                 Status    LastHeartbeatTime                 LastTransitionTime                Reason              Message
  <span class=nt>----</span>                 <span class=nt>------</span>    <span class=nt>-----------------</span>                 <span class=nt>------------------</span>                <span class=nt>------</span>              <span class=nt>-------</span>
  NetworkUnavailable   False     Tue, 11 Feb 2025 09:05:10 +0000   Tue, 11 Feb 2025 09:05:10 +0000   FlannelIsUp         Flannel is running on this node
  MemoryPressure       Unknown   Tue, 11 Feb 2025 09:04:53 +0000   Tue, 11 Feb 2025 09:06:55 +0000   NodeStatusUnknown   Kubelet stopped posting node status.
  DiskPressure         Unknown   Tue, 11 Feb 2025 09:04:53 +0000   Tue, 11 Feb 2025 09:06:55 +0000   NodeStatusUnknown   Kubelet stopped posting node status.
  PIDPressure          Unknown   Tue, 11 Feb 2025 09:04:53 +0000   Tue, 11 Feb 2025 09:06:55 +0000   NodeStatusUnknown   Kubelet stopped posting node status.
  Ready                Unknown   Tue, 11 Feb 2025 09:04:53 +0000   Tue, 11 Feb 2025 09:06:55 +0000   NodeStatusUnknown   Kubelet stopped posting node status.
Addresses:
  InternalIP:  172.30.1.2
  Hostname:    controlplane
Capacity:
  cpu:                1
  ephemeral-storage:  20134592Ki
  hugepages-2Mi:      0
  memory:             2030940Ki
  pods:               110
Allocatable:
  cpu:                1
  ephemeral-storage:  19586931083
  hugepages-2Mi:      0
  memory:             1928540Ki
  pods:               110
System Info:
  Machine ID:                 388a2d0f867a4404bc12a0093bd9ed8d
  System UUID:                0e235667-c51b-41d2-ad6f-c64514563910
  Boot ID:                    b507baf9-425b-4b36-88ea-d81538d9779a
  Kernel Version:             5.4.0-131-generic
  OS Image:                   Ubuntu 20.04.5 LTS
  Operating System:           linux
  Architecture:               amd64
  Container Runtime Version:  containerd://1.7.22
  Kubelet Version:            v1.31.0
  Kube-Proxy Version:
PodCIDR:                      192.168.0.0/24
PodCIDRs:                     192.168.0.0/24
Non-terminated Pods:          <span class=o>(</span>8 <span class=k>in </span>total<span class=o>)</span>
  Namespace                   Name                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age
  <span class=nt>---------</span>                   <span class=nt>----</span>                                       <span class=nt>------------</span>  <span class=nt>----------</span>  <span class=nt>---------------</span>  <span class=nt>-------------</span>  <span class=nt>---</span>
  kube-system                 calico-kube-controllers-94fb6bc47-rxh7x    0 <span class=o>(</span>0%<span class=o>)</span>        0 <span class=o>(</span>0%<span class=o>)</span>      0 <span class=o>(</span>0%<span class=o>)</span>           0 <span class=o>(</span>0%<span class=o>)</span>         13d
  kube-system                 canal-zl4tq                                25m <span class=o>(</span>2%<span class=o>)</span>      0 <span class=o>(</span>0%<span class=o>)</span>      0 <span class=o>(</span>0%<span class=o>)</span>           0 <span class=o>(</span>0%<span class=o>)</span>         13d
  kube-system                 etcd-controlplane                          25m <span class=o>(</span>2%<span class=o>)</span>      0 <span class=o>(</span>0%<span class=o>)</span>      100Mi <span class=o>(</span>5%<span class=o>)</span>       0 <span class=o>(</span>0%<span class=o>)</span>         13d
  kube-system                 kube-apiserver-controlplane                50m <span class=o>(</span>5%<span class=o>)</span>      0 <span class=o>(</span>0%<span class=o>)</span>      0 <span class=o>(</span>0%<span class=o>)</span>           0 <span class=o>(</span>0%<span class=o>)</span>         13d
  kube-system                 kube-controller-manager-controlplane       25m <span class=o>(</span>2%<span class=o>)</span>      0 <span class=o>(</span>0%<span class=o>)</span>      0 <span class=o>(</span>0%<span class=o>)</span>           0 <span class=o>(</span>0%<span class=o>)</span>         13d
  kube-system                 kube-proxy-2mfwz                           0 <span class=o>(</span>0%<span class=o>)</span>        0 <span class=o>(</span>0%<span class=o>)</span>      0 <span class=o>(</span>0%<span class=o>)</span>           0 <span class=o>(</span>0%<span class=o>)</span>         13d
  kube-system                 kube-scheduler-controlplane                25m <span class=o>(</span>2%<span class=o>)</span>      0 <span class=o>(</span>0%<span class=o>)</span>      0 <span class=o>(</span>0%<span class=o>)</span>           0 <span class=o>(</span>0%<span class=o>)</span>         13d
  local-path-storage          local-path-provisioner-6c5cff8948-2x89z    0 <span class=o>(</span>0%<span class=o>)</span>        0 <span class=o>(</span>0%<span class=o>)</span>      0 <span class=o>(</span>0%<span class=o>)</span>           0 <span class=o>(</span>0%<span class=o>)</span>         13d
Allocated resources:
  <span class=o>(</span>Total limits may be over 100 percent, i.e., overcommitted.<span class=o>)</span>
  Resource           Requests    Limits
  <span class=nt>--------</span>           <span class=nt>--------</span>    <span class=nt>------</span>
  cpu                150m <span class=o>(</span>15%<span class=o>)</span>  0 <span class=o>(</span>0%<span class=o>)</span>
  memory             100Mi <span class=o>(</span>5%<span class=o>)</span>  0 <span class=o>(</span>0%<span class=o>)</span>
  ephemeral-storage  0 <span class=o>(</span>0%<span class=o>)</span>      0 <span class=o>(</span>0%<span class=o>)</span>
  hugepages-2Mi      0 <span class=o>(</span>0%<span class=o>)</span>      0 <span class=o>(</span>0%<span class=o>)</span>
Events:
  Type     Reason                   Age                  From             Message
  <span class=nt>----</span>     <span class=nt>------</span>                   <span class=nt>----</span>                 <span class=nt>----</span>             <span class=nt>-------</span>
  Normal   Starting                 3m34s                kube-proxy
  Normal   Starting                 13d                  kube-proxy
  Normal   Starting                 13d                  kube-proxy
  Normal   Starting                 13d                  kubelet          Starting kubelet.
  Warning  CgroupV1                 13d                  kubelet          Cgroup v1 support is <span class=k>in </span>maintenance mode, please migrate to Cgroup v2.
  Normal   NodeHasSufficientPID     13d                  kubelet          Node controlplane status is now: NodeHasSufficientPID
  Normal   NodeAllocatableEnforced  13d                  kubelet          Updated Node Allocatable limit across pods
  Normal   NodeHasSufficientMemory  13d                  kubelet          Node controlplane status is now: NodeHasSufficientMemory
  Normal   NodeHasNoDiskPressure    13d                  kubelet          Node controlplane status is now: NodeHasNoDiskPressure
  Normal   RegisteredNode           13d                  node-controller  Node controlplane event: Registered Node controlplane <span class=k>in </span>Controller
  Normal   NodeReady                13d                  kubelet          Node controlplane status is now: NodeReady
  Normal   RegisteredNode           13d                  node-controller  Node controlplane event: Registered Node controlplane <span class=k>in </span>Controller
  Normal   NodeAllocatableEnforced  13d                  kubelet          Updated Node Allocatable limit across pods
  Normal   Starting                 13d                  kubelet          Starting kubelet.
  Warning  CgroupV1                 13d                  kubelet          Cgroup v1 support is <span class=k>in </span>maintenance mode, please migrate to Cgroup v2.
  Normal   NodeHasSufficientMemory  13d <span class=o>(</span>x8 over 13d<span class=o>)</span>    kubelet          Node controlplane status is now: NodeHasSufficientMemory
  Normal   NodeHasNoDiskPressure    13d <span class=o>(</span>x7 over 13d<span class=o>)</span>    kubelet          Node controlplane status is now: NodeHasNoDiskPressure
  Normal   NodeHasSufficientPID     13d <span class=o>(</span>x7 over 13d<span class=o>)</span>    kubelet          Node controlplane status is now: NodeHasSufficientPID
  Normal   RegisteredNode           13d                  node-controller  Node controlplane event: Registered Node controlplane <span class=k>in </span>Controller
  Normal   Starting                 4m5s                 kubelet          Starting kubelet.
  Warning  CgroupV1                 4m5s                 kubelet          Cgroup v1 support is <span class=k>in </span>maintenance mode, please migrate to Cgroup v2.
  Normal   NodeHasSufficientMemory  4m4s <span class=o>(</span>x8 over 4m4s<span class=o>)</span>  kubelet          Node controlplane status is now: NodeHasSufficientMemory
  Normal   NodeHasNoDiskPressure    4m4s <span class=o>(</span>x7 over 4m4s<span class=o>)</span>  kubelet          Node controlplane status is now: NodeHasNoDiskPressure
  Normal   NodeHasSufficientPID     4m4s <span class=o>(</span>x7 over 4m4s<span class=o>)</span>  kubelet          Node controlplane status is now: NodeHasSufficientPID
  Normal   NodeAllocatableEnforced  4m4s                 kubelet          Updated Node Allocatable limit across pods
  Normal   RegisteredNode           3m23s                node-controller  Node controlplane event: Registered Node controlplane <span class=k>in </span>Controller
  Normal   NodeNotReady             103s                 node-controller  Node controlplane status is now: NodeNotReady

<span class=nv>$ </span>kubectl get nodes controlplane <span class=nt>-o</span> yaml
apiVersion: v1
kind: Node
metadata:
  annotations:
    flannel.alpha.coreos.com/backend-data: <span class=s1>&#39;{&#34;VNI&#34;:1,&#34;VtepMAC&#34;:&#34;76:af:a0:b0:4b:41&#34;}&#39;</span>
    flannel.alpha.coreos.com/backend-type: vxlan
    flannel.alpha.coreos.com/kube-subnet-manager: <span class=s2>&#34;true&#34;</span>
    flannel.alpha.coreos.com/public-ip: 172.30.1.2
    kubeadm.alpha.kubernetes.io/cri-socket: unix:///var/run/containerd/containerd.sock
    node.alpha.kubernetes.io/ttl: <span class=s2>&#34;0&#34;</span>
    projectcalico.org/IPv4Address: 172.30.1.2/24
    projectcalico.org/IPv4IPIPTunnelAddr: 192.168.0.1
    volumes.kubernetes.io/controller-managed-attach-detach: <span class=s2>&#34;true&#34;</span>
  creationTimestamp: <span class=s2>&#34;2025-01-28T16:04:13Z&#34;</span>
  labels:
    beta.kubernetes.io/arch: amd64
    beta.kubernetes.io/os: linux
    kubernetes.io/arch: amd64
    kubernetes.io/hostname: controlplane
    kubernetes.io/os: linux
    node-role.kubernetes.io/control-plane: <span class=s2>&#34;&#34;</span>
    node.kubernetes.io/exclude-from-external-load-balancers: <span class=s2>&#34;&#34;</span>
  name: controlplane
  resourceVersion: <span class=s2>&#34;2144&#34;</span>
  uid: 52bb0db8-eeb9-48ee-8e38-a386487ad66e
spec:
  podCIDR: 192.168.0.0/24
  podCIDRs:
  - 192.168.0.0/24
  taints:
  - effect: NoSchedule
    key: node-role.kubernetes.io/control-plane
  - effect: NoSchedule
    key: node.kubernetes.io/unreachable
    timeAdded: <span class=s2>&#34;2025-02-11T09:06:55Z&#34;</span>
  - effect: NoExecute
    key: node.kubernetes.io/unreachable
    timeAdded: <span class=s2>&#34;2025-02-11T09:07:01Z&#34;</span>
status:
  addresses:
  - address: 172.30.1.2
    <span class=nb>type</span>: InternalIP
  - address: controlplane
    <span class=nb>type</span>: Hostname
  allocatable:
    cpu: <span class=s2>&#34;1&#34;</span>
    ephemeral-storage: <span class=s2>&#34;19586931083&#34;</span>
    hugepages-2Mi: <span class=s2>&#34;0&#34;</span>
    memory: 1928540Ki
    pods: <span class=s2>&#34;110&#34;</span>
  capacity:
    cpu: <span class=s2>&#34;1&#34;</span>
    ephemeral-storage: 20134592Ki
    hugepages-2Mi: <span class=s2>&#34;0&#34;</span>
    memory: 2030940Ki
    pods: <span class=s2>&#34;110&#34;</span>
  conditions:
  - lastHeartbeatTime: <span class=s2>&#34;2025-02-11T09:05:10Z&#34;</span>
    lastTransitionTime: <span class=s2>&#34;2025-02-11T09:05:10Z&#34;</span>
    message: Flannel is running on this node
    reason: FlannelIsUp
    status: <span class=s2>&#34;False&#34;</span>
    <span class=nb>type</span>: NetworkUnavailable
  - lastHeartbeatTime: <span class=s2>&#34;2025-02-11T09:04:53Z&#34;</span>
    lastTransitionTime: <span class=s2>&#34;2025-02-11T09:06:55Z&#34;</span>
    message: Kubelet stopped posting node status.
    reason: NodeStatusUnknown
    status: Unknown
    <span class=nb>type</span>: MemoryPressure
  - lastHeartbeatTime: <span class=s2>&#34;2025-02-11T09:04:53Z&#34;</span>
    lastTransitionTime: <span class=s2>&#34;2025-02-11T09:06:55Z&#34;</span>
    message: Kubelet stopped posting node status.
    reason: NodeStatusUnknown
    status: Unknown
    <span class=nb>type</span>: DiskPressure
  - lastHeartbeatTime: <span class=s2>&#34;2025-02-11T09:04:53Z&#34;</span>
    lastTransitionTime: <span class=s2>&#34;2025-02-11T09:06:55Z&#34;</span>
    message: Kubelet stopped posting node status.
    reason: NodeStatusUnknown
    status: Unknown
    <span class=nb>type</span>: PIDPressure
  - lastHeartbeatTime: <span class=s2>&#34;2025-02-11T09:04:53Z&#34;</span>
    lastTransitionTime: <span class=s2>&#34;2025-02-11T09:06:55Z&#34;</span>
    message: Kubelet stopped posting node status.
    reason: NodeStatusUnknown
    status: Unknown
    <span class=nb>type</span>: Ready
  daemonEndpoints:
    kubeletEndpoint:
      Port: 10250
  images:
  - names:
    - docker.io/calico/cni@sha256:e60b90d7861e872efa720ead575008bc6eca7bee41656735dcaa8210b688fcd9
    - docker.io/calico/cni:v3.24.1
    sizeBytes: 87382462
  - names:
    - docker.io/calico/node@sha256:43f6cee5ca002505ea142b3821a76d585aa0c8d22bc58b7e48589ca7deb48c13
    - docker.io/calico/node:v3.24.1
    sizeBytes: 80180860
  - names:
    - registry.k8s.io/etcd@sha256:a6dc63e6e8cfa0307d7851762fa6b629afb18f28d8aa3fab5a6e91b4af60026a
    - registry.k8s.io/etcd:3.5.15-0
    sizeBytes: 56909194
  - names:
    - docker.io/calico/kube-controllers@sha256:4010b2739792ae5e77a750be909939c0a0a372e378f3c81020754efcf4a91efa
    - docker.io/calico/kube-controllers:v3.24.1
    sizeBytes: 31125927
  - names:
    - registry.k8s.io/kube-proxy@sha256:c727efb1c6f15a68060bf7f207f5c7a765355b7e3340c513e582ec819c5cd2fe
    - registry.k8s.io/kube-proxy:v1.31.0
    sizeBytes: 30207900
  - names:
    - registry.k8s.io/kube-apiserver@sha256:470179274deb9dc3a81df55cfc24823ce153147d4ebf2ed649a4f271f51eaddf
    - registry.k8s.io/kube-apiserver:v1.31.0
    sizeBytes: 28063421
  - names:
    - registry.k8s.io/kube-controller-manager@sha256:f6f3c33dda209e8434b83dacf5244c03b59b0018d93325ff21296a142b68497d
    - registry.k8s.io/kube-controller-manager:v1.31.0
    sizeBytes: 26240868
  - names:
    - quay.io/coreos/flannel@sha256:9a296fbb67790659adc3701e287adde3c59803b7fcefe354f1fc482840cdb3d9
    - quay.io/coreos/flannel:v0.15.1
    sizeBytes: 21673107
  - names:
    - docker.io/rancher/local-path-provisioner@sha256:349f2d75f8a90e218ce9a20e3e302368f2247cb36d676b46e9c27e1aac9ad683
    - docker.io/rancher/local-path-provisioner:master-head
    sizeBytes: 20727854
  - names:
    - registry.k8s.io/kube-scheduler@sha256:96ddae9c9b2e79342e0551e2d2ec422c0c02629a74d928924aaa069706619808
    - registry.k8s.io/kube-scheduler:v1.31.0
    sizeBytes: 20196722
  - names:
    - registry.k8s.io/coredns/coredns@sha256:1eeb4c7316bacb1d4c8ead65571cd92dd21e27359f0d4917f1a5822a73b75db1
    - registry.k8s.io/coredns/coredns:v1.11.1
    sizeBytes: 18182961
  - names:
    - registry.k8s.io/pause@sha256:ee6521f290b2168b6e0935a181d4cff9be1ac3f505666ef0e3c98fae8199917a
    - registry.k8s.io/pause:3.10
    sizeBytes: 320368
  - names:
    - registry.k8s.io/pause@sha256:1ff6c18fbef2045af6b9c16bf034cc421a29027b800e4f9b68ae9b1cb3e9ae07
    - registry.k8s.io/pause:3.5
    sizeBytes: 301416
  nodeInfo:
    architecture: amd64
    bootID: b507baf9-425b-4b36-88ea-d81538d9779a
    containerRuntimeVersion: containerd://1.7.22
    kernelVersion: 5.4.0-131-generic
    kubeProxyVersion: <span class=s2>&#34;&#34;</span>
    kubeletVersion: v1.31.0
    machineID: 388a2d0f867a4404bc12a0093bd9ed8d
    operatingSystem: linux
    osImage: Ubuntu 20.04.5 LTS
    systemUUID: 0e235667-c51b-41d2-ad6f-c64514563910

<span class=nv>$ </span><span class=nb>cat</span> /var/lib/kubelet/config.yaml
apiVersion: kubelet.config.k8s.io/v1beta1
authentication:
  anonymous:
    enabled: <span class=nb>false
  </span>webhook:
    cacheTTL: 0s
    enabled: <span class=nb>true
  </span>x509:
    clientCAFile: /etc/kubernetes/pki/CA.CERTIFICATE
authorization:
  mode: Webhook
  webhook:
    cacheAuthorizedTTL: 0s
    cacheUnauthorizedTTL: 0s
cgroupDriver: systemd
clusterDNS:
- 10.96.0.10
clusterDomain: cluster.local
containerRuntimeEndpoint: <span class=s2>&#34;&#34;</span>
cpuManagerReconcilePeriod: 0s
evictionPressureTransitionPeriod: 0s
fileCheckFrequency: 0s
healthzBindAddress: 127.0.0.1
healthzPort: 10248
httpCheckFrequency: 0s
imageMaximumGCAge: 0s
imageMinimumGCAge: 0s
kind: KubeletConfiguration
logging:
  flushFrequency: 0
  options:
    json:
      infoBufferSize: <span class=s2>&#34;0&#34;</span>
    text:
      infoBufferSize: <span class=s2>&#34;0&#34;</span>
  verbosity: 0
memorySwap: <span class=o>{}</span>
nodeStatusReportFrequency: 0s
nodeStatusUpdateFrequency: 0s
resolvConf: /run/systemd/resolve/resolv.conf
rotateCertificates: <span class=nb>true
</span>runtimeRequestTimeout: 0s
shutdownGracePeriod: 0s
shutdownGracePeriodCriticalPods: 0s
staticPodPath: /etc/kubernetes/manifests
streamingConnectionIdleTimeout: 0s
syncFrequency: 0s
volumeStatsAggPeriod: 0s
<span class=c># 这里藏着一个不太明显的问题原因：证书文件名 CA.CERTIFICATE 写错了。第一次做，竟然没有发现。</span>

<span class=nv>$ </span><span class=nb>cat</span> /etc/kubernetes/kubelet.conf
apiVersion: v1
clusters:
- cluster:
    certificate-authority-data: &lt;证书内容&gt;
    server: https://172.30.1.2:64433333
  name: kubernetes
contexts:
- context:
    cluster: kubernetes
    user: system:node:controlplane
  name: system:node:controlplane@kubernetes
current-context: system:node:controlplane@kubernetes
kind: Config
preferences: <span class=o>{}</span>
<span class=nb>users</span>:
- name: system:node:controlplane
  user:
    client-certificate: /var/lib/kubelet/pki/kubelet-client-current.pem
    client-key: /var/lib/kubelet/pki/kubelet-client-current.pem
<span class=c># 找到错误：端口号 64433333 是一个明显的配置错误。</span>

<span class=nv>$ </span>vim /etc/kubernetes/kubelet.conf
<span class=c># 将端口号从 64433333 改为 6443。</span>

<span class=nv>$ </span>kubectl get nodes
NAME           STATUS     ROLES           AGE   VERSION
controlplane   NotReady   control-plane   13d   v1.31.0
node01         Ready      &lt;none&gt;          13d   v1.31.0
<span class=c># 还是报错</span>

<span class=nv>$ </span>systemctl status kubelet.service
● kubelet.service - kubelet: The Kubernetes Node Agent
     Loaded: loaded <span class=o>(</span>/lib/systemd/system/kubelet.service<span class=p>;</span> enabled<span class=p>;</span> vendor preset: enabled<span class=o>)</span>
    Drop-In: /usr/lib/systemd/system/kubelet.service.d
             └─10-kubeadm.conf
     Active: activating <span class=o>(</span>auto-restart<span class=o>)</span> <span class=o>(</span>Result: exit-code<span class=o>)</span> since Tue 2025-02-11 09:07:56 UTC<span class=p>;</span> 9s ago
       Docs: https://kubernetes.io/docs/
    Process: 5045 <span class=nv>ExecStart</span><span class=o>=</span>/usr/bin/kubelet <span class=nv>$KUBELET_KUBECONFIG_ARGS</span> <span class=nv>$KUBELET_CONFIG_ARGS</span> <span class=nv>$KUBELET_KUBEADM_ARGS</span> <span class=nv>$KUBELET_EXTRA_ARGS</span> <span class=o>(</span><span class=nv>code</span><span class=o>=</span>exited&gt;
   Main PID: 5045 <span class=o>(</span><span class=nv>code</span><span class=o>=</span>exited, <span class=nv>status</span><span class=o>=</span>1/FAILURE<span class=o>)</span>
Feb 11 09:07:56 controlplane systemd[1]: kubelet.service: Main process exited, <span class=nv>code</span><span class=o>=</span>exited, <span class=nv>status</span><span class=o>=</span>1/FAILURE
Feb 11 09:07:56 controlplane systemd[1]: kubelet.service: Failed with result <span class=s1>&#39;exit-code&#39;</span><span class=nb>.</span>


<span class=nv>$ </span>journalctl <span class=nt>-u</span> kubelet <span class=nt>-f</span>
Feb 11 09:37:31 controlplane systemd[1]: kubelet.service: Scheduled restart job, restart counter is at 181.
Feb 11 09:37:31 controlplane systemd[1]: Stopped kubelet: The Kubernetes Node Agent.
Feb 11 09:37:31 controlplane systemd[1]: Started kubelet: The Kubernetes Node Agent.
Feb 11 09:37:31 controlplane kubelet[12741]: Flag <span class=nt>--container-runtime-endpoint</span> has been deprecated, This parameter should be <span class=nb>set </span>via the config file specified by the Kubelet<span class=s1>&#39;s --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Feb 11 09:37:31 controlplane kubelet[12741]: Flag --pod-infra-container-image has been deprecated, will be removed in a future release. Image garbage collector will get sandbox image information from CRI.
Feb 11 09:37:31 controlplane kubelet[12741]: Flag --container-runtime-endpoint has been deprecated, This parameter should be set via the config file specified by the Kubelet&#39;</span>s <span class=nt>--config</span> flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ <span class=k>for </span>more information.
Feb 11 09:37:31 controlplane kubelet[12741]: Flag <span class=nt>--cgroup-driver</span> has been deprecated, This parameter should be <span class=nb>set </span>via the config file specified by the Kubelet<span class=s1>&#39;s --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Feb 11 09:37:31 controlplane kubelet[12741]: Flag --eviction-hard has been deprecated, This parameter should be set via the config file specified by the Kubelet&#39;</span>s <span class=nt>--config</span> flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ <span class=k>for </span>more information.
Feb 11 09:37:31 controlplane kubelet[12741]: Flag <span class=nt>--fail-swap-on</span> has been deprecated, This parameter should be <span class=nb>set </span>via the config file specified by the Kubelet<span class=s1>&#39;s --config flag. See https://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/ for more information.
Feb 11 09:37:31 controlplane kubelet[12741]: I0211 09:37:31.856566   12741 server.go:206] &#34;--pod-infra-container-image will not be pruned by the image garbage collector in kubelet and should also be set in the remote runtime&#34;
Feb 11 09:37:31 controlplane kubelet[12741]: E0211 09:37:31.860454   12741 run.go:72] &#34;command failed&#34; err=&#34;failed to construct kubelet dependencies: unable to load client CA file /etc/kubernetes/pki/CA.CERTIFICATE: open /etc/kubernetes/pki/CA.CERTIFICATE: no such file or directory&#34;
Feb 11 09:37:31 controlplane systemd[1]: kubelet.service: Main process exited, code=exited, status=1/FAILURE
Feb 11 09:37:31 controlplane systemd[1]: kubelet.service: Failed with result &#39;</span>exit-code<span class=s1>&#39;.

# 从这个日志上来看，/etc/kubernetes/pki/CA.CERTIFICATE 文件不存在

$ ll /etc/kubernetes/pki
total 68
drwxr-xr-x 3 root root 4096 Jan 28 16:04 ./
drwxrwxr-x 4 root root 4096 Feb 11 09:35 ../
-rw-r--r-- 1 root root 1123 Jan 28 16:04 apiserver-etcd-client.crt
-rw------- 1 root root 1679 Jan 28 16:04 apiserver-etcd-client.key
-rw-r--r-- 1 root root 1176 Jan 28 16:04 apiserver-kubelet-client.crt
-rw------- 1 root root 1675 Jan 28 16:04 apiserver-kubelet-client.key
-rw-r--r-- 1 root root 1289 Jan 28 16:04 apiserver.crt
-rw------- 1 root root 1679 Jan 28 16:04 apiserver.key
-rw-r--r-- 1 root root 1107 Jan 28 16:04 ca.crt
-rw------- 1 root root 1675 Jan 28 16:04 ca.key
drwxr-xr-x 2 root root 4096 Jan 28 16:04 etcd/
-rw-r--r-- 1 root root 1123 Jan 28 16:04 front-proxy-ca.crt
-rw------- 1 root root 1679 Jan 28 16:04 front-proxy-ca.key
-rw-r--r-- 1 root root 1119 Jan 28 16:04 front-proxy-client.crt
-rw------- 1 root root 1675 Jan 28 16:04 front-proxy-client.key
-rw------- 1 root root 1675 Jan 28 16:04 sa.key
-rw------- 1 root root  451 Jan 28 16:04 sa.pub
# 通过查看目录下的文件，应该是 ca.crt

$ vim /var/lib/kubelet/config.yaml
# 修改证书文件名

$ systemctl restart kubelet.service

$ systemctl status kubelet.service
● kubelet.service - kubelet: The Kubernetes Node Agent
     Loaded: loaded (/lib/systemd/system/kubelet.service; enabled; vendor preset: enabled)
    Drop-In: /usr/lib/systemd/system/kubelet.service.d
             └─10-kubeadm.conf
     Active: active (running) since Tue 2025-02-11 09:45:36 UTC; 7s ago
       Docs: https://kubernetes.io/docs/
   Main PID: 15312 (kubelet)
      Tasks: 8 (limit: 2338)
     Memory: 27.6M
     CGroup: /system.slice/kubelet.service
             └─15312 /usr/bin/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --kubeconfig=/etc/kubernetes/kubelet.conf --config=/var/lib/kubelet/config.&gt;
Feb 11 09:45:37 controlplane kubelet[15312]: I0211 09:45:37.459777   15312 reconciler_common.go:245] &#34;operationExecutor.VerifyControllerAttachedVolume started for volume \&#34;sys&gt;
Feb 11 09:45:37 controlplane kubelet[15312]: I0211 09:45:37.459923   15312 reconciler_common.go:245] &#34;operationExecutor.VerifyControllerAttachedVolume started for volume \&#34;cni&gt;
Feb 11 09:45:37 controlplane kubelet[15312]: I0211 09:45:37.460253   15312 reconciler_common.go:245] &#34;operationExecutor.VerifyControllerAttachedVolume started for volume \&#34;lib&gt;
Feb 11 09:45:37 controlplane kubelet[15312]: I0211 09:45:37.460475   15312 reconciler_common.go:245] &#34;operationExecutor.VerifyControllerAttachedVolume started for volume \&#34;cni&gt;
Feb 11 09:45:37 controlplane kubelet[15312]: I0211 09:45:37.460620   15312 reconciler_common.go:245] &#34;operationExecutor.VerifyControllerAttachedVolume started for volume \&#34;cni&gt;
Feb 11 09:45:37 controlplane kubelet[15312]: I0211 09:45:37.460856   15312 reconciler_common.go:245] &#34;operationExecutor.VerifyControllerAttachedVolume started for volume \&#34;xta&gt;
Feb 11 09:45:37 controlplane kubelet[15312]: I0211 09:45:37.461044   15312 reconciler_common.go:245] &#34;operationExecutor.VerifyControllerAttachedVolume started for volume \&#34;xta&gt;
Feb 11 09:45:37 controlplane kubelet[15312]: I0211 09:45:37.461238   15312 reconciler_common.go:245] &#34;operationExecutor.VerifyControllerAttachedVolume started for volume \&#34;pol&gt;
Feb 11 09:45:37 controlplane kubelet[15312]: I0211 09:45:37.461403   15312 reconciler_common.go:245] &#34;operationExecutor.VerifyControllerAttachedVolume started for volume \&#34;nod&gt;
Feb 11 09:45:37 controlplane kubelet[15312]: I0211 09:45:37.461545   15312 reconciler_common.go:245] &#34;operationExecutor.VerifyControllerAttachedVolume started for volume \&#34;var&gt;

$ kubectl get nodes
NAME           STATUS   ROLES           AGE   VERSION
controlplane   Ready    control-plane   13d   v1.31.0
node01         Ready    &lt;none&gt;          13d   v1.31.0

$ kubectl get pod -A
NAMESPACE            NAME                                      READY   STATUS    RESTARTS      AGE
kube-system          calico-kube-controllers-94fb6bc47-tlwdz   1/1     Running   0             34m
kube-system          canal-phldr                               2/2     Running   2 (42m ago)   13d
kube-system          canal-zl4tq                               2/2     Running   2 (42m ago)   13d
kube-system          coredns-57888bfdc7-685jj                  1/1     Running   1 (42m ago)   13d
kube-system          coredns-57888bfdc7-bbwzr                  1/1     Running   1 (42m ago)   13d
kube-system          etcd-controlplane                         1/1     Running   2 (42m ago)   13d
kube-system          kube-apiserver-controlplane               1/1     Running   2 (42m ago)   13d
kube-system          kube-controller-manager-controlplane      1/1     Running   2 (42m ago)   13d
kube-system          kube-proxy-2mfwz                          1/1     Running   2 (42m ago)   13d
kube-system          kube-proxy-z2ps8                          1/1     Running   1 (42m ago)   13d
kube-system          kube-scheduler-controlplane               1/1     Running   2 (42m ago)   13d
local-path-storage   local-path-provisioner-6c5cff8948-2hvr9   1/1     Running   0             34m</span></code></pre></div></div></div></div><div class=sect1><h2 id=_troubleshooting_node_not_ready>7. Troubleshooting - Node Not Ready</h2><div class=sectionbody><div class=paragraph><p><a href=https://killercoda.com/sachin/course/CKA/node-notready target=_blank rel=noopener>Troubleshooting - Node Not Ready</a></p></div><div class=sidebarblock><div class=content><div class=paragraph><p><code>kubelet</code> service not running in <code>controlplane</code>, it will cause the <code>controlplane</code> in <code>NotReady</code> state, so fix this issue</p></div></div></div><div class=listingblock><div class=content><pre class="rouge highlight nowrap"><code data-lang=bash><span class=c># @author D瓜哥 · <a href=https://www.diguage.com target=_blank>https://www.diguage.com</a></span>

<span class=nv>$ </span>kubectl get nodes
NAME           STATUS     ROLES           AGE   VERSION
controlplane   NotReady   control-plane   13d   v1.31.0
node01         Ready      &lt;none&gt;          13d   v1.31.0

<span class=nv>$ </span>kubectl describe nodes controlplane
Name:               controlplane
Roles:              control-plane
Labels:             beta.kubernetes.io/arch<span class=o>=</span>amd64
                    beta.kubernetes.io/os<span class=o>=</span>linux
                    kubernetes.io/arch<span class=o>=</span>amd64
                    kubernetes.io/hostname<span class=o>=</span>controlplane
                    kubernetes.io/os<span class=o>=</span>linux
                    node-role.kubernetes.io/control-plane<span class=o>=</span>
                    node.kubernetes.io/exclude-from-external-load-balancers<span class=o>=</span>
Annotations:        flannel.alpha.coreos.com/backend-data: <span class=o>{</span><span class=s2>&#34;VNI&#34;</span>:1,<span class=s2>&#34;VtepMAC&#34;</span>:<span class=s2>&#34;b6:8d:41:43:0d:65&#34;</span><span class=o>}</span>
                    flannel.alpha.coreos.com/backend-type: vxlan
                    flannel.alpha.coreos.com/kube-subnet-manager: <span class=nb>true
                    </span>flannel.alpha.coreos.com/public-ip: 172.30.1.2
                    kubeadm.alpha.kubernetes.io/cri-socket: unix:///var/run/containerd/containerd.sock
                    node.alpha.kubernetes.io/ttl: 0
                    projectcalico.org/IPv4Address: 172.30.1.2/24
                    projectcalico.org/IPv4IPIPTunnelAddr: 192.168.0.1
                    volumes.kubernetes.io/controller-managed-attach-detach: <span class=nb>true
</span>CreationTimestamp:  Tue, 28 Jan 2025 16:04:13 +0000
Taints:             node.kubernetes.io/unreachable:NoExecute
                    node-role.kubernetes.io/control-plane:NoSchedule
                    node.kubernetes.io/unreachable:NoSchedule
Unschedulable:      <span class=nb>false
</span>Lease:
  HolderIdentity:  controlplane
  AcquireTime:     &lt;<span class=nb>unset</span><span class=o>&gt;</span>
  RenewTime:       Tue, 11 Feb 2025 11:30:37 +0000
Conditions:
  Type                 Status    LastHeartbeatTime                 LastTransitionTime                Reason              Message
  <span class=nt>----</span>                 <span class=nt>------</span>    <span class=nt>-----------------</span>                 <span class=nt>------------------</span>                <span class=nt>------</span>              <span class=nt>-------</span>
  NetworkUnavailable   False     Tue, 11 Feb 2025 11:27:57 +0000   Tue, 11 Feb 2025 11:27:57 +0000   FlannelIsUp         Flannel is running on this node
  MemoryPressure       Unknown   Tue, 11 Feb 2025 11:27:34 +0000   Tue, 11 Feb 2025 11:31:20 +0000   NodeStatusUnknown   Kubelet stopped posting node status.
  DiskPressure         Unknown   Tue, 11 Feb 2025 11:27:34 +0000   Tue, 11 Feb 2025 11:31:20 +0000   NodeStatusUnknown   Kubelet stopped posting node status.
  PIDPressure          Unknown   Tue, 11 Feb 2025 11:27:34 +0000   Tue, 11 Feb 2025 11:31:20 +0000   NodeStatusUnknown   Kubelet stopped posting node status.
  Ready                Unknown   Tue, 11 Feb 2025 11:27:34 +0000   Tue, 11 Feb 2025 11:31:20 +0000   NodeStatusUnknown   Kubelet stopped posting node status.
Addresses:
  InternalIP:  172.30.1.2
  Hostname:    controlplane
Capacity:
  cpu:                1
  ephemeral-storage:  20134592Ki
  hugepages-2Mi:      0
  memory:             2030940Ki
  pods:               110
Allocatable:
  cpu:                1
  ephemeral-storage:  19586931083
  hugepages-2Mi:      0
  memory:             1928540Ki
  pods:               110
System Info:
  Machine ID:                 388a2d0f867a4404bc12a0093bd9ed8d
  System UUID:                48fb2326-9ee6-42f1-a8a3-a7b96c40e292
  Boot ID:                    4076bb40-dbc3-4306-ad96-6e3ce0b2aecf
  Kernel Version:             5.4.0-131-generic
  OS Image:                   Ubuntu 20.04.5 LTS
  Operating System:           linux
  Architecture:               amd64
  Container Runtime Version:  containerd://1.7.22
  Kubelet Version:            v1.31.0
  Kube-Proxy Version:
PodCIDR:                      192.168.0.0/24
PodCIDRs:                     192.168.0.0/24
Non-terminated Pods:          <span class=o>(</span>8 <span class=k>in </span>total<span class=o>)</span>
  Namespace                   Name                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age
  <span class=nt>---------</span>                   <span class=nt>----</span>                                       <span class=nt>------------</span>  <span class=nt>----------</span>  <span class=nt>---------------</span>  <span class=nt>-------------</span>  <span class=nt>---</span>
  kube-system                 calico-kube-controllers-94fb6bc47-rxh7x    0 <span class=o>(</span>0%<span class=o>)</span>        0 <span class=o>(</span>0%<span class=o>)</span>      0 <span class=o>(</span>0%<span class=o>)</span>           0 <span class=o>(</span>0%<span class=o>)</span>         13d
  kube-system                 canal-zl4tq                                25m <span class=o>(</span>2%<span class=o>)</span>      0 <span class=o>(</span>0%<span class=o>)</span>      0 <span class=o>(</span>0%<span class=o>)</span>           0 <span class=o>(</span>0%<span class=o>)</span>         13d
  kube-system                 etcd-controlplane                          25m <span class=o>(</span>2%<span class=o>)</span>      0 <span class=o>(</span>0%<span class=o>)</span>      100Mi <span class=o>(</span>5%<span class=o>)</span>       0 <span class=o>(</span>0%<span class=o>)</span>         13d
  kube-system                 kube-apiserver-controlplane                50m <span class=o>(</span>5%<span class=o>)</span>      0 <span class=o>(</span>0%<span class=o>)</span>      0 <span class=o>(</span>0%<span class=o>)</span>           0 <span class=o>(</span>0%<span class=o>)</span>         13d
  kube-system                 kube-controller-manager-controlplane       25m <span class=o>(</span>2%<span class=o>)</span>      0 <span class=o>(</span>0%<span class=o>)</span>      0 <span class=o>(</span>0%<span class=o>)</span>           0 <span class=o>(</span>0%<span class=o>)</span>         13d
  kube-system                 kube-proxy-2mfwz                           0 <span class=o>(</span>0%<span class=o>)</span>        0 <span class=o>(</span>0%<span class=o>)</span>      0 <span class=o>(</span>0%<span class=o>)</span>           0 <span class=o>(</span>0%<span class=o>)</span>         13d
  kube-system                 kube-scheduler-controlplane                25m <span class=o>(</span>2%<span class=o>)</span>      0 <span class=o>(</span>0%<span class=o>)</span>      0 <span class=o>(</span>0%<span class=o>)</span>           0 <span class=o>(</span>0%<span class=o>)</span>         13d
  local-path-storage          local-path-provisioner-6c5cff8948-2x89z    0 <span class=o>(</span>0%<span class=o>)</span>        0 <span class=o>(</span>0%<span class=o>)</span>      0 <span class=o>(</span>0%<span class=o>)</span>           0 <span class=o>(</span>0%<span class=o>)</span>         13d
Allocated resources:
  <span class=o>(</span>Total limits may be over 100 percent, i.e., overcommitted.<span class=o>)</span>
  Resource           Requests    Limits
  <span class=nt>--------</span>           <span class=nt>--------</span>    <span class=nt>------</span>
  cpu                150m <span class=o>(</span>15%<span class=o>)</span>  0 <span class=o>(</span>0%<span class=o>)</span>
  memory             100Mi <span class=o>(</span>5%<span class=o>)</span>  0 <span class=o>(</span>0%<span class=o>)</span>
  ephemeral-storage  0 <span class=o>(</span>0%<span class=o>)</span>      0 <span class=o>(</span>0%<span class=o>)</span>
  hugepages-2Mi      0 <span class=o>(</span>0%<span class=o>)</span>      0 <span class=o>(</span>0%<span class=o>)</span>
Events:
  Type     Reason                   Age                    From             Message
  <span class=nt>----</span>     <span class=nt>------</span>                   <span class=nt>----</span>                   <span class=nt>----</span>             <span class=nt>-------</span>
  Normal   Starting                 3m58s                  kube-proxy
  Normal   Starting                 13d                    kube-proxy
  Normal   Starting                 13d                    kube-proxy
  Normal   Starting                 13d                    kubelet          Starting kubelet.
  Warning  CgroupV1                 13d                    kubelet          Cgroup v1 support is <span class=k>in </span>maintenance mode, please migrate to Cgroup v2.
  Normal   NodeHasSufficientPID     13d                    kubelet          Node controlplane status is now: NodeHasSufficientPID
  Normal   NodeAllocatableEnforced  13d                    kubelet          Updated Node Allocatable limit across pods
  Normal   NodeHasSufficientMemory  13d                    kubelet          Node controlplane status is now: NodeHasSufficientMemory
  Normal   NodeHasNoDiskPressure    13d                    kubelet          Node controlplane status is now: NodeHasNoDiskPressure
  Normal   RegisteredNode           13d                    node-controller  Node controlplane event: Registered Node controlplane <span class=k>in </span>Controller
  Normal   NodeReady                13d                    kubelet          Node controlplane status is now: NodeReady
  Normal   RegisteredNode           13d                    node-controller  Node controlplane event: Registered Node controlplane <span class=k>in </span>Controller
  Normal   NodeAllocatableEnforced  13d                    kubelet          Updated Node Allocatable limit across pods
  Normal   Starting                 13d                    kubelet          Starting kubelet.
  Warning  CgroupV1                 13d                    kubelet          Cgroup v1 support is <span class=k>in </span>maintenance mode, please migrate to Cgroup v2.
  Normal   NodeHasSufficientMemory  13d <span class=o>(</span>x8 over 13d<span class=o>)</span>      kubelet          Node controlplane status is now: NodeHasSufficientMemory
  Normal   NodeHasNoDiskPressure    13d <span class=o>(</span>x7 over 13d<span class=o>)</span>      kubelet          Node controlplane status is now: NodeHasNoDiskPressure
  Normal   NodeHasSufficientPID     13d <span class=o>(</span>x7 over 13d<span class=o>)</span>      kubelet          Node controlplane status is now: NodeHasSufficientPID
  Normal   RegisteredNode           13d                    node-controller  Node controlplane event: Registered Node controlplane <span class=k>in </span>Controller
  Normal   Starting                 4m28s                  kubelet          Starting kubelet.
  Warning  CgroupV1                 4m28s                  kubelet          Cgroup v1 support is <span class=k>in </span>maintenance mode, please migrate to Cgroup v2.
  Normal   NodeAllocatableEnforced  4m27s                  kubelet          Updated Node Allocatable limit across pods
  Normal   NodeHasSufficientMemory  4m26s <span class=o>(</span>x8 over 4m27s<span class=o>)</span>  kubelet          Node controlplane status is now: NodeHasSufficientMemory
  Normal   NodeHasNoDiskPressure    4m26s <span class=o>(</span>x7 over 4m27s<span class=o>)</span>  kubelet          Node controlplane status is now: NodeHasNoDiskPressure
  Normal   NodeHasSufficientPID     4m26s <span class=o>(</span>x7 over 4m27s<span class=o>)</span>  kubelet          Node controlplane status is now: NodeHasSufficientPID
  Normal   RegisteredNode           3m44s                  node-controller  Node controlplane event: Registered Node controlplane <span class=k>in </span>Controller
  Normal   NodeNotReady             19s                    node-controller  Node controlplane status is now: NodeNotReady

<span class=nv>$ </span>kubectl get pod <span class=nt>-A</span>
NAMESPACE            NAME                                      READY   STATUS    RESTARTS        AGE
kube-system          calico-kube-controllers-94fb6bc47-rxh7x   1/1     Running   2 <span class=o>(</span>5m37s ago<span class=o>)</span>   13d
kube-system          canal-phldr                               2/2     Running   2 <span class=o>(</span>5m43s ago<span class=o>)</span>   13d
kube-system          canal-zl4tq                               2/2     Running   2 <span class=o>(</span>5m37s ago<span class=o>)</span>   13d
kube-system          coredns-57888bfdc7-685jj                  1/1     Running   1 <span class=o>(</span>5m43s ago<span class=o>)</span>   13d
kube-system          coredns-57888bfdc7-bbwzr                  1/1     Running   1 <span class=o>(</span>5m43s ago<span class=o>)</span>   13d
kube-system          etcd-controlplane                         1/1     Running   2 <span class=o>(</span>5m37s ago<span class=o>)</span>   13d
kube-system          kube-apiserver-controlplane               1/1     Running   2 <span class=o>(</span>5m37s ago<span class=o>)</span>   13d
kube-system          kube-controller-manager-controlplane      1/1     Running   2 <span class=o>(</span>5m37s ago<span class=o>)</span>   13d
kube-system          kube-proxy-2mfwz                          1/1     Running   2 <span class=o>(</span>5m37s ago<span class=o>)</span>   13d
kube-system          kube-proxy-z2ps8                          1/1     Running   1 <span class=o>(</span>5m43s ago<span class=o>)</span>   13d
kube-system          kube-scheduler-controlplane               1/1     Running   2 <span class=o>(</span>5m37s ago<span class=o>)</span>   13d
local-path-storage   local-path-provisioner-6c5cff8948-2x89z   1/1     Running   2 <span class=o>(</span>5m37s ago<span class=o>)</span>   13d

<span class=nv>$ </span>systemctl status kubelet.service
● kubelet.service - kubelet: The Kubernetes Node Agent
     Loaded: loaded <span class=o>(</span>/lib/systemd/system/kubelet.service<span class=p>;</span> enabled<span class=p>;</span> vendor preset: enabled<span class=o>)</span>
    Drop-In: /usr/lib/systemd/system/kubelet.service.d
             └─10-kubeadm.conf
     Active: inactive <span class=o>(</span>dead<span class=o>)</span> since Tue 2025-02-11 11:30:42 UTC<span class=p>;</span> 2min 31s ago
       Docs: https://kubernetes.io/docs/
    Process: 1559 <span class=nv>ExecStart</span><span class=o>=</span>/usr/bin/kubelet <span class=nv>$KUBELET_KUBECONFIG_ARGS</span> <span class=nv>$KUBELET_CONFIG_ARGS</span> <span class=nv>$KUBELET_KUBEADM_ARGS</span> <span class=nv>$KUBELET_EXTRA_ARGS</span> <span class=o>(</span><span class=nv>code</span><span class=o>=</span>exited, <span class=nv>status</span><span class=o>=</span>0/SUCCESS<span class=o>)</span>
   Main PID: 1559 <span class=o>(</span><span class=nv>code</span><span class=o>=</span>exited, <span class=nv>status</span><span class=o>=</span>0/SUCCESS<span class=o>)</span>

Feb 11 11:28:07 controlplane kubelet[1559]: E0211 11:28:07.227865    1559 kuberuntime_manager.go:1477] <span class=s2>&#34;Failed to stop sandbox&#34;</span> <span class=nv>podSandboxID</span><span class=o>={</span><span class=s2>&#34;Type&#34;</span>:<span class=s2>&#34;containerd&#34;</span>,<span class=s2>&#34;ID&gt;
Feb 11 11:28:07 controlplane kubelet[1559]: E0211 11:28:07.230932    1559 log.go:32] &#34;</span>StopPodSandbox from runtime service failed<span class=s2>&#34; err=&#34;</span>rpc error: code <span class=o>=</span> Unknown desc&gt;
Feb 11 11:28:07 controlplane kubelet[1559]: E0211 11:28:07.231029    1559 kuberuntime_manager.go:1477] <span class=s2>&#34;Failed to stop sandbox&#34;</span> <span class=nv>podSandboxID</span><span class=o>={</span><span class=s2>&#34;Type&#34;</span>:<span class=s2>&#34;containerd&#34;</span>,<span class=s2>&#34;ID&gt;
Feb 11 11:28:07 controlplane kubelet[1559]: E0211 11:28:07.918666    1559 kuberuntime_manager.go:1077] &#34;</span>killPodWithSyncResult failed<span class=s2>&#34; err=&#34;</span>failed to <span class=se>\&#34;</span>KillPodSandbox&gt;
Feb 11 11:28:07 controlplane kubelet[1559]: E0211 11:28:07.922105    1559 pod_workers.go:1301] <span class=s2>&#34;Error syncing pod, skipping&#34;</span> <span class=nv>err</span><span class=o>=</span><span class=s2>&#34;failed to </span><span class=se>\&#34;</span><span class=s2>KillPodSandbox</span><span class=se>\&#34;</span><span class=s2> for </span><span class=se>\&#34;</span><span class=s2>&gt;
Feb 11 11:28:07 controlplane kubelet[1559]: E0211 11:28:07.936683    1559 kuberuntime_manager.go:1077] &#34;</span>killPodWithSyncResult failed<span class=s2>&#34; err=&#34;</span>failed to <span class=se>\&#34;</span>KillPodSandbox&gt;
Feb 11 11:28:07 controlplane kubelet[1559]: E0211 11:28:07.936746    1559 pod_workers.go:1301] <span class=s2>&#34;Error syncing pod, skipping&#34;</span> <span class=nv>err</span><span class=o>=</span><span class=s2>&#34;failed to </span><span class=se>\&#34;</span><span class=s2>KillPodSandbox</span><span class=se>\&#34;</span><span class=s2> for </span><span class=se>\&#34;</span><span class=s2>&gt;
Feb 11 11:30:42 controlplane systemd[1]: Stopping kubelet: The Kubernetes Node Agent...
Feb 11 11:30:42 controlplane systemd[1]: kubelet.service: Succeeded.
Feb 11 11:30:42 controlplane systemd[1]: Stopped kubelet: The Kubernetes Node Agent.

</span><span class=nv>$ </span><span class=s2>cat /usr/lib/systemd/system/kubelet.service.d/10-kubeadm.conf
# Note: This dropin only works with kubeadm and kubelet v1.11+
[Service]
Environment=&#34;</span><span class=nv>KUBELET_KUBECONFIG_ARGS</span><span class=o>=</span><span class=nt>--bootstrap-kubeconfig</span><span class=o>=</span>/etc/kubernetes/bootstrap-kubelet.conf <span class=nt>--kubeconfig</span><span class=o>=</span>/etc/kubernetes/kubelet.conf<span class=s2>&#34;
Environment=&#34;</span><span class=nv>KUBELET_CONFIG_ARGS</span><span class=o>=</span><span class=nt>--config</span><span class=o>=</span>/var/lib/kubelet/config.yaml<span class=s2>&#34;
# This is a file that &#34;</span>kubeadm init<span class=s2>&#34; and &#34;</span>kubeadm <span class=nb>join</span><span class=s2>&#34; generates at runtime, populating the KUBELET_KUBEADM_ARGS variable dynamically
EnvironmentFile=-/var/lib/kubelet/kubeadm-flags.env
# This is a file that the user can use for overrides of the kubelet args as a last resort. Preferably, the user should use
# the .NodeRegistration.KubeletExtraArgs object in the configuration files instead. KUBELET_EXTRA_ARGS should be sourced from this file.
EnvironmentFile=-/etc/default/kubelet
ExecStart=
ExecStart=/usr/bin/kubelet </span><span class=nv>$KUBELET_KUBECONFIG_ARGS</span><span class=s2> </span><span class=nv>$KUBELET_CONFIG_ARGS</span><span class=s2> </span><span class=nv>$KUBELET_KUBEADM_ARGS</span><span class=s2> </span><span class=nv>$KUBELET_EXTRA_ARGS</span><span class=s2>

</span><span class=err>$</span><span class=s2>

</span><span class=err>$</span><span class=s2>

</span><span class=nv>$ </span><span class=s2>cat /usr/lib/systemd/system/kubelet.service.d/10-kubeadm.conf
# Note: This dropin only works with kubeadm and kubelet v1.11+
[Service]
Environment=&#34;</span><span class=nv>KUBELET_KUBECONFIG_ARGS</span><span class=o>=</span><span class=nt>--bootstrap-kubeconfig</span><span class=o>=</span>/etc/kubernetes/bootstrap-kubelet.conf <span class=nt>--kubeconfig</span><span class=o>=</span>/etc/kubernetes/kubelet.conf<span class=s2>&#34;
Environment=&#34;</span><span class=nv>KUBELET_CONFIG_ARGS</span><span class=o>=</span><span class=nt>--config</span><span class=o>=</span>/var/lib/kubelet/config.yaml<span class=s2>&#34;
# This is a file that &#34;</span>kubeadm init<span class=s2>&#34; and &#34;</span>kubeadm <span class=nb>join</span><span class=s2>&#34; generates at runtime, populating the KUBELET_KUBEADM_ARGS variable dynamically
EnvironmentFile=-/var/lib/kubelet/kubeadm-flags.env
# This is a file that the user can use for overrides of the kubelet args as a last resort. Preferably, the user should use
# the .NodeRegistration.KubeletExtraArgs object in the configuration files instead. KUBELET_EXTRA_ARGS should be sourced from this file.
EnvironmentFile=-/etc/default/kubelet
ExecStart=
ExecStart=/usr/bin/kubelet </span><span class=nv>$KUBELET_KUBECONFIG_ARGS</span><span class=s2> </span><span class=nv>$KUBELET_CONFIG_ARGS</span><span class=s2> </span><span class=nv>$KUBELET_KUBEADM_ARGS</span><span class=s2> </span><span class=nv>$KUBELET_EXTRA_ARGS</span><span class=s2>

</span><span class=err>$</span><span class=s2>

</span><span class=err>$</span><span class=s2>

</span><span class=nv>$ </span><span class=s2>cat /etc/kubernetes/bootstrap-kubelet.conf
cat: /etc/kubernetes/bootstrap-kubelet.conf: No such file or directory

</span><span class=err>$</span><span class=s2>

</span><span class=nv>$ </span><span class=s2>cat /etc/kubernetes/kubelet.conf
apiVersion: v1
clusters:
- cluster:
    certificate-authority-data: 《证书内容》
    server: https://172.30.1.2:6443
  name: kubernetes
contexts:
- context:
    cluster: kubernetes
    user: system:node:controlplane
  name: system:node:controlplane@kubernetes
current-context: system:node:controlplane@kubernetes
kind: Config
preferences: {}
users:
- name: system:node:controlplane
  user:
    client-certificate: /var/lib/kubelet/pki/kubelet-client-current.pem
    client-key: /var/lib/kubelet/pki/kubelet-client-current.pem

</span><span class=err>$</span><span class=s2>

</span><span class=nv>$ </span><span class=s2>cat /var/lib/kubelet/config.yaml
apiVersion: kubelet.config.k8s.io/v1beta1
authentication:
  anonymous:
    enabled: false
  webhook:
    cacheTTL: 0s
    enabled: true
  x509:
    clientCAFile: /etc/kubernetes/pki/ca.crt
authorization:
  mode: Webhook
  webhook:
    cacheAuthorizedTTL: 0s
    cacheUnauthorizedTTL: 0s
cgroupDriver: systemd
clusterDNS:
- 10.96.0.10
clusterDomain: cluster.local
containerRuntimeEndpoint: &#34;&#34;
cpuManagerReconcilePeriod: 0s
evictionPressureTransitionPeriod: 0s
fileCheckFrequency: 0s
healthzBindAddress: 127.0.0.1
healthzPort: 10248
httpCheckFrequency: 0s
imageMaximumGCAge: 0s
imageMinimumGCAge: 0s
kind: KubeletConfiguration
logging:
  flushFrequency: 0
  options:
    json:
      infoBufferSize: &#34;</span>0<span class=s2>&#34;
    text:
      infoBufferSize: &#34;</span>0<span class=s2>&#34;
  verbosity: 0
memorySwap: {}
nodeStatusReportFrequency: 0s
nodeStatusUpdateFrequency: 0s
resolvConf: /run/systemd/resolve/resolv.conf
rotateCertificates: true
runtimeRequestTimeout: 0s
shutdownGracePeriod: 0s
shutdownGracePeriodCriticalPods: 0s
staticPodPath: /etc/kubernetes/manifests
streamingConnectionIdleTimeout: 0s
syncFrequency: 0s
volumeStatsAggPeriod: 0s

</span><span class=nv>$ </span><span class=s2>cat /run/systemd/resolve/resolv.conf
# This file is managed by man:systemd-resolved(8). Do not edit.
#
# This is a dynamic resolv.conf file for connecting local clients directly to
# all known uplink DNS servers. This file lists all configured search domains.
#
# Third party programs must not access this file directly, but only through the
# symlink at /etc/resolv.conf. To manage man:resolv.conf(5) in a different way,
# replace this symlink by a static file or a different symlink.
#
# See man:systemd-resolved.service(8) for details about the supported modes of
# operation for /etc/resolv.conf.

nameserver 8.8.8.8
nameserver 1.1.1.1



## 1、使用 kubectl 命令初步检查 ######################
</span><span class=nv>$ </span><span class=s2>kubectl get nodes -o wide
NAME           STATUS     ROLES           AGE   VERSION   INTERNAL-IP   EXTERNAL-IP   OS-IMAGE             KERNEL-VERSION      CONTAINER-RUNTIME
controlplane   NotReady   control-plane   13d   v1.31.0   172.30.1.2    &lt;none&gt;        Ubuntu 20.04.5 LTS   5.4.0-131-generic   containerd://1.7.22
node01         Ready      &lt;none&gt;          13d   v1.31.0   172.30.2.2    &lt;none&gt;        Ubuntu 20.04.5 LTS   5.4.0-131-generic   containerd://1.7.22

</span><span class=nv>$ </span><span class=s2>kubectl describe node controlplane
Name:               controlplane
Roles:              control-plane
Labels:             beta.kubernetes.io/arch=amd64
                    beta.kubernetes.io/os=linux
                    kubernetes.io/arch=amd64
                    kubernetes.io/hostname=controlplane
                    kubernetes.io/os=linux
                    node-role.kubernetes.io/control-plane=
                    node.kubernetes.io/exclude-from-external-load-balancers=
Annotations:        flannel.alpha.coreos.com/backend-data: {&#34;</span>VNI<span class=s2>&#34;:1,&#34;</span>VtepMAC<span class=s2>&#34;:&#34;</span>b6:8d:41:43:0d:65<span class=s2>&#34;}
                    flannel.alpha.coreos.com/backend-type: vxlan
                    flannel.alpha.coreos.com/kube-subnet-manager: true
                    flannel.alpha.coreos.com/public-ip: 172.30.1.2
                    kubeadm.alpha.kubernetes.io/cri-socket: unix:///var/run/containerd/containerd.sock
                    node.alpha.kubernetes.io/ttl: 0
                    projectcalico.org/IPv4Address: 172.30.1.2/24
                    projectcalico.org/IPv4IPIPTunnelAddr: 192.168.0.1
                    volumes.kubernetes.io/controller-managed-attach-detach: true
CreationTimestamp:  Tue, 28 Jan 2025 16:04:13 +0000
Taints:             node.kubernetes.io/unreachable:NoExecute
                    node-role.kubernetes.io/control-plane:NoSchedule
                    node.kubernetes.io/unreachable:NoSchedule
Unschedulable:      false
Lease:
  HolderIdentity:  controlplane
  AcquireTime:     &lt;unset&gt;
  RenewTime:       Tue, 11 Feb 2025 11:30:37 +0000
Conditions:
  Type                 Status    LastHeartbeatTime                 LastTransitionTime                Reason              Message
  ----                 ------    -----------------                 ------------------                ------              -------
  NetworkUnavailable   False     Tue, 11 Feb 2025 11:27:57 +0000   Tue, 11 Feb 2025 11:27:57 +0000   FlannelIsUp         Flannel is running on this node
  MemoryPressure       Unknown   Tue, 11 Feb 2025 11:27:34 +0000   Tue, 11 Feb 2025 11:31:20 +0000   NodeStatusUnknown   Kubelet stopped posting node status.
  DiskPressure         Unknown   Tue, 11 Feb 2025 11:27:34 +0000   Tue, 11 Feb 2025 11:31:20 +0000   NodeStatusUnknown   Kubelet stopped posting node status.
  PIDPressure          Unknown   Tue, 11 Feb 2025 11:27:34 +0000   Tue, 11 Feb 2025 11:31:20 +0000   NodeStatusUnknown   Kubelet stopped posting node status.
  Ready                Unknown   Tue, 11 Feb 2025 11:27:34 +0000   Tue, 11 Feb 2025 11:31:20 +0000   NodeStatusUnknown   Kubelet stopped posting node status.
Addresses:
  InternalIP:  172.30.1.2
  Hostname:    controlplane
Capacity:
  cpu:                1
  ephemeral-storage:  20134592Ki
  hugepages-2Mi:      0
  memory:             2030940Ki
  pods:               110
Allocatable:
  cpu:                1
  ephemeral-storage:  19586931083
  hugepages-2Mi:      0
  memory:             1928540Ki
  pods:               110
System Info:
  Machine ID:                 388a2d0f867a4404bc12a0093bd9ed8d
  System UUID:                48fb2326-9ee6-42f1-a8a3-a7b96c40e292
  Boot ID:                    4076bb40-dbc3-4306-ad96-6e3ce0b2aecf
  Kernel Version:             5.4.0-131-generic
  OS Image:                   Ubuntu 20.04.5 LTS
  Operating System:           linux
  Architecture:               amd64
  Container Runtime Version:  containerd://1.7.22
  Kubelet Version:            v1.31.0
  Kube-Proxy Version:
PodCIDR:                      192.168.0.0/24
PodCIDRs:                     192.168.0.0/24
Non-terminated Pods:          (8 in total)
  Namespace                   Name                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age
  ---------                   ----                                       ------------  ----------  ---------------  -------------  ---
  kube-system                 calico-kube-controllers-94fb6bc47-rxh7x    0 (0%)        0 (0%)      0 (0%)           0 (0%)         13d
  kube-system                 canal-zl4tq                                25m (2%)      0 (0%)      0 (0%)           0 (0%)         13d
  kube-system                 etcd-controlplane                          25m (2%)      0 (0%)      100Mi (5%)       0 (0%)         13d
  kube-system                 kube-apiserver-controlplane                50m (5%)      0 (0%)      0 (0%)           0 (0%)         13d
  kube-system                 kube-controller-manager-controlplane       25m (2%)      0 (0%)      0 (0%)           0 (0%)         13d
  kube-system                 kube-proxy-2mfwz                           0 (0%)        0 (0%)      0 (0%)           0 (0%)         13d
  kube-system                 kube-scheduler-controlplane                25m (2%)      0 (0%)      0 (0%)           0 (0%)         13d
  local-path-storage          local-path-provisioner-6c5cff8948-2x89z    0 (0%)        0 (0%)      0 (0%)           0 (0%)         13d
Allocated resources:
  (Total limits may be over 100 percent, i.e., overcommitted.)
  Resource           Requests    Limits
  --------           --------    ------
  cpu                150m (15%)  0 (0%)
  memory             100Mi (5%)  0 (0%)
  ephemeral-storage  0 (0%)      0 (0%)
  hugepages-2Mi      0 (0%)      0 (0%)
Events:
  Type     Reason                   Age                From             Message
  ----     ------                   ----               ----             -------
  Normal   Starting                 10m                kube-proxy
  Normal   Starting                 13d                kube-proxy
  Normal   Starting                 13d                kube-proxy
  Normal   Starting                 13d                kubelet          Starting kubelet.
  Warning  CgroupV1                 13d                kubelet          Cgroup v1 support is in maintenance mode, please migrate to Cgroup v2.
  Normal   NodeHasSufficientPID     13d                kubelet          Node controlplane status is now: NodeHasSufficientPID
  Normal   NodeAllocatableEnforced  13d                kubelet          Updated Node Allocatable limit across pods
  Normal   NodeHasSufficientMemory  13d                kubelet          Node controlplane status is now: NodeHasSufficientMemory
  Normal   NodeHasNoDiskPressure    13d                kubelet          Node controlplane status is now: NodeHasNoDiskPressure
  Normal   RegisteredNode           13d                node-controller  Node controlplane event: Registered Node controlplane in Controller
  Normal   NodeReady                13d                kubelet          Node controlplane status is now: NodeReady
  Normal   RegisteredNode           13d                node-controller  Node controlplane event: Registered Node controlplane in Controller
  Normal   NodeAllocatableEnforced  13d                kubelet          Updated Node Allocatable limit across pods
  Normal   Starting                 13d                kubelet          Starting kubelet.
  Warning  CgroupV1                 13d                kubelet          Cgroup v1 support is in maintenance mode, please migrate to Cgroup v2.
  Normal   NodeHasSufficientMemory  13d (x8 over 13d)  kubelet          Node controlplane status is now: NodeHasSufficientMemory
  Normal   NodeHasNoDiskPressure    13d (x7 over 13d)  kubelet          Node controlplane status is now: NodeHasNoDiskPressure
  Normal   NodeHasSufficientPID     13d (x7 over 13d)  kubelet          Node controlplane status is now: NodeHasSufficientPID
  Normal   RegisteredNode           13d                node-controller  Node controlplane event: Registered Node controlplane in Controller
  Normal   Starting                 10m                kubelet          Starting kubelet.
  Warning  CgroupV1                 10m                kubelet          Cgroup v1 support is in maintenance mode, please migrate to Cgroup v2.
  Normal   NodeAllocatableEnforced  10m                kubelet          Updated Node Allocatable limit across pods
  Normal   NodeHasSufficientMemory  10m (x8 over 10m)  kubelet          Node controlplane status is now: NodeHasSufficientMemory
  Normal   NodeHasNoDiskPressure    10m (x7 over 10m)  kubelet          Node controlplane status is now: NodeHasNoDiskPressure
  Normal   NodeHasSufficientPID     10m (x7 over 10m)  kubelet          Node controlplane status is now: NodeHasSufficientPID
  Normal   RegisteredNode           10m                node-controller  Node controlplane event: Registered Node controlplane in Controller
  Normal   NodeNotReady             6m44s              node-controller  Node controlplane status is now: NodeNotReady
# 检查 Events 部分
# 1、如果有 资源不足 (Insufficient CPU/Memory/Disk)，说明该节点的资源不足，Kubernetes 无法调度 Pod。
# 2、如果有 Network 相关错误，可能是 kube-proxy 或 CNI 网络插件的问题。
# 3、如果有 &#34;</span>PLEG is not healthy<span class=s2>&#34;，可能是容器运行时（Container Runtime）的问题。

## 2、检查 kubelet 进程状态 ######################
</span><span class=nv>$ </span><span class=s2>sudo  systemctl status kubelet.service
● kubelet.service - kubelet: The Kubernetes Node Agent
     Loaded: loaded (/lib/systemd/system/kubelet.service; enabled; vendor preset: enabled)
    Drop-In: /usr/lib/systemd/system/kubelet.service.d
             └─10-kubeadm.conf
     Active: inactive (dead) since Tue 2025-02-11 11:30:42 UTC; 12min ago
       Docs: https://kubernetes.io/docs/
    Process: 1559 ExecStart=/usr/bin/kubelet </span><span class=nv>$KUBELET_KUBECONFIG_ARGS</span><span class=s2> </span><span class=nv>$KUBELET_CONFIG_ARGS</span><span class=s2> </span><span class=nv>$KUBELET_KUBEADM_ARGS</span><span class=s2> </span><span class=nv>$KUBELET_EXTRA_ARGS</span><span class=s2> (code=exited, status=0/SUCCESS)
   Main PID: 1559 (code=exited, status=0/SUCCESS)
# 这里是 inactive 状态，说明 kubelet 服务没有正常运行
Feb 11 11:28:07 controlplane kubelet[1559]: E0211 11:28:07.227865    1559 kuberuntime_manager.go:1477] &#34;</span>Failed to stop sandbox<span class=s2>&#34; podSandboxID={&#34;</span>Type<span class=s2>&#34;:&#34;</span>containerd<span class=s2>&#34;,&#34;</span>ID<span class=s2>&#34;:&#34;</span>6887f95e0&gt;
Feb 11 11:28:07 controlplane kubelet[1559]: E0211 11:28:07.230932    1559 log.go:32] <span class=s2>&#34;StopPodSandbox from runtime service failed&#34;</span> <span class=nv>err</span><span class=o>=</span><span class=s2>&#34;rpc error: code = Unknown desc = failed to&gt;
Feb 11 11:28:07 controlplane kubelet[1559]: E0211 11:28:07.231029    1559 kuberuntime_manager.go:1477] &#34;</span>Failed to stop sandbox<span class=s2>&#34; podSandboxID={&#34;</span>Type<span class=s2>&#34;:&#34;</span>containerd<span class=s2>&#34;,&#34;</span>ID<span class=s2>&#34;:&#34;</span>08ce81ea2&gt;
Feb 11 11:28:07 controlplane kubelet[1559]: E0211 11:28:07.918666    1559 kuberuntime_manager.go:1077] <span class=s2>&#34;killPodWithSyncResult failed&#34;</span> <span class=nv>err</span><span class=o>=</span><span class=s2>&#34;failed to </span><span class=se>\&#34;</span><span class=s2>KillPodSandbox</span><span class=se>\&#34;</span><span class=s2> for </span><span class=se>\&#34;</span><span class=s2>0ff&gt;
Feb 11 11:28:07 controlplane kubelet[1559]: E0211 11:28:07.922105    1559 pod_workers.go:1301] &#34;</span>Error syncing pod, skipping<span class=s2>&#34; err=&#34;</span>failed to <span class=se>\&#34;</span>KillPodSandbox<span class=se>\&#34;</span> <span class=k>for</span> <span class=se>\&#34;</span>0ff4fb33-fde&gt;
Feb 11 11:28:07 controlplane kubelet[1559]: E0211 11:28:07.936683    1559 kuberuntime_manager.go:1077] <span class=s2>&#34;killPodWithSyncResult failed&#34;</span> <span class=nv>err</span><span class=o>=</span><span class=s2>&#34;failed to </span><span class=se>\&#34;</span><span class=s2>KillPodSandbox</span><span class=se>\&#34;</span><span class=s2> for </span><span class=se>\&#34;</span><span class=s2>a2c&gt;
Feb 11 11:28:07 controlplane kubelet[1559]: E0211 11:28:07.936746    1559 pod_workers.go:1301] &#34;</span>Error syncing pod, skipping<span class=s2>&#34; err=&#34;</span>failed to <span class=se>\&#34;</span>KillPodSandbox<span class=se>\&#34;</span> <span class=k>for</span> <span class=se>\&#34;</span>a2ca051a-3ce&gt;
Feb 11 11:30:42 controlplane systemd[1]: Stopping kubelet: The Kubernetes Node Agent...
Feb 11 11:30:42 controlplane systemd[1]: kubelet.service: Succeeded.
Feb 11 11:30:42 controlplane systemd[1]: Stopped kubelet: The Kubernetes Node Agent.

<span class=nv>$ </span><span class=nb>sudo </span>systemctl restart kubelet

<span class=nv>$ </span><span class=nb>sudo  </span>systemctl status kubelet.service
● kubelet.service - kubelet: The Kubernetes Node Agent
     Loaded: loaded <span class=o>(</span>/lib/systemd/system/kubelet.service<span class=p>;</span> enabled<span class=p>;</span> vendor preset: enabled<span class=o>)</span>
    Drop-In: /usr/lib/systemd/system/kubelet.service.d
             └─10-kubeadm.conf
     Active: active <span class=o>(</span>running<span class=o>)</span> since Tue 2025-02-11 11:43:47 UTC<span class=p>;</span> 11s ago
       Docs: https://kubernetes.io/docs/
   Main PID: 8049 <span class=o>(</span>kubelet<span class=o>)</span>
      Tasks: 10 <span class=o>(</span>limit: 2338<span class=o>)</span>
     Memory: 61.0M
     CGroup: /system.slice/kubelet.service
             └─8049 /usr/bin/kubelet <span class=nt>--bootstrap-kubeconfig</span><span class=o>=</span>/etc/kubernetes/bootstrap-kubelet.conf <span class=nt>--kubeconfig</span><span class=o>=</span>/etc/kubernetes/kubelet.conf <span class=nt>--config</span><span class=o>=</span>/var/lib/kubelet/config.yam&gt;

Feb 11 11:43:52 controlplane kubelet[8049]: I0211 11:43:52.556359    8049 reconciler_common.go:159] <span class=s2>&#34;operationExecutor.UnmountVolume started for volume </span><span class=se>\&#34;</span><span class=s2>kube-api-access-ffnhw</span><span class=se>\&#34;</span><span class=s2>&gt;
Feb 11 11:43:52 controlplane kubelet[8049]: I0211 11:43:52.556680    8049 reconciler_common.go:159] &#34;</span>operationExecutor.UnmountVolume started <span class=k>for </span>volume <span class=se>\&#34;</span>kube-api-access-76v2x<span class=se>\&#34;</span><span class=o>&gt;</span>
Feb 11 11:43:52 controlplane kubelet[8049]: I0211 11:43:52.563485    8049 operation_generator.go:803] UnmountVolume.TearDown succeeded <span class=k>for </span>volume <span class=s2>&#34;kubernetes.io/configmap/a2ca05&gt;
Feb 11 11:43:52 controlplane kubelet[8049]: I0211 11:43:52.600291    8049 operation_generator.go:803] UnmountVolume.TearDown succeeded for volume &#34;</span>kubernetes.io/projected/a2ca05&gt;
Feb 11 11:43:52 controlplane kubelet[8049]: I0211 11:43:52.610885    8049 operation_generator.go:803] UnmountVolume.TearDown succeeded <span class=k>for </span>volume <span class=s2>&#34;kubernetes.io/projected/0ff4fb&gt;
Feb 11 11:43:52 controlplane kubelet[8049]: I0211 11:43:52.663089    8049 reconciler_common.go:288] &#34;</span>Volume detached <span class=k>for </span>volume <span class=se>\&#34;</span>config-volume<span class=se>\&#34;</span> <span class=o>(</span>UniqueName: <span class=se>\&#34;</span>kubernetes.io/co&gt;
Feb 11 11:43:52 controlplane kubelet[8049]: I0211 11:43:52.665072    8049 reconciler_common.go:288] <span class=s2>&#34;Volume detached for volume </span><span class=se>\&#34;</span><span class=s2>kube-api-access-ffnhw</span><span class=se>\&#34;</span><span class=s2> (UniqueName: </span><span class=se>\&#34;</span><span class=s2>kubernet&gt;
Feb 11 11:43:52 controlplane kubelet[8049]: I0211 11:43:52.665494    8049 reconciler_common.go:288] &#34;</span>Volume detached <span class=k>for </span>volume <span class=se>\&#34;</span>kube-api-access-76v2x<span class=se>\&#34;</span> <span class=o>(</span>UniqueName: <span class=se>\&#34;</span>kubernet&gt;
Feb 11 11:43:54 controlplane kubelet[8049]: I0211 11:43:54.650014    8049 kubelet_volumes.go:163] <span class=s2>&#34;Cleaned up orphaned pod volumes dir&#34;</span> <span class=nv>podUID</span><span class=o>=</span><span class=s2>&#34;0ff4fb33-fde6-4587-ae22-a2a7e9ab5&gt;
Feb 11 11:43:54 controlplane kubelet[8049]: I0211 11:43:54.652099    8049 kubelet_volumes.go:163] &#34;</span>Cleaned up orphaned pod volumes <span class=nb>dir</span><span class=s2>&#34; podUID=&#34;</span>a2ca051a-3ced-4122-a2bc-622713bc1&gt;

<span class=nv>$ </span>journalctl <span class=nt>-u</span> kubelet <span class=nt>-f</span>
<span class=nt>--</span> Logs begin at Sun 2022-11-13 17:25:58 UTC. <span class=nt>--</span>
Feb 11 11:43:52 controlplane kubelet[8049]: I0211 11:43:52.556359    8049 reconciler_common.go:159] <span class=s2>&#34;operationExecutor.UnmountVolume started for volume </span><span class=se>\&#34;</span><span class=s2>kube-api-access-ffnhw</span><span class=se>\&#34;</span><span class=s2> (UniqueName: </span><span class=se>\&#34;</span><span class=s2>kubernetes.io/projected/0ff4fb33-fde6-4587-ae22-a2a7e9ab55e6-kube-api-access-ffnhw</span><span class=se>\&#34;</span><span class=s2>) pod </span><span class=se>\&#34;</span><span class=s2>0ff4fb33-fde6-4587-ae22-a2a7e9ab55e6</span><span class=se>\&#34;</span><span class=s2> (UID: </span><span class=se>\&#34;</span><span class=s2>0ff4fb33-fde6-4587-ae22-a2a7e9ab55e6</span><span class=se>\&#34;</span><span class=s2>) &#34;</span>
Feb 11 11:43:52 controlplane kubelet[8049]: I0211 11:43:52.556680    8049 reconciler_common.go:159] <span class=s2>&#34;operationExecutor.UnmountVolume started for volume </span><span class=se>\&#34;</span><span class=s2>kube-api-access-76v2x</span><span class=se>\&#34;</span><span class=s2> (UniqueName: </span><span class=se>\&#34;</span><span class=s2>kubernetes.io/projected/a2ca051a-3ced-4122-a2bc-622713bc16cf-kube-api-access-76v2x</span><span class=se>\&#34;</span><span class=s2>) pod </span><span class=se>\&#34;</span><span class=s2>a2ca051a-3ced-4122-a2bc-622713bc16cf</span><span class=se>\&#34;</span><span class=s2> (UID: </span><span class=se>\&#34;</span><span class=s2>a2ca051a-3ced-4122-a2bc-622713bc16cf</span><span class=se>\&#34;</span><span class=s2>) &#34;</span>
Feb 11 11:43:52 controlplane kubelet[8049]: I0211 11:43:52.563485    8049 operation_generator.go:803] UnmountVolume.TearDown succeeded <span class=k>for </span>volume <span class=s2>&#34;kubernetes.io/configmap/a2ca051a-3ced-4122-a2bc-622713bc16cf-config-volume&#34;</span> <span class=o>(</span>OuterVolumeSpecName: <span class=s2>&#34;config-volume&#34;</span><span class=o>)</span> pod <span class=s2>&#34;a2ca051a-3ced-4122-a2bc-622713bc16cf&#34;</span> <span class=o>(</span>UID: <span class=s2>&#34;a2ca051a-3ced-4122-a2bc-622713bc16cf&#34;</span><span class=o>)</span><span class=nb>.</span> InnerVolumeSpecName <span class=s2>&#34;config-volume&#34;</span><span class=nb>.</span> PluginName <span class=s2>&#34;kubernetes.io/configmap&#34;</span>, VolumeGidValue <span class=s2>&#34;&#34;</span>
Feb 11 11:43:52 controlplane kubelet[8049]: I0211 11:43:52.600291    8049 operation_generator.go:803] UnmountVolume.TearDown succeeded <span class=k>for </span>volume <span class=s2>&#34;kubernetes.io/projected/a2ca051a-3ced-4122-a2bc-622713bc16cf-kube-api-access-76v2x&#34;</span> <span class=o>(</span>OuterVolumeSpecName: <span class=s2>&#34;kube-api-access-76v2x&#34;</span><span class=o>)</span> pod <span class=s2>&#34;a2ca051a-3ced-4122-a2bc-622713bc16cf&#34;</span> <span class=o>(</span>UID: <span class=s2>&#34;a2ca051a-3ced-4122-a2bc-622713bc16cf&#34;</span><span class=o>)</span><span class=nb>.</span> InnerVolumeSpecName <span class=s2>&#34;kube-api-access-76v2x&#34;</span><span class=nb>.</span> PluginName <span class=s2>&#34;kubernetes.io/projected&#34;</span>, VolumeGidValue <span class=s2>&#34;&#34;</span>
Feb 11 11:43:52 controlplane kubelet[8049]: I0211 11:43:52.610885    8049 operation_generator.go:803] UnmountVolume.TearDown succeeded <span class=k>for </span>volume <span class=s2>&#34;kubernetes.io/projected/0ff4fb33-fde6-4587-ae22-a2a7e9ab55e6-kube-api-access-ffnhw&#34;</span> <span class=o>(</span>OuterVolumeSpecName: <span class=s2>&#34;kube-api-access-ffnhw&#34;</span><span class=o>)</span> pod <span class=s2>&#34;0ff4fb33-fde6-4587-ae22-a2a7e9ab55e6&#34;</span> <span class=o>(</span>UID: <span class=s2>&#34;0ff4fb33-fde6-4587-ae22-a2a7e9ab55e6&#34;</span><span class=o>)</span><span class=nb>.</span> InnerVolumeSpecName <span class=s2>&#34;kube-api-access-ffnhw&#34;</span><span class=nb>.</span> PluginName <span class=s2>&#34;kubernetes.io/projected&#34;</span>, VolumeGidValue <span class=s2>&#34;&#34;</span>
Feb 11 11:43:52 controlplane kubelet[8049]: I0211 11:43:52.663089    8049 reconciler_common.go:288] <span class=s2>&#34;Volume detached for volume </span><span class=se>\&#34;</span><span class=s2>config-volume</span><span class=se>\&#34;</span><span class=s2> (UniqueName: </span><span class=se>\&#34;</span><span class=s2>kubernetes.io/configmap/a2ca051a-3ced-4122-a2bc-622713bc16cf-config-volume</span><span class=se>\&#34;</span><span class=s2>) on node </span><span class=se>\&#34;</span><span class=s2>controlplane</span><span class=se>\&#34;</span><span class=s2> DevicePath </span><span class=se>\&#34;\&#34;</span><span class=s2>&#34;</span>
Feb 11 11:43:52 controlplane kubelet[8049]: I0211 11:43:52.665072    8049 reconciler_common.go:288] <span class=s2>&#34;Volume detached for volume </span><span class=se>\&#34;</span><span class=s2>kube-api-access-ffnhw</span><span class=se>\&#34;</span><span class=s2> (UniqueName: </span><span class=se>\&#34;</span><span class=s2>kubernetes.io/projected/0ff4fb33-fde6-4587-ae22-a2a7e9ab55e6-kube-api-access-ffnhw</span><span class=se>\&#34;</span><span class=s2>) on node </span><span class=se>\&#34;</span><span class=s2>controlplane</span><span class=se>\&#34;</span><span class=s2> DevicePath </span><span class=se>\&#34;\&#34;</span><span class=s2>&#34;</span>
Feb 11 11:43:52 controlplane kubelet[8049]: I0211 11:43:52.665494    8049 reconciler_common.go:288] <span class=s2>&#34;Volume detached for volume </span><span class=se>\&#34;</span><span class=s2>kube-api-access-76v2x</span><span class=se>\&#34;</span><span class=s2> (UniqueName: </span><span class=se>\&#34;</span><span class=s2>kubernetes.io/projected/a2ca051a-3ced-4122-a2bc-622713bc16cf-kube-api-access-76v2x</span><span class=se>\&#34;</span><span class=s2>) on node </span><span class=se>\&#34;</span><span class=s2>controlplane</span><span class=se>\&#34;</span><span class=s2> DevicePath </span><span class=se>\&#34;\&#34;</span><span class=s2>&#34;</span>
Feb 11 11:43:54 controlplane kubelet[8049]: I0211 11:43:54.650014    8049 kubelet_volumes.go:163] <span class=s2>&#34;Cleaned up orphaned pod volumes dir&#34;</span> <span class=nv>podUID</span><span class=o>=</span><span class=s2>&#34;0ff4fb33-fde6-4587-ae22-a2a7e9ab55e6&#34;</span> <span class=nv>path</span><span class=o>=</span><span class=s2>&#34;/var/lib/kubelet/pods/0ff4fb33-fde6-4587-ae22-a2a7e9ab55e6/volumes&#34;</span>
Feb 11 11:43:54 controlplane kubelet[8049]: I0211 11:43:54.652099    8049 kubelet_volumes.go:163] <span class=s2>&#34;Cleaned up orphaned pod volumes dir&#34;</span> <span class=nv>podUID</span><span class=o>=</span><span class=s2>&#34;a2ca051a-3ced-4122-a2bc-622713bc16cf&#34;</span> <span class=nv>path</span><span class=o>=</span><span class=s2>&#34;/var/lib/kubelet/pods/a2ca051a-3ced-4122-a2bc-622713bc16cf/volumes&#34;</span>

Feb 11 11:44:48 controlplane kubelet[8049]: I0211 11:44:48.706670    8049 scope.go:117] <span class=s2>&#34;RemoveContainer&#34;</span> <span class=nv>containerID</span><span class=o>=</span><span class=s2>&#34;853884d8d9eeb5edc8af3ada0dcd987419dd156d6f2e4f94f70c0dcd810bc821&#34;</span>
Feb 11 11:44:48 controlplane kubelet[8049]: I0211 11:44:48.731606    8049 scope.go:117] <span class=s2>&#34;RemoveContainer&#34;</span> <span class=nv>containerID</span><span class=o>=</span><span class=s2>&#34;af24e51735ac4c327f234fedae11265741f2094f733e2c0d41da384f93bff5a8&#34;</span>

<span class=c>## 3. 检查 kube-proxy 和网络状态 ######################</span>
<span class=nv>$ </span>kubectl get pods <span class=nt>-n</span> kube-system <span class=nt>-o</span> wide | <span class=nb>grep </span>kube-proxy
kube-proxy-2mfwz                          1/1     Running   2 <span class=o>(</span>20m ago<span class=o>)</span>   13d   172.30.1.2    controlplane   &lt;none&gt;           &lt;none&gt;
kube-proxy-z2ps8                          1/1     Running   1 <span class=o>(</span>20m ago<span class=o>)</span>   13d   172.30.2.2    node01         &lt;none&gt;           &lt;none&gt;

<span class=nv>$ </span>ip addr
1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000
    <span class=nb>link</span>/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
    inet6 ::1/128 scope host
       valid_lft forever preferred_lft forever
2: enp1s0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc fq_codel state UP group default qlen 1000
    <span class=nb>link</span>/ether 52:54:00:51:c9:d3 brd ff:ff:ff:ff:ff:ff
    inet 172.30.1.2/24 brd 172.30.1.255 scope global dynamic enp1s0
       valid_lft 86311336sec preferred_lft 86311336sec
3: docker0: &lt;NO-CARRIER,BROADCAST,MULTICAST,UP&gt; mtu 1454 qdisc noqueue state DOWN group default
    <span class=nb>link</span>/ether 02:42:d9:aa:43:b4 brd ff:ff:ff:ff:ff:ff
    inet 172.17.0.1/16 brd 172.17.255.255 scope global docker0
       valid_lft forever preferred_lft forever
4: flannel.1: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1450 qdisc noqueue state UNKNOWN group default
    <span class=nb>link</span>/ether b6:8d:41:43:0d:65 brd ff:ff:ff:ff:ff:ff
    inet 192.168.0.0/32 brd 192.168.0.0 scope global flannel.1
       valid_lft forever preferred_lft forever
    inet6 fe80::b48d:41ff:fe43:d65/64 scope <span class=nb>link
       </span>valid_lft forever preferred_lft forever

<span class=nv>$ </span><span class=nb>sudo </span>iptables <span class=nt>-L</span> <span class=nt>-v</span> <span class=nt>-n</span>
Chain INPUT <span class=o>(</span>policy ACCEPT 242K packets, 32M bytes<span class=o>)</span>
 pkts bytes target     prot opt <span class=k>in     </span>out     <span class=nb>source               </span>destination
 375K   56M cali-INPUT  all  <span class=nt>--</span>  <span class=k>*</span>      <span class=k>*</span>       0.0.0.0/0            0.0.0.0/0            /<span class=k>*</span> cali:Cz_u1IQiXIMmKD4c <span class=k>*</span>/
 4082  246K KUBE-PROXY-FIREWALL  all  <span class=nt>--</span>  <span class=k>*</span>      <span class=k>*</span>       0.0.0.0/0            0.0.0.0/0            ctstate NEW /<span class=k>*</span> kubernetes load balancer firewall <span class=k>*</span>/
 387K   78M KUBE-NODEPORTS  all  <span class=nt>--</span>  <span class=k>*</span>      <span class=k>*</span>       0.0.0.0/0            0.0.0.0/0            /<span class=k>*</span> kubernetes health check service ports <span class=k>*</span>/
 4083  247K KUBE-EXTERNAL-SERVICES  all  <span class=nt>--</span>  <span class=k>*</span>      <span class=k>*</span>       0.0.0.0/0            0.0.0.0/0            ctstate NEW /<span class=k>*</span> kubernetes externally-visible service portals <span class=k>*</span>/
 397K   91M KUBE-FIREWALL  all  <span class=nt>--</span>  <span class=k>*</span>      <span class=k>*</span>       0.0.0.0/0            0.0.0.0/0

Chain FORWARD <span class=o>(</span>policy ACCEPT 0 packets, 0 bytes<span class=o>)</span>
 pkts bytes target     prot opt <span class=k>in     </span>out     <span class=nb>source               </span>destination
    1    40 cali-FORWARD  all  <span class=nt>--</span>  <span class=k>*</span>      <span class=k>*</span>       0.0.0.0/0            0.0.0.0/0            /<span class=k>*</span> cali:wUHhoiAYhphO9Mso <span class=k>*</span>/
    0     0 KUBE-PROXY-FIREWALL  all  <span class=nt>--</span>  <span class=k>*</span>      <span class=k>*</span>       0.0.0.0/0            0.0.0.0/0            ctstate NEW /<span class=k>*</span> kubernetes load balancer firewall <span class=k>*</span>/
    0     0 KUBE-FORWARD  all  <span class=nt>--</span>  <span class=k>*</span>      <span class=k>*</span>       0.0.0.0/0            0.0.0.0/0            /<span class=k>*</span> kubernetes forwarding rules <span class=k>*</span>/
    0     0 KUBE-SERVICES  all  <span class=nt>--</span>  <span class=k>*</span>      <span class=k>*</span>       0.0.0.0/0            0.0.0.0/0            ctstate NEW /<span class=k>*</span> kubernetes service portals <span class=k>*</span>/
    0     0 KUBE-EXTERNAL-SERVICES  all  <span class=nt>--</span>  <span class=k>*</span>      <span class=k>*</span>       0.0.0.0/0            0.0.0.0/0            ctstate NEW /<span class=k>*</span> kubernetes externally-visible service portals <span class=k>*</span>/
    0     0 DOCKER-USER  all  <span class=nt>--</span>  <span class=k>*</span>      <span class=k>*</span>       0.0.0.0/0            0.0.0.0/0
    0     0 DOCKER-ISOLATION-STAGE-1  all  <span class=nt>--</span>  <span class=k>*</span>      <span class=k>*</span>       0.0.0.0/0            0.0.0.0/0
    0     0 ACCEPT     all  <span class=nt>--</span>  <span class=k>*</span>      docker0  0.0.0.0/0            0.0.0.0/0            ctstate RELATED,ESTABLISHED
    0     0 DOCKER     all  <span class=nt>--</span>  <span class=k>*</span>      docker0  0.0.0.0/0            0.0.0.0/0
    0     0 ACCEPT     all  <span class=nt>--</span>  docker0 <span class=o>!</span>docker0  0.0.0.0/0            0.0.0.0/0
    0     0 ACCEPT     all  <span class=nt>--</span>  docker0 docker0  0.0.0.0/0            0.0.0.0/0
    0     0 ACCEPT     all  <span class=nt>--</span>  <span class=k>*</span>      <span class=k>*</span>       192.168.0.0/16       0.0.0.0/0
    0     0 ACCEPT     all  <span class=nt>--</span>  <span class=k>*</span>      <span class=k>*</span>       0.0.0.0/0            192.168.0.0/16
    0     0 ACCEPT     all  <span class=nt>--</span>  <span class=k>*</span>      <span class=k>*</span>       0.0.0.0/0            0.0.0.0/0            /<span class=k>*</span> cali:S93hcgKJrXEqnTfs <span class=k>*</span>/ /<span class=k>*</span> Policy explicitly accepted packet. <span class=k>*</span>/ mark match 0x10000/0x10000
    0     0 MARK       all  <span class=nt>--</span>  <span class=k>*</span>      <span class=k>*</span>       0.0.0.0/0            0.0.0.0/0            /<span class=k>*</span> cali:mp77cMpurHhyjLrM <span class=k>*</span>/ MARK or 0x10000

Chain OUTPUT <span class=o>(</span>policy ACCEPT 241K packets, 35M bytes<span class=o>)</span>
 pkts bytes target     prot opt <span class=k>in     </span>out     <span class=nb>source               </span>destination
 374K   74M cali-OUTPUT  all  <span class=nt>--</span>  <span class=k>*</span>      <span class=k>*</span>       0.0.0.0/0            0.0.0.0/0            /<span class=k>*</span> cali:tVnHkvAo15HuiPy0 <span class=k>*</span>/
 4045  244K KUBE-PROXY-FIREWALL  all  <span class=nt>--</span>  <span class=k>*</span>      <span class=k>*</span>       0.0.0.0/0            0.0.0.0/0            ctstate NEW /<span class=k>*</span> kubernetes load balancer firewall <span class=k>*</span>/
 4045  244K KUBE-SERVICES  all  <span class=nt>--</span>  <span class=k>*</span>      <span class=k>*</span>       0.0.0.0/0            0.0.0.0/0            ctstate NEW /<span class=k>*</span> kubernetes service portals <span class=k>*</span>/
 397K   82M KUBE-FIREWALL  all  <span class=nt>--</span>  <span class=k>*</span>      <span class=k>*</span>       0.0.0.0/0            0.0.0.0/0

Chain DOCKER <span class=o>(</span>1 references<span class=o>)</span>
 pkts bytes target     prot opt <span class=k>in     </span>out     <span class=nb>source               </span>destination

Chain DOCKER-ISOLATION-STAGE-1 <span class=o>(</span>1 references<span class=o>)</span>
 pkts bytes target     prot opt <span class=k>in     </span>out     <span class=nb>source               </span>destination
    0     0 DOCKER-ISOLATION-STAGE-2  all  <span class=nt>--</span>  docker0 <span class=o>!</span>docker0  0.0.0.0/0            0.0.0.0/0
    0     0 RETURN     all  <span class=nt>--</span>  <span class=k>*</span>      <span class=k>*</span>       0.0.0.0/0            0.0.0.0/0

Chain DOCKER-ISOLATION-STAGE-2 <span class=o>(</span>1 references<span class=o>)</span>
 pkts bytes target     prot opt <span class=k>in     </span>out     <span class=nb>source               </span>destination
    0     0 DROP       all  <span class=nt>--</span>  <span class=k>*</span>      docker0  0.0.0.0/0            0.0.0.0/0
    0     0 RETURN     all  <span class=nt>--</span>  <span class=k>*</span>      <span class=k>*</span>       0.0.0.0/0            0.0.0.0/0

Chain DOCKER-USER <span class=o>(</span>1 references<span class=o>)</span>
 pkts bytes target     prot opt <span class=k>in     </span>out     <span class=nb>source               </span>destination
    0     0 RETURN     all  <span class=nt>--</span>  <span class=k>*</span>      <span class=k>*</span>       0.0.0.0/0            0.0.0.0/0

Chain KUBE-EXTERNAL-SERVICES <span class=o>(</span>2 references<span class=o>)</span>
 pkts bytes target     prot opt <span class=k>in     </span>out     <span class=nb>source               </span>destination

Chain KUBE-FIREWALL <span class=o>(</span>2 references<span class=o>)</span>
 pkts bytes target     prot opt <span class=k>in     </span>out     <span class=nb>source               </span>destination
    0     0 DROP       all  <span class=nt>--</span>  <span class=k>*</span>      <span class=k>*</span>      <span class=o>!</span>127.0.0.0/8          127.0.0.0/8          /<span class=k>*</span> block incoming localnet connections <span class=k>*</span>/ <span class=o>!</span> ctstate RELATED,ESTABLISHED,DNAT

Chain KUBE-FORWARD <span class=o>(</span>1 references<span class=o>)</span>
 pkts bytes target     prot opt <span class=k>in     </span>out     <span class=nb>source               </span>destination
    0     0 DROP       all  <span class=nt>--</span>  <span class=k>*</span>      <span class=k>*</span>       0.0.0.0/0            0.0.0.0/0            ctstate INVALID nfacct-name  ct_state_invalid_dropped_pkts
    0     0 ACCEPT     all  <span class=nt>--</span>  <span class=k>*</span>      <span class=k>*</span>       0.0.0.0/0            0.0.0.0/0            /<span class=k>*</span> kubernetes forwarding rules <span class=k>*</span>/ mark match 0x4000/0x4000
    0     0 ACCEPT     all  <span class=nt>--</span>  <span class=k>*</span>      <span class=k>*</span>       0.0.0.0/0            0.0.0.0/0            /<span class=k>*</span> kubernetes forwarding conntrack rule <span class=k>*</span>/ ctstate RELATED,ESTABLISHED

Chain KUBE-KUBELET-CANARY <span class=o>(</span>0 references<span class=o>)</span>
 pkts bytes target     prot opt <span class=k>in     </span>out     <span class=nb>source               </span>destination

Chain KUBE-NODEPORTS <span class=o>(</span>1 references<span class=o>)</span>
 pkts bytes target     prot opt <span class=k>in     </span>out     <span class=nb>source               </span>destination

Chain KUBE-PROXY-CANARY <span class=o>(</span>0 references<span class=o>)</span>
 pkts bytes target     prot opt <span class=k>in     </span>out     <span class=nb>source               </span>destination

Chain KUBE-PROXY-FIREWALL <span class=o>(</span>3 references<span class=o>)</span>
 pkts bytes target     prot opt <span class=k>in     </span>out     <span class=nb>source               </span>destination

Chain KUBE-SERVICES <span class=o>(</span>2 references<span class=o>)</span>
 pkts bytes target     prot opt <span class=k>in     </span>out     <span class=nb>source               </span>destination

Chain cali-FORWARD <span class=o>(</span>1 references<span class=o>)</span>
 pkts bytes target     prot opt <span class=k>in     </span>out     <span class=nb>source               </span>destination
    1    40 MARK       all  <span class=nt>--</span>  <span class=k>*</span>      <span class=k>*</span>       0.0.0.0/0            0.0.0.0/0            /<span class=k>*</span> cali:vjrMJCRpqwy5oRoX <span class=k>*</span>/ MARK and 0xfff1ffff
    1    40 cali-from-hep-forward  all  <span class=nt>--</span>  <span class=k>*</span>      <span class=k>*</span>       0.0.0.0/0            0.0.0.0/0            /<span class=k>*</span> cali:A_sPAO0mcxbT9mOV <span class=k>*</span>/ mark match 0x0/0x10000
    1    40 cali-from-wl-dispatch  all  <span class=nt>--</span>  cali+  <span class=k>*</span>       0.0.0.0/0            0.0.0.0/0            /<span class=k>*</span> cali:8ZoYfO5HKXWbB3pk <span class=k>*</span>/
    0     0 cali-to-wl-dispatch  all  <span class=nt>--</span>  <span class=k>*</span>      cali+   0.0.0.0/0            0.0.0.0/0            /<span class=k>*</span> cali:jdEuaPBe14V2hutn <span class=k>*</span>/
    0     0 cali-to-hep-forward  all  <span class=nt>--</span>  <span class=k>*</span>      <span class=k>*</span>       0.0.0.0/0            0.0.0.0/0            /<span class=k>*</span> cali:12bc6HljsMKsmfr- <span class=k>*</span>/
    0     0 cali-cidr-block  all  <span class=nt>--</span>  <span class=k>*</span>      <span class=k>*</span>       0.0.0.0/0            0.0.0.0/0            /<span class=k>*</span> cali:NOSxoaGx8OIstr1z <span class=k>*</span>/

Chain cali-INPUT <span class=o>(</span>1 references<span class=o>)</span>
 pkts bytes target     prot opt <span class=k>in     </span>out     <span class=nb>source               </span>destination
 2222  318K cali-wl-to-host  all  <span class=nt>--</span>  cali+  <span class=k>*</span>       0.0.0.0/0            0.0.0.0/0           <span class=o>[</span>goto]  /<span class=k>*</span> cali:FewJpBykm9iJ-YNH <span class=k>*</span>/
    0     0 ACCEPT     all  <span class=nt>--</span>  <span class=k>*</span>      <span class=k>*</span>       0.0.0.0/0            0.0.0.0/0            /<span class=k>*</span> cali:hder3ARWznqqv8Va <span class=k>*</span>/ mark match 0x10000/0x10000
 373K   55M MARK       all  <span class=nt>--</span>  <span class=k>*</span>      <span class=k>*</span>       0.0.0.0/0            0.0.0.0/0            /<span class=k>*</span> cali:xgOu2uJft6H9oDGF <span class=k>*</span>/ MARK and 0xfff0ffff
 373K   55M cali-from-host-endpoint  all  <span class=nt>--</span>  <span class=k>*</span>      <span class=k>*</span>       0.0.0.0/0            0.0.0.0/0            /<span class=k>*</span> cali:_-d-qojMfHM6NwBo <span class=k>*</span>/
    0     0 ACCEPT     all  <span class=nt>--</span>  <span class=k>*</span>      <span class=k>*</span>       0.0.0.0/0            0.0.0.0/0            /<span class=k>*</span> cali:LqmE76MP94lZTGhA <span class=k>*</span>/ /<span class=k>*</span> Host endpoint policy accepted packet. <span class=k>*</span>/ mark match 0x10000/0x10000

Chain cali-OUTPUT <span class=o>(</span>1 references<span class=o>)</span>
 pkts bytes target     prot opt <span class=k>in     </span>out     <span class=nb>source               </span>destination
    0     0 ACCEPT     all  <span class=nt>--</span>  <span class=k>*</span>      <span class=k>*</span>       0.0.0.0/0            0.0.0.0/0            /<span class=k>*</span> cali:Mq1_rAdXXH3YkrzW <span class=k>*</span>/ mark match 0x10000/0x10000
 2134  838K RETURN     all  <span class=nt>--</span>  <span class=k>*</span>      cali+   0.0.0.0/0            0.0.0.0/0            /<span class=k>*</span> cali:69FkRTJDvD5Vu6Vl <span class=k>*</span>/
 372K   73M MARK       all  <span class=nt>--</span>  <span class=k>*</span>      <span class=k>*</span>       0.0.0.0/0            0.0.0.0/0            /<span class=k>*</span> cali:Fskumj4SGQtDV6GC <span class=k>*</span>/ MARK and 0xfff0ffff
 370K   72M cali-to-host-endpoint  all  <span class=nt>--</span>  <span class=k>*</span>      <span class=k>*</span>       0.0.0.0/0            0.0.0.0/0            /<span class=k>*</span> cali:1F4VWEsQu0QbRwKf <span class=k>*</span>/ <span class=o>!</span> ctstate DNAT
    0     0 ACCEPT     all  <span class=nt>--</span>  <span class=k>*</span>      <span class=k>*</span>       0.0.0.0/0            0.0.0.0/0            /<span class=k>*</span> cali:m8Eqm15x1MjD24LD <span class=k>*</span>/ /<span class=k>*</span> Host endpoint policy accepted packet. <span class=k>*</span>/ mark match 0x10000/0x10000

Chain cali-cidr-block <span class=o>(</span>1 references<span class=o>)</span>
 pkts bytes target     prot opt <span class=k>in     </span>out     <span class=nb>source               </span>destination

Chain cali-from-hep-forward <span class=o>(</span>1 references<span class=o>)</span>
 pkts bytes target     prot opt <span class=k>in     </span>out     <span class=nb>source               </span>destination

Chain cali-from-host-endpoint <span class=o>(</span>1 references<span class=o>)</span>
 pkts bytes target     prot opt <span class=k>in     </span>out     <span class=nb>source               </span>destination

Chain cali-from-wl-dispatch <span class=o>(</span>2 references<span class=o>)</span>
 pkts bytes target     prot opt <span class=k>in     </span>out     <span class=nb>source               </span>destination
    0     0 DROP       all  <span class=nt>--</span>  <span class=k>*</span>      <span class=k>*</span>       0.0.0.0/0            0.0.0.0/0            /<span class=k>*</span> cali:zTj6P0TIgYvgz-md <span class=k>*</span>/ /<span class=k>*</span> Unknown interface <span class=k>*</span>/

Chain cali-to-hep-forward <span class=o>(</span>1 references<span class=o>)</span>
 pkts bytes target     prot opt <span class=k>in     </span>out     <span class=nb>source               </span>destination

Chain cali-to-host-endpoint <span class=o>(</span>1 references<span class=o>)</span>
 pkts bytes target     prot opt <span class=k>in     </span>out     <span class=nb>source               </span>destination

Chain cali-to-wl-dispatch <span class=o>(</span>1 references<span class=o>)</span>
 pkts bytes target     prot opt <span class=k>in     </span>out     <span class=nb>source               </span>destination
    0     0 DROP       all  <span class=nt>--</span>  <span class=k>*</span>      <span class=k>*</span>       0.0.0.0/0            0.0.0.0/0            /<span class=k>*</span> cali:7KNphB1nNHw80nIO <span class=k>*</span>/ /<span class=k>*</span> Unknown interface <span class=k>*</span>/

Chain cali-wl-to-host <span class=o>(</span>1 references<span class=o>)</span>
 pkts bytes target     prot opt <span class=k>in     </span>out     <span class=nb>source               </span>destination
 2222  318K cali-from-wl-dispatch  all  <span class=nt>--</span>  <span class=k>*</span>      <span class=k>*</span>       0.0.0.0/0            0.0.0.0/0            /<span class=k>*</span> cali:Ee9Sbo10IpVujdIY <span class=k>*</span>/
  102  6120 ACCEPT     all  <span class=nt>--</span>  <span class=k>*</span>      <span class=k>*</span>       0.0.0.0/0            0.0.0.0/0            /<span class=k>*</span> cali:nSZbcOoG1xPONxb8 <span class=k>*</span>/ /<span class=k>*</span> Configured DefaultEndpointToHostAction <span class=k>*</span>/

<span class=c>## 4. 检查 CNI 网络插件 ######################</span>
<span class=nv>$ </span><span class=nb>ls</span> /etc/cni/net.d/
10-canal.conflist  calico-kubeconfig

<span class=nv>$ </span>kubectl get pods <span class=nt>-n</span> kube-system | <span class=nb>grep</span> <span class=nt>-E</span> <span class=s1>&#39;calico|flannel|weave|cilium&#39;</span>
calico-kube-controllers-94fb6bc47-5f2hb   1/1     Running   0             31m

<span class=nv>$ </span>kubectl get pods <span class=nt>-n</span> kube-system | <span class=nb>grep </span>etcd
etcd-controlplane                         1/1     Running   2 <span class=o>(</span>41m ago<span class=o>)</span>   13d

<span class=c>## 5. 检查 etcd 状态（仅适用于 Control Plane） ######################</span>
<span class=nv>$ </span>kubectl get pods <span class=nt>-n</span> kube-system | <span class=nb>grep </span>etcd
etcd-controlplane                         1/1     Running   2 <span class=o>(</span>41m ago<span class=o>)</span>   13d

<span class=nv>$ </span>kubectl logs <span class=nt>-n</span> kube-system  etcd-controlplane
<span class=o>{</span><span class=s2>&#34;level&#34;</span>:<span class=s2>&#34;warn&#34;</span>,<span class=s2>&#34;ts&#34;</span>:<span class=s2>&#34;2025-02-11T11:27:56.420948Z&#34;</span>,<span class=s2>&#34;caller&#34;</span>:<span class=s2>&#34;etcdserver/util.go:170&#34;</span>,<span class=s2>&#34;msg&#34;</span>:<span class=s2>&#34;apply request took too long&#34;</span>,<span class=s2>&#34;took&#34;</span>:<span class=s2>&#34;315.642782ms&#34;</span>,<span class=s2>&#34;expected-duration&#34;</span>:<span class=s2>&#34;100ms&#34;</span>,<span class=s2>&#34;prefix&#34;</span>:<span class=s2>&#34;read-only range &#34;</span>,<span class=s2>&#34;request&#34;</span>:<span class=s2>&#34;key:</span><span class=se>\&#34;</span><span class=s2>/registry/crd.projectcalico.org/kubecontrollersconfigurations/</span><span class=se>\&#34;</span><span class=s2> range_end:</span><span class=se>\&#34;</span><span class=s2>/registry/crd.projectcalico.org/kubecontrollersconfigurations0</span><span class=se>\&#34;</span><span class=s2> count_only:true &#34;</span>,<span class=s2>&#34;response&#34;</span>:<span class=s2>&#34;range_response_count:0 size:7&#34;</span><span class=o>}</span>
<span class=o>{</span><span class=s2>&#34;level&#34;</span>:<span class=s2>&#34;info&#34;</span>,<span class=s2>&#34;ts&#34;</span>:<span class=s2>&#34;2025-02-11T11:37:24.009156Z&#34;</span>,<span class=s2>&#34;caller&#34;</span>:<span class=s2>&#34;mvcc/hash.go:137&#34;</span>,<span class=s2>&#34;msg&#34;</span>:<span class=s2>&#34;storing new hash&#34;</span>,<span class=s2>&#34;hash&#34;</span>:1297372836,<span class=s2>&#34;revision&#34;</span>:2391,<span class=s2>&#34;compact-revision&#34;</span>:-1<span class=o>}</span>
<span class=o>{</span><span class=s2>&#34;level&#34;</span>:<span class=s2>&#34;info&#34;</span>,<span class=s2>&#34;ts&#34;</span>:<span class=s2>&#34;2025-02-11T11:42:23.610574Z&#34;</span>,<span class=s2>&#34;caller&#34;</span>:<span class=s2>&#34;mvcc/index.go:214&#34;</span>,<span class=s2>&#34;msg&#34;</span>:<span class=s2>&#34;compact tree index&#34;</span>,<span class=s2>&#34;revision&#34;</span>:2822<span class=o>}</span>
<span class=c># 没发现什么有价值的线索</span>

<span class=nv>$ </span>kubectl get pod <span class=nt>-n</span> kube-system  etcd-controlplane <span class=nt>-o</span> yaml
apiVersion: v1
kind: Pod
metadata:
  annotations:
    kubeadm.kubernetes.io/etcd.advertise-client-urls: https://172.30.1.2:2379
    kubernetes.io/config.hash: 4fb3015641784f175e793600c1e22e8c
    kubernetes.io/config.mirror: 4fb3015641784f175e793600c1e22e8c
    kubernetes.io/config.seen: <span class=s2>&#34;2025-01-28T16:05:18.818121481Z&#34;</span>
    kubernetes.io/config.source: file
  creationTimestamp: <span class=s2>&#34;2025-01-28T16:05:48Z&#34;</span>
  labels:
    component: etcd
    tier: control-plane
  name: etcd-controlplane
  namespace: kube-system
  ownerReferences:
  - apiVersion: v1
    controller: <span class=nb>true
    </span>kind: Node
    name: controlplane
    uid: 52bb0db8-eeb9-48ee-8e38-a386487ad66e
  resourceVersion: <span class=s2>&#34;3392&#34;</span>
  uid: ec057033-29c2-4e99-b97e-5ffbcd859f07
spec:
  containers:
  - <span class=nb>command</span>:
    - etcd
    - <span class=nt>--advertise-client-urls</span><span class=o>=</span>https://172.30.1.2:2379
    - <span class=nt>--cert-file</span><span class=o>=</span>/etc/kubernetes/pki/etcd/server.crt
    - <span class=nt>--client-cert-auth</span><span class=o>=</span><span class=nb>true</span>
    - <span class=nt>--data-dir</span><span class=o>=</span>/var/lib/etcd
    - <span class=nt>--experimental-initial-corrupt-check</span><span class=o>=</span><span class=nb>true</span>
    - <span class=nt>--experimental-watch-progress-notify-interval</span><span class=o>=</span>5s
    - <span class=nt>--initial-advertise-peer-urls</span><span class=o>=</span>https://172.30.1.2:2380
    - <span class=nt>--initial-cluster</span><span class=o>=</span><span class=nv>controlplane</span><span class=o>=</span>https://172.30.1.2:2380
    - <span class=nt>--key-file</span><span class=o>=</span>/etc/kubernetes/pki/etcd/server.key
    - <span class=nt>--listen-client-urls</span><span class=o>=</span>https://127.0.0.1:2379,https://172.30.1.2:2379
    - <span class=nt>--listen-metrics-urls</span><span class=o>=</span>http://127.0.0.1:2381
    - <span class=nt>--listen-peer-urls</span><span class=o>=</span>https://172.30.1.2:2380
    - <span class=nt>--name</span><span class=o>=</span>controlplane
    - <span class=nt>--peer-cert-file</span><span class=o>=</span>/etc/kubernetes/pki/etcd/peer.crt
    - <span class=nt>--peer-client-cert-auth</span><span class=o>=</span><span class=nb>true</span>
    - <span class=nt>--peer-key-file</span><span class=o>=</span>/etc/kubernetes/pki/etcd/peer.key
    - <span class=nt>--peer-trusted-ca-file</span><span class=o>=</span>/etc/kubernetes/pki/etcd/ca.crt
    - <span class=nt>--snapshot-count</span><span class=o>=</span>10000
    - <span class=nt>--trusted-ca-file</span><span class=o>=</span>/etc/kubernetes/pki/etcd/ca.crt
    image: registry.k8s.io/etcd:3.5.15-0
    imagePullPolicy: IfNotPresent
    livenessProbe:
      failureThreshold: 8
      httpGet:
        host: 127.0.0.1
        path: /livez
        port: 2381
        scheme: HTTP
      initialDelaySeconds: 10
      periodSeconds: 10
      successThreshold: 1
      timeoutSeconds: 15
    name: etcd
    readinessProbe:
      failureThreshold: 3
      httpGet:
        host: 127.0.0.1
        path: /readyz
        port: 2381
        scheme: HTTP
      periodSeconds: 1
      successThreshold: 1
      timeoutSeconds: 15
    resources:
      requests:
        cpu: 25m
        memory: 100Mi
    startupProbe:
      failureThreshold: 24
      httpGet:
        host: 127.0.0.1
        path: /readyz
        port: 2381
        scheme: HTTP
      initialDelaySeconds: 10
      periodSeconds: 10
      successThreshold: 1
      timeoutSeconds: 15
    terminationMessagePath: /dev/termination-log
    terminationMessagePolicy: File
    volumeMounts:
    - mountPath: /var/lib/etcd
      name: etcd-data
    - mountPath: /etc/kubernetes/pki/etcd
      name: etcd-certs
  dnsPolicy: ClusterFirst
  enableServiceLinks: <span class=nb>true
  </span>hostNetwork: <span class=nb>true
  </span>nodeName: controlplane
  preemptionPolicy: PreemptLowerPriority
  priority: 2000001000
  priorityClassName: system-node-critical
  restartPolicy: Always
  schedulerName: default-scheduler
  securityContext:
    seccompProfile:
      <span class=nb>type</span>: RuntimeDefault
  terminationGracePeriodSeconds: 30
  tolerations:
  - effect: NoExecute
    operator: Exists
  volumes:
  - hostPath:
      path: /etc/kubernetes/pki/etcd
      <span class=nb>type</span>: DirectoryOrCreate
    name: etcd-certs
  - hostPath:
      path: /var/lib/etcd
      <span class=nb>type</span>: DirectoryOrCreate
    name: etcd-data
status:
  conditions:
  - lastProbeTime: null
    lastTransitionTime: <span class=s2>&#34;2025-02-11T11:43:49Z&#34;</span>
    status: <span class=s2>&#34;True&#34;</span>
    <span class=nb>type</span>: PodReadyToStartContainers
  - lastProbeTime: null
    lastTransitionTime: <span class=s2>&#34;2025-02-11T11:43:49Z&#34;</span>
    status: <span class=s2>&#34;True&#34;</span>
    <span class=nb>type</span>: Initialized
  - lastProbeTime: null
    lastTransitionTime: <span class=s2>&#34;2025-02-11T11:44:00Z&#34;</span>
    status: <span class=s2>&#34;True&#34;</span>
    <span class=nb>type</span>: Ready
  - lastProbeTime: null
    lastTransitionTime: <span class=s2>&#34;2025-02-11T11:44:00Z&#34;</span>
    status: <span class=s2>&#34;True&#34;</span>
    <span class=nb>type</span>: ContainersReady
  - lastProbeTime: null
    lastTransitionTime: <span class=s2>&#34;2025-02-11T11:43:49Z&#34;</span>
    status: <span class=s2>&#34;True&#34;</span>
    <span class=nb>type</span>: PodScheduled
  containerStatuses:
  - containerID: containerd://912d54b56de354794e7e58973102797721329dfbc16fdb25de4ff681410aff27
    image: registry.k8s.io/etcd:3.5.15-0
    imageID: registry.k8s.io/etcd@sha256:a6dc63e6e8cfa0307d7851762fa6b629afb18f28d8aa3fab5a6e91b4af60026a
    lastState:
      terminated:
        containerID: containerd://47e2c96d8ae02c2b675900f50a5f75f5c10fdcb50b18d20c64350bb6180fcb01
        exitCode: 255
        finishedAt: <span class=s2>&#34;2025-02-11T11:27:00Z&#34;</span>
        reason: Unknown
        startedAt: <span class=s2>&#34;2025-01-28T16:19:06Z&#34;</span>
    name: etcd
    ready: <span class=nb>true
    </span>restartCount: 2
    started: <span class=nb>true
    </span>state:
      running:
        startedAt: <span class=s2>&#34;2025-02-11T11:27:17Z&#34;</span>
  hostIP: 172.30.1.2
  hostIPs:
  - ip: 172.30.1.2
  phase: Running
  podIP: 172.30.1.2
  podIPs:
  - ip: 172.30.1.2
  qosClass: Burstable
  startTime: <span class=s2>&#34;2025-02-11T11:43:49Z&#34;</span>

<span class=nv>$ ETCDCTL_API</span><span class=o>=</span>3 etcdctl <span class=nt>--endpoints</span><span class=o>=</span>https://127.0.0.1:2379 endpoint status <span class=nt>--write-out</span><span class=o>=</span>table <span class=nt>--cert</span><span class=o>=</span>/etc/kubernetes/pki/etcd/server.crt <span class=nt>--key</span><span class=o>=</span>/etc/kubernetes/pki/etcd/server.key
<span class=o>{</span><span class=s2>&#34;level&#34;</span>:<span class=s2>&#34;warn&#34;</span>,<span class=s2>&#34;ts&#34;</span>:<span class=s2>&#34;2025-02-11T12:13:05.232Z&#34;</span>,<span class=s2>&#34;logger&#34;</span>:<span class=s2>&#34;etcd-client&#34;</span>,<span class=s2>&#34;caller&#34;</span>:<span class=s2>&#34;v3/retry_interceptor.go:62&#34;</span>,<span class=s2>&#34;msg&#34;</span>:<span class=s2>&#34;retrying of unary invoker failed&#34;</span>,<span class=s2>&#34;target&#34;</span>:<span class=s2>&#34;etcd-endpoints://0xc0002e0c40/#initially=[https://127.0.0.1:2379]&#34;</span>,<span class=s2>&#34;attempt&#34;</span>:0,<span class=s2>&#34;error&#34;</span>:<span class=s2>&#34;rpc error: code = DeadlineExceeded desc = latest balancer error: last connection error: connection error: desc = </span><span class=se>\&#34;</span><span class=s2>transport: authentication handshake failed: x509: certificate signed by unknown authority</span><span class=se>\&#34;</span><span class=s2>&#34;</span><span class=o>}</span>
Failed to get the status of endpoint https://127.0.0.1:2379 <span class=o>(</span>context deadline exceeded<span class=o>)</span>
+----------+----+---------+---------+-----------+------------+-----------+------------+--------------------+--------+
| ENDPOINT | ID | VERSION | DB SIZE | IS LEADER | IS LEARNER | RAFT TERM | RAFT INDEX | RAFT APPLIED INDEX | ERRORS |
+----------+----+---------+---------+-----------+------------+-----------+------------+--------------------+--------+
+----------+----+---------+---------+-----------+------------+-----------+------------+--------------------+--------+

<span class=nv>$ ETCDCTL_API</span><span class=o>=</span>3 etcdctl <span class=nt>--endpoints</span><span class=o>=</span>https://127.0.0.1:2379 endpoint status <span class=nt>--write-out</span><span class=o>=</span>table <span class=nt>--cert</span><span class=o>=</span>/etc/kubernetes/pki/etcd/server.crt <span class=nt>--key</span><span class=o>=</span>/etc/kubernetes/pki/etcd/server.key <span class=nt>--cacert</span><span class=o>=</span>/etc/kubernetes/pki/etcd/ca.crt  <span class=c># 增加了 CA 参数</span>
+------------------------+------------------+---------+---------+-----------+------------+-----------+------------+--------------------+--------+
|        ENDPOINT        |        ID        | VERSION | DB SIZE | IS LEADER | IS LEARNER | RAFT TERM | RAFT INDEX | RAFT APPLIED INDEX | ERRORS |
+------------------------+------------------+---------+---------+-----------+------------+-----------+------------+--------------------+--------+
| https://127.0.0.1:2379 | 264d7b068180479b |  3.5.15 |  7.2 MB |      <span class=nb>true</span> |      <span class=nb>false</span> |         5 |       7012 |               7012 |        |
+------------------------+------------------+---------+---------+-----------+------------+-----------+------------+--------------------+--------+

<span class=c>## 6. 检查 container runtime (Docker 或 containerd) ######################</span>
<span class=nv>$ </span><span class=nb>sudo </span>systemctl status containerd.service
● containerd.service - containerd container runtime
     Loaded: loaded <span class=o>(</span>/lib/systemd/system/containerd.service<span class=p>;</span> enabled<span class=p>;</span> vendor preset: enabled<span class=o>)</span>
     Active: active <span class=o>(</span>running<span class=o>)</span> since Tue 2025-02-11 13:52:25 UTC<span class=p>;</span> 14min ago
       Docs: https://containerd.io
    Process: 673 <span class=nv>ExecStartPre</span><span class=o>=</span>/sbin/modprobe overlay <span class=o>(</span><span class=nv>code</span><span class=o>=</span>exited, <span class=nv>status</span><span class=o>=</span>0/SUCCESS<span class=o>)</span>
   Main PID: 676 <span class=o>(</span>containerd<span class=o>)</span>
      Tasks: 123
     Memory: 109.9M
     CGroup: /system.slice/containerd.service
             ├─ 676 /usr/bin/containerd
             ├─1777 /usr/bin/containerd-shim-runc-v2 <span class=nt>-namespace</span> k8s.io <span class=nt>-id</span> 59dcf616b0a4d8ef4c20a518f0d4ba031a8935ef22aa38921bae97e314fe44d3 <span class=nt>-address</span> /run/containerd/containerd.sock
             ├─1778 /usr/bin/containerd-shim-runc-v2 <span class=nt>-namespace</span> k8s.io <span class=nt>-id</span> 7da214d6618674a4808f09ed4db670440c3b0a60b75ce85dab52ddc763a5ce94 <span class=nt>-address</span> /run/containerd/containerd.sock
             ├─1781 /usr/bin/containerd-shim-runc-v2 <span class=nt>-namespace</span> k8s.io <span class=nt>-id</span> e4b242f37513f75a37e0682bac345960070f1702ecca9352dea73fed18bf9572 <span class=nt>-address</span> /run/containerd/containerd.sock
             ├─1799 /usr/bin/containerd-shim-runc-v2 <span class=nt>-namespace</span> k8s.io <span class=nt>-id</span> 109e3bd03077590438941feba58d3f163b9edbd51858b6df5b9b8c5a338ed06e <span class=nt>-address</span> /run/containerd/containerd.sock
             ├─2517 /usr/bin/containerd-shim-runc-v2 <span class=nt>-namespace</span> k8s.io <span class=nt>-id</span> 2103e6b1f943e457bdf6bfd90e5ff085f44b723cb4c790a5cc4c98172f218e13 <span class=nt>-address</span> /run/containerd/containerd.sock
             ├─2565 /usr/bin/containerd-shim-runc-v2 <span class=nt>-namespace</span> k8s.io <span class=nt>-id</span> a39609efa2fbeb39fbd2b451bc041102cbf110561485d62742f35a733be5f80b <span class=nt>-address</span> /run/containerd/containerd.sock
             ├─3760 /usr/bin/containerd-shim-runc-v2 <span class=nt>-namespace</span> k8s.io <span class=nt>-id</span> 4dd757f349bdf876d1592ad9b73bfee6c5efb1289998f75472cdf898facedc3e <span class=nt>-address</span> /run/containerd/containerd.sock
             └─4183 /usr/bin/containerd-shim-runc-v2 <span class=nt>-namespace</span> k8s.io <span class=nt>-id</span> 764d13ee0730c60a60b6bf3be500663d293251f96dfc7588acab7caa6b71758d <span class=nt>-address</span> /run/containerd/containerd.sock

Feb 11 13:54:40 controlplane containerd[676]: 2025-02-11 13:54:40.367 <span class=o>[</span>INFO][4531] dataplane_linux.go 520: CleanUpNamespace called with no netns name, ignoring. <span class=nv>ContainerID</span><span class=o>=</span><span class=s2>&#34;a10e7596c12e57d118895d47c8ee1bec3a05f16f9c9bbd43cc02f19054b6ee6a&#34;</span> <span class=nv>iface</span><span class=o>=</span><span class=s2>&#34;eth0&#34;</span> <span class=nv>netns</span><span class=o>=</span><span class=s2>&#34;&#34;</span>
Feb 11 13:54:40 controlplane containerd[676]: 2025-02-11 13:54:40.367 <span class=o>[</span>INFO][4531] k8s.go 583: Releasing IP address<span class=o>(</span>es<span class=o>)</span> <span class=nv>ContainerID</span><span class=o>=</span><span class=s2>&#34;a10e7596c12e57d118895d47c8ee1bec3a05f16f9c9bbd43cc02f19054b6ee6a&#34;</span>
Feb 11 13:54:40 controlplane containerd[676]: 2025-02-11 13:54:40.367 <span class=o>[</span>INFO][4531] utils.go 196: Calico CNI releasing IP address <span class=nv>ContainerID</span><span class=o>=</span><span class=s2>&#34;a10e7596c12e57d118895d47c8ee1bec3a05f16f9c9bbd43cc02f19054b6ee6a&#34;</span>
Feb 11 13:54:40 controlplane containerd[676]: 2025-02-11 13:54:40.367 <span class=o>[</span>INFO][4531] utils.go 214: Using dummy podCidrs to release the IPs <span class=nv>ContainerID</span><span class=o>=</span><span class=s2>&#34;a10e7596c12e57d118895d47c8ee1bec3a05f16f9c9bbd43cc02f19054b6ee6a&#34;</span> <span class=nv>podCidrv4</span><span class=o>=</span><span class=s2>&#34;0.0.0.0/0&#34;</span> <span class=nv>podCidrv6</span><span class=o>=</span><span class=s2>&#34;::/0&#34;</span>
Feb 11 13:54:40 controlplane containerd[676]: 2025-02-11 13:54:40.368 <span class=o>[</span>INFO][4531] utils.go 345: Calico CNI passing podCidr to host-local IPAM: 0.0.0.0/0 <span class=nv>ContainerID</span><span class=o>=</span><span class=s2>&#34;a10e7596c12e57d118895d47c8ee1bec3a05f16f9c9bbd43cc02f19054b6ee6a&#34;</span>
Feb 11 13:54:40 controlplane containerd[676]: 2025-02-11 13:54:40.374 <span class=o>[</span>INFO][4531] k8s.go 589: Teardown processing complete. <span class=nv>ContainerID</span><span class=o>=</span><span class=s2>&#34;a10e7596c12e57d118895d47c8ee1bec3a05f16f9c9bbd43cc02f19054b6ee6a&#34;</span>
Feb 11 13:54:40 controlplane containerd[676]: <span class=nb>time</span><span class=o>=</span><span class=s2>&#34;2025-02-11T13:54:40.379627074Z&#34;</span> <span class=nv>level</span><span class=o>=</span>info <span class=nv>msg</span><span class=o>=</span><span class=s2>&#34;TearDown network for sandbox </span><span class=se>\&#34;</span><span class=s2>a10e7596c12e57d118895d47c8ee1bec3a05f16f9c9bbd43cc02f19054b6ee6a</span><span class=se>\&#34;</span><span class=s2> successfully&#34;</span>
Feb 11 13:54:40 controlplane containerd[676]: <span class=nb>time</span><span class=o>=</span><span class=s2>&#34;2025-02-11T13:54:40.395434207Z&#34;</span> <span class=nv>level</span><span class=o>=</span>warning <span class=nv>msg</span><span class=o>=</span><span class=s2>&#34;Failed to get podSandbox status for container event for sandboxID </span><span class=se>\&#34;</span><span class=s2>a10e7596c12e57d118895d47c8ee1bec3a05f16f9c9bbd43cc02f19054b6ee6a</span><span class=se>\&#34;</span><span class=s2>: an error occurred when try to find sandbox: not found. Sending the event with nil podSandboxStatus.&#34;</span>
Feb 11 13:54:40 controlplane containerd[676]: <span class=nb>time</span><span class=o>=</span><span class=s2>&#34;2025-02-11T13:54:40.395824835Z&#34;</span> <span class=nv>level</span><span class=o>=</span>info <span class=nv>msg</span><span class=o>=</span><span class=s2>&#34;RemovePodSandbox </span><span class=se>\&#34;</span><span class=s2>a10e7596c12e57d118895d47c8ee1bec3a05f16f9c9bbd43cc02f19054b6ee6a</span><span class=se>\&#34;</span><span class=s2> returns successfully&#34;</span>
Feb 11 14:03:13 controlplane containerd[676]: <span class=nb>time</span><span class=o>=</span><span class=s2>&#34;2025-02-11T14:03:13.436751316Z&#34;</span> <span class=nv>level</span><span class=o>=</span>info <span class=nv>msg</span><span class=o>=</span><span class=s2>&#34;No cni config template is specified, wait for other system components to drop the config.&#34;</span>

<span class=c>## 7. 检查节点的 taint ######################</span>
<span class=nv>$ </span>kubectl describe nodes | <span class=nb>grep </span>Taints
Taints:             node-role.kubernetes.io/control-plane:NoSchedule
Taints:             &lt;none&gt;

<span class=c># 删除污点命令</span>
<span class=c># $ kubectl taint nodes &lt;node-name&gt; node-role.kubernetes.io/control-plane:NoSchedule-</span>

<span class=c>###</span>

<span class=nv>$ </span><span class=nb>sudo </span>systemctl restart kubelet.service

<span class=nv>$ </span><span class=nb>sudo </span>systemctl status kubelet.service
● kubelet.service - kubelet: The Kubernetes Node Agent
     Loaded: loaded <span class=o>(</span>/lib/systemd/system/kubelet.service<span class=p>;</span> enabled<span class=p>;</span> vendor preset: enabled<span class=o>)</span>
    Drop-In: /usr/lib/systemd/system/kubelet.service.d
             └─10-kubeadm.conf
     Active: active <span class=o>(</span>running<span class=o>)</span> since Tue 2025-02-11 14:03:12 UTC<span class=p>;</span> 15s ago
       Docs: https://kubernetes.io/docs/
   Main PID: 8010 <span class=o>(</span>kubelet<span class=o>)</span>
      Tasks: 9 <span class=o>(</span>limit: 2338<span class=o>)</span>
     Memory: 61.4M
     CGroup: /system.slice/kubelet.service
             └─8010 /usr/bin/kubelet <span class=nt>--bootstrap-kubeconfig</span><span class=o>=</span>/etc/kubernetes/bootstrap-kubelet.conf <span class=nt>--kubeconfig</span><span class=o>=</span>/etc/kubernetes/kubelet.conf <span class=nt>--co</span><span class=o>&gt;</span>
Feb 11 14:03:13 controlplane kubelet[8010]: I0211 14:03:13.866052    8010 reconciler_common.go:245] <span class=s2>&#34;operationExecutor.VerifyControllerAttachedVo&gt;
Feb 11 14:03:13 controlplane kubelet[8010]: I0211 14:03:13.866259    8010 reconciler_common.go:245] &#34;</span>operationExecutor.VerifyControllerAttachedVo&gt;
Feb 11 14:03:13 controlplane kubelet[8010]: I0211 14:03:13.878484    8010 reconciler_common.go:245] <span class=s2>&#34;operationExecutor.VerifyControllerAttachedVo&gt;
Feb 11 14:03:13 controlplane kubelet[8010]: I0211 14:03:13.878976    8010 reconciler_common.go:245] &#34;</span>operationExecutor.VerifyControllerAttachedVo&gt;
Feb 11 14:03:13 controlplane kubelet[8010]: I0211 14:03:13.879326    8010 reconciler_common.go:245] <span class=s2>&#34;operationExecutor.VerifyControllerAttachedVo&gt;
Feb 11 14:03:13 controlplane kubelet[8010]: I0211 14:03:13.879591    8010 reconciler_common.go:245] &#34;</span>operationExecutor.VerifyControllerAttachedVo&gt;
Feb 11 14:03:13 controlplane kubelet[8010]: I0211 14:03:13.879837    8010 reconciler_common.go:245] <span class=s2>&#34;operationExecutor.VerifyControllerAttachedVo&gt;
Feb 11 14:03:13 controlplane kubelet[8010]: I0211 14:03:13.880117    8010 reconciler_common.go:245] &#34;</span>operationExecutor.VerifyControllerAttachedVo&gt;
Feb 11 14:03:13 controlplane kubelet[8010]: I0211 14:03:13.883482    8010 reconciler_common.go:245] <span class=s2>&#34;operationExecutor.VerifyControllerAttachedVo&gt;
Feb 11 14:03:13 controlplane kubelet[8010]: I0211 14:03:13.883896    8010 reconciler_common.go:245] &#34;</span>operationExecutor.VerifyControllerAttachedVo&gt;

<span class=nv>$ </span>kubectl get  nodes
NAME           STATUS   ROLES           AGE   VERSION
controlplane   Ready    control-plane   13d   v1.31.0
node01         Ready    &lt;none&gt;          13d   v1.31.0</code></pre></div></div><div class="admonitionblock tip"><table><tbody><tr><td class=icon><i class="fa icon-tip" title=Tip></i></td><td class=content>重启一下，竟然意外好了！神奇！</td></tr></tbody></table></div><div class=paragraph><p>关于节点的 <code>NotReady</code> 问题，大概有如下几个原因以及对应的排查命令：</p></div><table class="tableblock frame-all grid-all fit-content"><col><col><thead><tr><th class="tableblock halign-left valign-top">问题类别</th><th class="tableblock halign-left valign-top">检查命令</th></tr></thead><tbody><tr><td class="tableblock halign-left valign-top"><p class=tableblock>kubelet 异常</p></td><td class="tableblock halign-left valign-top"><p class=tableblock><code>systemctl status kubelet</code></p></td></tr><tr><td class="tableblock halign-left valign-top"><p class=tableblock>kube-proxy 问题</p></td><td class="tableblock halign-left valign-top"><p class=tableblock><code>kubectl get pods -n kube-system</code></p></td></tr><tr><td class="tableblock halign-left valign-top"><p class=tableblock>CNI 插件异常</p></td><td class="tableblock halign-left valign-top"><p class=tableblock><code>ls /etc/cni/net.d/</code></p></td></tr><tr><td class="tableblock halign-left valign-top"><p class=tableblock>etcd 失败</p></td><td class="tableblock halign-left valign-top"><p class=tableblock><code>kubectl get pods -n kube-system</code></p></td></tr><tr><td class="tableblock halign-left valign-top"><p class=tableblock>Docker/containerd 异常</p></td><td class="tableblock halign-left valign-top"><p class=tableblock><code>systemctl status docker</code> 或 <code>systemctl status containerd</code></p></td></tr><tr><td class="tableblock halign-left valign-top"><p class=tableblock>资源不足</p></td><td class="tableblock halign-left valign-top"><p class=tableblock><code>kubectl describe node &lt;node-name></code></p></td></tr><tr><td class="tableblock halign-left valign-top"><p class=tableblock>taints 影响调度</p></td><td class="tableblock halign-left valign-top"><p class=tableblock><code>kubectl describe node &lt;node-name></code></p></td></tr></tbody></table></div></div><div class=sect1><h2 id=_troubleshooting_etcd_backup_issue>8. Troubleshooting - ETCD Backup Issue</h2><div class=sectionbody><div class=paragraph><p><a href=https://killercoda.com/sachin/course/CKA/etcd-backup-issue target=_blank rel=noopener>Troubleshooting - ETCD Backup Issue</a></p></div><div class=sidebarblock><div class=content><div class=paragraph><p>something is not working at the moment on controlplane node(Cause NotReady state), check that and <code>etcd-controlplane</code> pod is running in <code>kube-system</code> environment, take backup and store it in <code>/opt/cluster_backup.db</code> file, and also store backup console output store it in <code>backup.txt</code></p></div><div class=paragraph><p><code>ssh controlplane</code></p></div></div></div><div class=listingblock><div class=content><pre class="rouge highlight nowrap"><code data-lang=bash><span class=c># @author D瓜哥 · <a href=https://www.diguage.com target=_blank>https://www.diguage.com</a></span>

<span class=nv>$ </span>kubectl <span class=nt>-n</span> kube-system get pod
NAME                                      READY   STATUS    RESTARTS        AGE
calico-kube-controllers-94fb6bc47-rxh7x   1/1     Running   2 <span class=o>(</span>5m15s ago<span class=o>)</span>   13d
canal-phldr                               2/2     Running   2 <span class=o>(</span>5m15s ago<span class=o>)</span>   13d
canal-zl4tq                               2/2     Running   2 <span class=o>(</span>5m15s ago<span class=o>)</span>   13d
coredns-57888bfdc7-685jj                  1/1     Running   1 <span class=o>(</span>5m15s ago<span class=o>)</span>   13d
coredns-57888bfdc7-bbwzr                  1/1     Running   1 <span class=o>(</span>5m15s ago<span class=o>)</span>   13d
etcd-controlplane                         1/1     Running   2 <span class=o>(</span>5m15s ago<span class=o>)</span>   13d
kube-apiserver-controlplane               1/1     Running   2 <span class=o>(</span>5m15s ago<span class=o>)</span>   13d
kube-controller-manager-controlplane      1/1     Running   2 <span class=o>(</span>5m15s ago<span class=o>)</span>   13d
kube-proxy-2mfwz                          1/1     Running   2 <span class=o>(</span>5m15s ago<span class=o>)</span>   13d
kube-proxy-z2ps8                          1/1     Running   1 <span class=o>(</span>5m15s ago<span class=o>)</span>   13d
kube-scheduler-controlplane               1/1     Running   2 <span class=o>(</span>5m15s ago<span class=o>)</span>   13d

<span class=nv>$ </span>kubectl <span class=nb>exec</span> <span class=nt>-it</span> etcd-controlplane <span class=nt>--</span> sh
Error from server <span class=o>(</span>NotFound<span class=o>)</span>: pods <span class=s2>&#34;etcd-controlplane&#34;</span> not found

<span class=nv>$ </span>kubectl <span class=nt>-n</span> kube-system <span class=nb>exec</span> <span class=nt>-it</span> etcd-controlplane <span class=nt>--</span> sh
error: Internal error occurred: error sending request: Post <span class=s2>&#34;https://172.30.1.2:10250/exec/kube-system/etcd-controlplane/etcd?command=sh&amp;input=1&amp;output=1&amp;tty=1&#34;</span>: dial tcp 172.30.1.2:10250: connect: connection refused

<span class=nv>$ ETCDCTL_API</span><span class=o>=</span>3 etcdctl <span class=nt>--endpoints</span><span class=o>=</span>https://127.0.0.1:2379 endpoint status <span class=nt>--write-out</span><span class=o>=</span>table <span class=nt>--cert</span><span class=o>=</span>/etc/kubernetes/pki/etcd/server.crt <span class=nt>--key</span><span class=o>=</span>/etc/kubernetes/pki/etcd/server.key <span class=nt>--cacert</span><span class=o>=</span>/etc/kubernetes/pki/etcd/ca.crt
+------------------------+------------------+---------+---------+-----------+------------+-----------+------------+--------------------+--------+
|        ENDPOINT        |        ID        | VERSION | DB SIZE | IS LEADER | IS LEARNER | RAFT TERM | RAFT INDEX | RAFT APPLIED INDEX | ERRORS |
+------------------------+------------------+---------+---------+-----------+------------+-----------+------------+--------------------+--------+
| https://127.0.0.1:2379 | 264d7b068180479b |  3.5.15 |  6.2 MB |      <span class=nb>true</span> |      <span class=nb>false</span> |         5 |       2889 |               2889 |        |
+------------------------+------------------+---------+---------+-----------+------------+-----------+------------+--------------------+--------+

<span class=nv>$ </span>kubectl get nodes
NAME           STATUS     ROLES           AGE   VERSION
controlplane   NotReady   control-plane   13d   v1.31.0
node01         Ready      &lt;none&gt;          13d   v1.31.0

<span class=nv>$ </span>kubectl describe nodes controlplane
Name:               controlplane
Roles:              control-plane
Labels:             beta.kubernetes.io/arch<span class=o>=</span>amd64
                    beta.kubernetes.io/os<span class=o>=</span>linux
                    kubernetes.io/arch<span class=o>=</span>amd64
                    kubernetes.io/hostname<span class=o>=</span>controlplane
                    kubernetes.io/os<span class=o>=</span>linux
                    node-role.kubernetes.io/control-plane<span class=o>=</span>
                    node.kubernetes.io/exclude-from-external-load-balancers<span class=o>=</span>
Annotations:        flannel.alpha.coreos.com/backend-data: <span class=o>{</span><span class=s2>&#34;VNI&#34;</span>:1,<span class=s2>&#34;VtepMAC&#34;</span>:<span class=s2>&#34;42:f3:52:39:a6:02&#34;</span><span class=o>}</span>
                    flannel.alpha.coreos.com/backend-type: vxlan
                    flannel.alpha.coreos.com/kube-subnet-manager: <span class=nb>true
                    </span>flannel.alpha.coreos.com/public-ip: 172.30.1.2
                    kubeadm.alpha.kubernetes.io/cri-socket: unix:///var/run/containerd/containerd.sock
                    node.alpha.kubernetes.io/ttl: 0
                    projectcalico.org/IPv4Address: 172.30.1.2/24
                    projectcalico.org/IPv4IPIPTunnelAddr: 192.168.0.1
                    volumes.kubernetes.io/controller-managed-attach-detach: <span class=nb>true
</span>CreationTimestamp:  Tue, 28 Jan 2025 16:04:13 +0000
Taints:             node.kubernetes.io/unreachable:NoExecute
                    node-role.kubernetes.io/control-plane:NoSchedule
                    node.kubernetes.io/unreachable:NoSchedule
Unschedulable:      <span class=nb>false
</span>Lease:
  HolderIdentity:  controlplane
  AcquireTime:     &lt;<span class=nb>unset</span><span class=o>&gt;</span>
  RenewTime:       Tue, 11 Feb 2025 14:19:33 +0000
Conditions:
  Type                 Status    LastHeartbeatTime                 LastTransitionTime                Reason              Message
  <span class=nt>----</span>                 <span class=nt>------</span>    <span class=nt>-----------------</span>                 <span class=nt>------------------</span>                <span class=nt>------</span>              <span class=nt>-------</span>
  NetworkUnavailable   False     Tue, 11 Feb 2025 14:17:05 +0000   Tue, 11 Feb 2025 14:17:05 +0000   FlannelIsUp         Flannel is running on this node
  MemoryPressure       Unknown   Tue, 11 Feb 2025 14:16:49 +0000   Tue, 11 Feb 2025 14:20:15 +0000   NodeStatusUnknown   Kubelet stopped posting node status.
  DiskPressure         Unknown   Tue, 11 Feb 2025 14:16:49 +0000   Tue, 11 Feb 2025 14:20:15 +0000   NodeStatusUnknown   Kubelet stopped posting node status.
  PIDPressure          Unknown   Tue, 11 Feb 2025 14:16:49 +0000   Tue, 11 Feb 2025 14:20:15 +0000   NodeStatusUnknown   Kubelet stopped posting node status.
  Ready                Unknown   Tue, 11 Feb 2025 14:16:49 +0000   Tue, 11 Feb 2025 14:20:15 +0000   NodeStatusUnknown   Kubelet stopped posting node status.
Addresses:
  InternalIP:  172.30.1.2
  Hostname:    controlplane
Capacity:
  cpu:                1
  ephemeral-storage:  20134592Ki
  hugepages-2Mi:      0
  memory:             2030940Ki
  pods:               110
Allocatable:
  cpu:                1
  ephemeral-storage:  19586931083
  hugepages-2Mi:      0
  memory:             1928540Ki
  pods:               110
System Info:
  Machine ID:                 388a2d0f867a4404bc12a0093bd9ed8d
  System UUID:                8d237551-ac0a-43de-a1a6-4f0c70c32c61
  Boot ID:                    533e1434-95b5-4104-b082-1555c3e5d9b6
  Kernel Version:             5.4.0-131-generic
  OS Image:                   Ubuntu 20.04.5 LTS
  Operating System:           linux
  Architecture:               amd64
  Container Runtime Version:  containerd://1.7.22
  Kubelet Version:            v1.31.0
  Kube-Proxy Version:
PodCIDR:                      192.168.0.0/24
PodCIDRs:                     192.168.0.0/24
Non-terminated Pods:          <span class=o>(</span>8 <span class=k>in </span>total<span class=o>)</span>
  Namespace                   Name                                       CPU Requests  CPU Limits  Memory Requests  Memory Limits  Age
  <span class=nt>---------</span>                   <span class=nt>----</span>                                       <span class=nt>------------</span>  <span class=nt>----------</span>  <span class=nt>---------------</span>  <span class=nt>-------------</span>  <span class=nt>---</span>
  kube-system                 calico-kube-controllers-94fb6bc47-rxh7x    0 <span class=o>(</span>0%<span class=o>)</span>        0 <span class=o>(</span>0%<span class=o>)</span>      0 <span class=o>(</span>0%<span class=o>)</span>           0 <span class=o>(</span>0%<span class=o>)</span>         13d
  kube-system                 canal-zl4tq                                25m <span class=o>(</span>2%<span class=o>)</span>      0 <span class=o>(</span>0%<span class=o>)</span>      0 <span class=o>(</span>0%<span class=o>)</span>           0 <span class=o>(</span>0%<span class=o>)</span>         13d
  kube-system                 etcd-controlplane                          25m <span class=o>(</span>2%<span class=o>)</span>      0 <span class=o>(</span>0%<span class=o>)</span>      100Mi <span class=o>(</span>5%<span class=o>)</span>       0 <span class=o>(</span>0%<span class=o>)</span>         13d
  kube-system                 kube-apiserver-controlplane                50m <span class=o>(</span>5%<span class=o>)</span>      0 <span class=o>(</span>0%<span class=o>)</span>      0 <span class=o>(</span>0%<span class=o>)</span>           0 <span class=o>(</span>0%<span class=o>)</span>         13d
  kube-system                 kube-controller-manager-controlplane       25m <span class=o>(</span>2%<span class=o>)</span>      0 <span class=o>(</span>0%<span class=o>)</span>      0 <span class=o>(</span>0%<span class=o>)</span>           0 <span class=o>(</span>0%<span class=o>)</span>         13d
  kube-system                 kube-proxy-2mfwz                           0 <span class=o>(</span>0%<span class=o>)</span>        0 <span class=o>(</span>0%<span class=o>)</span>      0 <span class=o>(</span>0%<span class=o>)</span>           0 <span class=o>(</span>0%<span class=o>)</span>         13d
  kube-system                 kube-scheduler-controlplane                25m <span class=o>(</span>2%<span class=o>)</span>      0 <span class=o>(</span>0%<span class=o>)</span>      0 <span class=o>(</span>0%<span class=o>)</span>           0 <span class=o>(</span>0%<span class=o>)</span>         13d
  local-path-storage          local-path-provisioner-6c5cff8948-2x89z    0 <span class=o>(</span>0%<span class=o>)</span>        0 <span class=o>(</span>0%<span class=o>)</span>      0 <span class=o>(</span>0%<span class=o>)</span>           0 <span class=o>(</span>0%<span class=o>)</span>         13d
Allocated resources:
  <span class=o>(</span>Total limits may be over 100 percent, i.e., overcommitted.<span class=o>)</span>
  Resource           Requests    Limits
  <span class=nt>--------</span>           <span class=nt>--------</span>    <span class=nt>------</span>
  cpu                150m <span class=o>(</span>15%<span class=o>)</span>  0 <span class=o>(</span>0%<span class=o>)</span>
  memory             100Mi <span class=o>(</span>5%<span class=o>)</span>  0 <span class=o>(</span>0%<span class=o>)</span>
  ephemeral-storage  0 <span class=o>(</span>0%<span class=o>)</span>      0 <span class=o>(</span>0%<span class=o>)</span>
  hugepages-2Mi      0 <span class=o>(</span>0%<span class=o>)</span>      0 <span class=o>(</span>0%<span class=o>)</span>
Events:
  Type     Reason                   Age                From             Message
  <span class=nt>----</span>     <span class=nt>------</span>                   <span class=nt>----</span>               <span class=nt>----</span>             <span class=nt>-------</span>
  Normal   Starting                 9m37s              kube-proxy
  Normal   Starting                 13d                kube-proxy
  Normal   Starting                 13d                kube-proxy
  Normal   Starting                 13d                kubelet          Starting kubelet.
  Warning  CgroupV1                 13d                kubelet          Cgroup v1 support is <span class=k>in </span>maintenance mode, please migrate to Cgroup v2.
  Normal   NodeAllocatableEnforced  13d                kubelet          Updated Node Allocatable limit across pods
  Normal   NodeHasSufficientMemory  13d                kubelet          Node controlplane status is now: NodeHasSufficientMemory
  Normal   NodeHasNoDiskPressure    13d                kubelet          Node controlplane status is now: NodeHasNoDiskPressure
  Normal   NodeHasSufficientPID     13d                kubelet          Node controlplane status is now: NodeHasSufficientPID
  Normal   RegisteredNode           13d                node-controller  Node controlplane event: Registered Node controlplane <span class=k>in </span>Controller
  Normal   NodeReady                13d                kubelet          Node controlplane status is now: NodeReady
  Normal   RegisteredNode           13d                node-controller  Node controlplane event: Registered Node controlplane <span class=k>in </span>Controller
  Warning  CgroupV1                 13d                kubelet          Cgroup v1 support is <span class=k>in </span>maintenance mode, please migrate to Cgroup v2.
  Normal   NodeHasSufficientMemory  13d <span class=o>(</span>x8 over 13d<span class=o>)</span>  kubelet          Node controlplane status is now: NodeHasSufficientMemory
  Normal   NodeHasNoDiskPressure    13d <span class=o>(</span>x7 over 13d<span class=o>)</span>  kubelet          Node controlplane status is now: NodeHasNoDiskPressure
  Normal   NodeHasSufficientPID     13d <span class=o>(</span>x7 over 13d<span class=o>)</span>  kubelet          Node controlplane status is now: NodeHasSufficientPID
  Normal   NodeAllocatableEnforced  13d                kubelet          Updated Node Allocatable limit across pods
  Normal   Starting                 13d                kubelet          Starting kubelet.
  Normal   RegisteredNode           13d                node-controller  Node controlplane event: Registered Node controlplane <span class=k>in </span>Controller
  Warning  CgroupV1                 10m                kubelet          Cgroup v1 support is <span class=k>in </span>maintenance mode, please migrate to Cgroup v2.
  Normal   Starting                 10m                kubelet          Starting kubelet.
  Warning  InvalidDiskCapacity      10m                kubelet          invalid capacity 0 on image filesystem
  Normal   NodeHasSufficientMemory  10m <span class=o>(</span>x7 over 10m<span class=o>)</span>  kubelet          Node controlplane status is now: NodeHasSufficientMemory
  Normal   NodeHasNoDiskPressure    10m <span class=o>(</span>x7 over 10m<span class=o>)</span>  kubelet          Node controlplane status is now: NodeHasNoDiskPressure
  Normal   NodeHasSufficientPID     10m <span class=o>(</span>x7 over 10m<span class=o>)</span>  kubelet          Node controlplane status is now: NodeHasSufficientPID
  Normal   NodeAllocatableEnforced  10m                kubelet          Updated Node Allocatable limit across pods
  Normal   RegisteredNode           9m25s              node-controller  Node controlplane event: Registered Node controlplane <span class=k>in </span>Controller
  Normal   NodeNotReady             6m19s              node-controller  Node controlplane status is now: NodeNotReady

<span class=nv>$ </span><span class=nb>sudo </span>systemctl status kubelet.service
● kubelet.service - kubelet: The Kubernetes Node Agent
     Loaded: loaded <span class=o>(</span>/lib/systemd/system/kubelet.service<span class=p>;</span> enabled<span class=p>;</span> vendor preset: enabled<span class=o>)</span>
    Drop-In: /usr/lib/systemd/system/kubelet.service.d
             └─10-kubeadm.conf
     Active: inactive <span class=o>(</span>dead<span class=o>)</span> since Tue 2025-02-11 14:19:40 UTC<span class=p>;</span> 7min ago
       Docs: https://kubernetes.io/docs/
    Process: 717 <span class=nv>ExecStart</span><span class=o>=</span>/usr/bin/kubelet <span class=nv>$KUBELET_KUBECONFIG_ARGS</span> <span class=nv>$KUBELET_CONFIG_ARGS</span> <span class=nv>$KUBELET_KUBEADM_ARGS</span> <span class=nv>$KUBELET_EXTRA_ARGS</span> <span class=o>(</span><span class=nv>code</span><span class=o>=</span>exited,&gt;
   Main PID: 717 <span class=o>(</span><span class=nv>code</span><span class=o>=</span>exited, <span class=nv>status</span><span class=o>=</span>0/SUCCESS<span class=o>)</span>

Feb 11 14:17:20 controlplane kubelet[717]: E0211 14:17:20.130711     717 kuberuntime_manager.go:1477] <span class=s2>&#34;Failed to stop sandbox&#34;</span> <span class=nv>podSandboxID</span><span class=o>={</span><span class=s2>&#34;Typ&gt;
Feb 11 14:17:20 controlplane kubelet[717]: E0211 14:17:20.641359     717 kuberuntime_manager.go:1077] &#34;</span>killPodWithSyncResult failed<span class=s2>&#34; err=&#34;</span>failed <span class=o>&gt;</span>
Feb 11 14:17:20 controlplane kubelet[717]: E0211 14:17:20.644886     717 pod_workers.go:1301] <span class=s2>&#34;Error syncing pod, skipping&#34;</span> <span class=nv>err</span><span class=o>=</span><span class=s2>&#34;failed to </span><span class=se>\&#34;</span><span class=s2>Kill&gt;
Feb 11 14:17:21 controlplane kubelet[717]: E0211 14:17:21.209751     717 log.go:32] &#34;</span>StopPodSandbox from runtime service failed<span class=s2>&#34; err=&#34;</span>rpc error: <span class=o>&gt;</span>
Feb 11 14:17:21 controlplane kubelet[717]: E0211 14:17:21.209901     717 kuberuntime_manager.go:1477] <span class=s2>&#34;Failed to stop sandbox&#34;</span> <span class=nv>podSandboxID</span><span class=o>={</span><span class=s2>&#34;Typ&gt;
Feb 11 14:17:21 controlplane kubelet[717]: E0211 14:17:21.212353     717 kuberuntime_manager.go:1077] &#34;</span>killPodWithSyncResult failed<span class=s2>&#34; err=&#34;</span>failed <span class=o>&gt;</span>
Feb 11 14:17:21 controlplane kubelet[717]: E0211 14:17:21.212405     717 pod_workers.go:1301] <span class=s2>&#34;Error syncing pod, skipping&#34;</span> <span class=nv>err</span><span class=o>=</span><span class=s2>&#34;failed to </span><span class=se>\&#34;</span><span class=s2>Kill&gt;
Feb 11 14:19:40 controlplane systemd[1]: Stopping kubelet: The Kubernetes Node Agent...
Feb 11 14:19:40 controlplane systemd[1]: kubelet.service: Succeeded.
Feb 11 14:19:40 controlplane systemd[1]: Stopped kubelet: The Kubernetes Node Agent.

</span><span class=nv>$ </span><span class=s2>cat /usr/lib/systemd/system/kubelet.service.d/10-kubeadm.conf
# Note: This dropin only works with kubeadm and kubelet v1.11+
[Service]
Environment=&#34;</span><span class=nv>KUBELET_KUBECONFIG_ARGS</span><span class=o>=</span><span class=nt>--bootstrap-kubeconfig</span><span class=o>=</span>/etc/kubernetes/bootstrap-kubelet.conf <span class=nt>--kubeconfig</span><span class=o>=</span>/etc/kubernetes/kubelet.conf<span class=s2>&#34;
Environment=&#34;</span><span class=nv>KUBELET_CONFIG_ARGS</span><span class=o>=</span><span class=nt>--config</span><span class=o>=</span>/var/lib/kubelet/config.yaml<span class=s2>&#34;
# This is a file that &#34;</span>kubeadm init<span class=s2>&#34; and &#34;</span>kubeadm <span class=nb>join</span><span class=s2>&#34; generates at runtime, populating the KUBELET_KUBEADM_ARGS variable dynamically
EnvironmentFile=-/var/lib/kubelet/kubeadm-flags.env
# This is a file that the user can use for overrides of the kubelet args as a last resort. Preferably, the user should use
# the .NodeRegistration.KubeletExtraArgs object in the configuration files instead. KUBELET_EXTRA_ARGS should be sourced from this file.
EnvironmentFile=-/etc/default/kubelet
ExecStart=
ExecStart=/usr/bin/kubelet </span><span class=nv>$KUBELET_KUBECONFIG_ARGS</span><span class=s2> </span><span class=nv>$KUBELET_CONFIG_ARGS</span><span class=s2> </span><span class=nv>$KUBELET_KUBEADM_ARGS</span><span class=s2> </span><span class=nv>$KUBELET_EXTRA_ARGS</span><span class=s2>

</span><span class=nv>$ </span><span class=s2>cat /etc/kubernetes/kubelet.conf
apiVersion: v1
clusters:
- cluster:
    certificate-authority-data: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSURCVENDQWUyZ0F3SUJBZ0lJSDdONzMyNVpVZWN3RFFZSktvWklodmNOQVFFTEJRQXdGVEVUTUJFR0ExVUUKQXhNS2EzVmlaWEp1WlhSbGN6QWVGdzB5TlRBeE1qZ3hOVFU0TlRGYUZ3MHpOVEF4TWpZeE5qQXpOVEZhTUJVeApFekFSQmdOVkJBTVRDbXQxWW1WeWJtVjBaWE13Z2dFaU1BMEdDU3FHU0liM0RRRUJBUVVBQTRJQkR3QXdnZ0VLCkFvSUJBUURkcE1QZW90TFZpN2dOcUY1Y25WQk5NdmJtUkRwRGJQQXkxSkd1dDlkVDh2SExiZFdVd2tRTFdmVE4KYnNSTkwxL1pvQXNCYmtCYTlLZHVnQjNzcDRxZ1llTjZiQmpOSG5GSkNNNEFaRGZzRFdUNHpoUzRONTUrVjdQLwpEcDZkRWtMSm1YMHQrN1NkWkFBQVdxenJUVGdhZmRCL1BOMnpwRkMrbFdNSFpBYzlQbHVPT056Y1dYOXgzNTFvCmhhMEZJdVZaYTU5cnlyWTZ2ZjRrVmJ3UmNCRWxnQUdhcGIxWGpBeWdlSzdaa3VhRTA1aGFnOHNGandsWUoyVjAKeDBuSlFVSDFJc0RnL3JkZHVYYzhnYUMzUC9BOTNlYVV4NkdBZG5LRWl0aENxT1ZsRERrZUJqWjZzRDRRZ0IrNApWL3Vnd0Zzc2Zocit2VXBKZjc4MjNvVkxpeWVsQWdNQkFBR2pXVEJYTUE0R0ExVWREd0VCL3dRRUF3SUNwREFQCkJnTlZIUk1CQWY4RUJUQURBUUgvTUIwR0ExVWREZ1FXQkJUR1E0VGVvMmRBWDB0ZGVyZTlUQnlmK24xdG1EQVYKQmdOVkhSRUVEakFNZ2dwcmRXSmxjbTVsZEdWek1BMEdDU3FHU0liM0RRRUJDd1VBQTRJQkFRQXJjMmhVRk1XSwpaQ0M5Yi9nZk1uTW05MUJoclBIbUUrU0xIcHlaWHJEWTh6TURZSFNpK3NINHdVdFlCd0pyQlgxbStwU3lWTmJ6Ckdmd1hvWHZzMElLbGhpL1pvcFdFYWJuSWMrRW9icEwzVDVSRCtMYjV6VW5HT3FsYm9wekZiQ0xER1hsNnQ2aXMKdzhTM1BDWXJnNUNkenBoTkNLUjY2aVhKbk1YbFlmVE1FQXU1N2pVWFlmbTZCejBnVnhXbHdrVXo5MlZvQVlsago5VlhzdmZIVlp1MGNXZG5KbXBkTTRxZHliT0tkcVFPN0tlbmh6YXNmRHNRYzlRRmxPR1pIbkQ2OHAxY0E5MXZtCk1Iak83WERIT0VqNzV2c21NREU2dGM3Wmprd0tKRjJuSURNVVZBZHgwc1JhQmY0VVpNOFVuWFZTZ0lwMWJsSlgKZHpxRFE4c0xKWnFZCi0tLS0tRU5EIENFUlRJRklDQVRFLS0tLS0K
    server: https://172.30.1.2:6443
  name: kubernetes
contexts:
- context:
    cluster: kubernetes
    user: system:node:controlplane
  name: system:node:controlplane@kubernetes
current-context: system:node:controlplane@kubernetes
kind: Config
preferences: {}
users:
- name: system:node:controlplane
  user:
    client-certificate: /var/lib/kubelet/pki/kubelet-client-current.pem
    client-key: /var/lib/kubelet/pki/kubelet-client-current.pem

</span><span class=nv>$ </span><span class=s2>cat /var/lib/kubelet/config.yaml
apiVersion: kubelet.config.k8s.io/v1beta1
authentication:
  anonymous:
    enabled: false
  webhook:
    cacheTTL: 0s
    enabled: true
  x509:
    clientCAFile: /etc/kubernetes/pki/ca.crt
authorization:
  mode: Webhook
  webhook:
    cacheAuthorizedTTL: 0s
    cacheUnauthorizedTTL: 0s
cgroupDriver: systemd
clusterDNS:
- 10.96.0.10
clusterDomain: cluster.local
containerRuntimeEndpoint: &#34;&#34;
cpuManagerReconcilePeriod: 0s
evictionPressureTransitionPeriod: 0s
fileCheckFrequency: 0s
healthzBindAddress: 127.0.0.1
healthzPort: 10248
httpCheckFrequency: 0s
imageMaximumGCAge: 0s
imageMinimumGCAge: 0s
kind: KubeletConfiguration
logging:
  flushFrequency: 0
  options:
    json:
      infoBufferSize: &#34;</span>0<span class=s2>&#34;
    text:
      infoBufferSize: &#34;</span>0<span class=s2>&#34;
  verbosity: 0
memorySwap: {}
nodeStatusReportFrequency: 0s
nodeStatusUpdateFrequency: 0s
resolvConf: /run/systemd/resolve/resolv.conf
rotateCertificates: true
runtimeRequestTimeout: 0s
shutdownGracePeriod: 0s
shutdownGracePeriodCriticalPods: 0s
staticPodPath: /etc/kubernetes/manifests
streamingConnectionIdleTimeout: 0s
syncFrequency: 0s
volumeStatsAggPeriod: 0s

</span><span class=nv>$ </span><span class=s2>ETCDCTL_API=3 etcdctl --endpoints=https://127.0.0.1:2379 --cert=/etc/kubernetes/pki/etcd/server.crt --key=/etc/kubernetes/pki/etcd/server.key --cacert=/etc/kubernetes/pki/etcd/ca.crt snapshot save /opt/cluster_backup.db  | tee /opt/backup.txt
{&#34;</span>level<span class=s2>&#34;:&#34;</span>info<span class=s2>&#34;,&#34;</span>ts<span class=s2>&#34;:1739284249.253456,&#34;</span><span class=nb>caller</span><span class=s2>&#34;:&#34;</span>snapshot/v3_snapshot.go:68<span class=s2>&#34;,&#34;</span>msg<span class=s2>&#34;:&#34;</span>created temporary db file<span class=s2>&#34;,&#34;</span>path<span class=s2>&#34;:&#34;</span>/opt/cluster_backup.db.part<span class=s2>&#34;}
{&#34;</span>level<span class=s2>&#34;:&#34;</span>info<span class=s2>&#34;,&#34;</span>ts<span class=s2>&#34;:1739284249.2617893,&#34;</span>logger<span class=s2>&#34;:&#34;</span>client<span class=s2>&#34;,&#34;</span><span class=nb>caller</span><span class=s2>&#34;:&#34;</span>v3/maintenance.go:211<span class=s2>&#34;,&#34;</span>msg<span class=s2>&#34;:&#34;</span>opened snapshot stream<span class=p>;</span> downloading<span class=s2>&#34;}
{&#34;</span>level<span class=s2>&#34;:&#34;</span>info<span class=s2>&#34;,&#34;</span>ts<span class=s2>&#34;:1739284249.262126,&#34;</span><span class=nb>caller</span><span class=s2>&#34;:&#34;</span>snapshot/v3_snapshot.go:76<span class=s2>&#34;,&#34;</span>msg<span class=s2>&#34;:&#34;</span>fetching snapshot<span class=s2>&#34;,&#34;</span>endpoint<span class=s2>&#34;:&#34;</span>https://127.0.0.1:2379<span class=s2>&#34;}
{&#34;</span>level<span class=s2>&#34;:&#34;</span>info<span class=s2>&#34;,&#34;</span>ts<span class=s2>&#34;:1739284249.4180262,&#34;</span>logger<span class=s2>&#34;:&#34;</span>client<span class=s2>&#34;,&#34;</span><span class=nb>caller</span><span class=s2>&#34;:&#34;</span>v3/maintenance.go:219<span class=s2>&#34;,&#34;</span>msg<span class=s2>&#34;:&#34;</span>completed snapshot <span class=nb>read</span><span class=p>;</span> closing<span class=s2>&#34;}
{&#34;</span>level<span class=s2>&#34;:&#34;</span>info<span class=s2>&#34;,&#34;</span>ts<span class=s2>&#34;:1739284249.4338531,&#34;</span><span class=nb>caller</span><span class=s2>&#34;:&#34;</span>snapshot/v3_snapshot.go:91<span class=s2>&#34;,&#34;</span>msg<span class=s2>&#34;:&#34;</span>fetched snapshot<span class=s2>&#34;,&#34;</span>endpoint<span class=s2>&#34;:&#34;</span>https://127.0.0.1:2379<span class=s2>&#34;,&#34;</span>size<span class=s2>&#34;:&#34;</span>7.1 MB<span class=s2>&#34;,&#34;</span>took<span class=s2>&#34;:&#34;</span>now<span class=s2>&#34;}
{&#34;</span>level<span class=s2>&#34;:&#34;</span>info<span class=s2>&#34;,&#34;</span>ts<span class=s2>&#34;:1739284249.4340887,&#34;</span><span class=nb>caller</span><span class=s2>&#34;:&#34;</span>snapshot/v3_snapshot.go:100<span class=s2>&#34;,&#34;</span>msg<span class=s2>&#34;:&#34;</span>saved<span class=s2>&#34;,&#34;</span>path<span class=s2>&#34;:&#34;</span>/opt/cluster_backup.db<span class=s2>&#34;}
Snapshot saved at /opt/cluster_backup.db


</span><span class=nv>$ </span><span class=s2>kubectl get pods -n kube-system | grep kube-apiserver
kube-apiserver-controlplane               1/1     Running       2 (16m ago)   13d

</span><span class=nv>$ </span><span class=s2>sudo systemctl status kubelet
● kubelet.service - kubelet: The Kubernetes Node Agent
     Loaded: loaded (/lib/systemd/system/kubelet.service; enabled; vendor preset: enabled)
    Drop-In: /usr/lib/systemd/system/kubelet.service.d
             └─10-kubeadm.conf
     Active: inactive (dead) since Tue 2025-02-11 14:19:40 UTC; 13min ago
       Docs: https://kubernetes.io/docs/
    Process: 717 ExecStart=/usr/bin/kubelet </span><span class=nv>$KUBELET_KUBECONFIG_ARGS</span><span class=s2> </span><span class=nv>$KUBELET_CONFIG_ARGS</span><span class=s2> </span><span class=nv>$KUBELET_KUBEADM_ARGS</span><span class=s2> </span><span class=nv>$KUBELET_EXTRA_ARGS</span><span class=s2> (code=&gt;
   Main PID: 717 (code=exited, status=0/SUCCESS)

Feb 11 14:17:20 controlplane kubelet[717]: E0211 14:17:20.130711     717 kuberuntime_manager.go:1477] &#34;</span>Failed to stop sandbox<span class=s2>&#34; podSandboxI&gt;
Feb 11 14:17:20 controlplane kubelet[717]: E0211 14:17:20.641359     717 kuberuntime_manager.go:1077] &#34;</span>killPodWithSyncResult failed<span class=s2>&#34; err=&#34;</span><span class=o>&gt;</span>
Feb 11 14:17:20 controlplane kubelet[717]: E0211 14:17:20.644886     717 pod_workers.go:1301] <span class=s2>&#34;Error syncing pod, skipping&#34;</span> <span class=nv>err</span><span class=o>=</span><span class=s2>&#34;failed to&gt;
Feb 11 14:17:21 controlplane kubelet[717]: E0211 14:17:21.209751     717 log.go:32] &#34;</span>StopPodSandbox from runtime service failed<span class=s2>&#34; err=&#34;</span>rpc <span class=o>&gt;</span>
Feb 11 14:17:21 controlplane kubelet[717]: E0211 14:17:21.209901     717 kuberuntime_manager.go:1477] <span class=s2>&#34;Failed to stop sandbox&#34;</span> podSandboxI&gt;
Feb 11 14:17:21 controlplane kubelet[717]: E0211 14:17:21.212353     717 kuberuntime_manager.go:1077] <span class=s2>&#34;killPodWithSyncResult failed&#34;</span> <span class=nv>err</span><span class=o>=</span><span class=s2>&#34;&gt;
Feb 11 14:17:21 controlplane kubelet[717]: E0211 14:17:21.212405     717 pod_workers.go:1301] &#34;</span>Error syncing pod, skipping<span class=s2>&#34; err=&#34;</span>failed to&gt;
Feb 11 14:19:40 controlplane systemd[1]: Stopping kubelet: The Kubernetes Node Agent...
Feb 11 14:19:40 controlplane systemd[1]: kubelet.service: Succeeded.
Feb 11 14:19:40 controlplane systemd[1]: Stopped kubelet: The Kubernetes Node Agent.

<span class=nv>$ </span><span class=nb>sudo </span>systemctl restart kubelet.service

<span class=nv>$ </span><span class=nb>sudo </span>systemctl status kubelet
● kubelet.service - kubelet: The Kubernetes Node Agent
     Loaded: loaded <span class=o>(</span>/lib/systemd/system/kubelet.service<span class=p>;</span> enabled<span class=p>;</span> vendor preset: enabled<span class=o>)</span>
    Drop-In: /usr/lib/systemd/system/kubelet.service.d
             └─10-kubeadm.conf
     Active: active <span class=o>(</span>running<span class=o>)</span> since Tue 2025-02-11 14:33:32 UTC<span class=p>;</span> 3s ago
       Docs: https://kubernetes.io/docs/
   Main PID: 7121 <span class=o>(</span>kubelet<span class=o>)</span>
      Tasks: 10 <span class=o>(</span>limit: 2338<span class=o>)</span>
     Memory: 64.0M
     CGroup: /system.slice/kubelet.service
             └─7121 /usr/bin/kubelet <span class=nt>--bootstrap-kubeconfig</span><span class=o>=</span>/etc/kubernetes/bootstrap-kubelet.conf <span class=nt>--kubeconfig</span><span class=o>=</span>/etc/kubernetes/kubelet.co&gt;
Feb 11 14:33:35 controlplane kubelet[7121]: I0211 14:33:35.010902    7121 scope.go:117] <span class=s2>&#34;RemoveContainer&#34;</span> <span class=nv>containerID</span><span class=o>=</span><span class=s2>&#34;e383cd653371b1f67e2&gt;
Feb 11 14:33:35 controlplane kubelet[7121]: I0211 14:33:35.266713    7121 reconciler_common.go:159] &#34;</span>operationExecutor.UnmountVolume start&gt;
Feb 11 14:33:35 controlplane kubelet[7121]: I0211 14:33:35.267086    7121 reconciler_common.go:159] <span class=s2>&#34;operationExecutor.UnmountVolume start&gt;
Feb 11 14:33:35 controlplane kubelet[7121]: I0211 14:33:35.267283    7121 reconciler_common.go:159] &#34;</span>operationExecutor.UnmountVolume start&gt;
Feb 11 14:33:35 controlplane kubelet[7121]: I0211 14:33:35.270896    7121 operation_generator.go:803] UnmountVolume.TearDown succeeded <span class=k>for</span><span class=o>&gt;</span>
Feb 11 14:33:35 controlplane kubelet[7121]: I0211 14:33:35.292394    7121 operation_generator.go:803] UnmountVolume.TearDown succeeded <span class=k>for</span><span class=o>&gt;</span>
Feb 11 14:33:35 controlplane kubelet[7121]: I0211 14:33:35.293432    7121 operation_generator.go:803] UnmountVolume.TearDown succeeded <span class=k>for</span><span class=o>&gt;</span>
Feb 11 14:33:35 controlplane kubelet[7121]: I0211 14:33:35.367884    7121 reconciler_common.go:288] <span class=s2>&#34;Volume detached for volume </span><span class=se>\&#34;</span><span class=s2>kube-api&gt;
Feb 11 14:33:35 controlplane kubelet[7121]: I0211 14:33:35.368182    7121 reconciler_common.go:288] &#34;</span>Volume detached <span class=k>for </span>volume <span class=se>\&#34;</span>config-v&gt;
Feb 11 14:33:35 controlplane kubelet[7121]: I0211 14:33:35.368310    7121 reconciler_common.go:288] <span class=s2>&#34;Volume detached for volume </span><span class=se>\&#34;</span><span class=s2>kube-api&gt;

</span><span class=nv>$ </span><span class=s2>kubectl get nodes
NAME           STATUS   ROLES           AGE   VERSION
controlplane   Ready    control-plane   13d   v1.31.0
node01         Ready    &lt;none&gt;          13d   v1.31.0
# 重启，系统恢复正常</span></code></pre></div></div><div class="admonitionblock note"><table><tbody><tr><td class=icon><i class="fa icon-note" title=Note></i></td><td class=content>系统恢复正常，备份也正常保存，日志也有。但是，检查没通过。</td></tr></tbody></table></div></div></div><div class=sect1><h2 id=_troubleshooting_controller_manager_issue>9. Troubleshooting - Controller Manager Issue</h2><div class=sectionbody><div class=paragraph><p><a href=https://killercoda.com/sachin/course/CKA/controller-manager-issue target=_blank rel=noopener>Troubleshooting - Controller Manager Issue</a></p></div><div class=sidebarblock><div class=content><div class=paragraph><p><code>video-app</code> deployment replicas 0. fix this issue</p></div><div class=paragraph><p>expected: 2 replicas</p></div></div></div><div class=listingblock><div class=content><pre class="rouge highlight nowrap"><code data-lang=bash><span class=c># @author D瓜哥 · <a href=https://www.diguage.com target=_blank>https://www.diguage.com</a></span>

<span class=nv>$ </span>kubectl get deployments.apps video-app <span class=nt>-o</span> wide
NAME        READY   UP-TO-DATE   AVAILABLE   AGE   CONTAINERS   IMAGES        SELECTOR
video-app   0/2     0            0           94s   redis        redis:7.2.1   <span class=nv>app</span><span class=o>=</span>video-app

<span class=nv>$ </span>kubectl describe deployments.apps video-app
Name:                   video-app
Namespace:              default
CreationTimestamp:      Wed, 12 Feb 2025 13:32:03 +0000
Labels:                 <span class=nv>app</span><span class=o>=</span>video-app
Annotations:            &lt;none&gt;
Selector:               <span class=nv>app</span><span class=o>=</span>video-app
Replicas:               2 desired | 0 updated | 0 total | 0 available | 0 unavailable
StrategyType:           RollingUpdate
MinReadySeconds:        0
RollingUpdateStrategy:  25% max unavailable, 25% max surge
Pod Template:
  Labels:  <span class=nv>app</span><span class=o>=</span>video-app
  Containers:
   redis:
    Image:         redis:7.2.1
    Port:          &lt;none&gt;
    Host Port:     &lt;none&gt;
    Environment:   &lt;none&gt;
    Mounts:        &lt;none&gt;
  Volumes:         &lt;none&gt;
  Node-Selectors:  &lt;none&gt;
  Tolerations:     &lt;none&gt;
Events:            &lt;none&gt;

<span class=nv>$ </span>kubectl get deployments.apps video-app <span class=nt>-o</span> yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  creationTimestamp: <span class=s2>&#34;2025-02-12T13:32:03Z&#34;</span>
  generation: 1
  labels:
    app: video-app
  name: video-app
  namespace: default
  resourceVersion: <span class=s2>&#34;2031&#34;</span>
  uid: 6bb0d745-f901-4f19-abc8-52007c724c77
spec:
  progressDeadlineSeconds: 600
  replicas: 2
  revisionHistoryLimit: 10
  selector:
    matchLabels:
      app: video-app
  strategy:
    rollingUpdate:
      maxSurge: 25%
      maxUnavailable: 25%
    <span class=nb>type</span>: RollingUpdate
  template:
    metadata:
      creationTimestamp: null
      labels:
        app: video-app
    spec:
      containers:
      - image: redis:7.2.1
        imagePullPolicy: IfNotPresent
        name: redis
        resources: <span class=o>{}</span>
        terminationMessagePath: /dev/termination-log
        terminationMessagePolicy: File
      dnsPolicy: ClusterFirst
      restartPolicy: Always
      schedulerName: default-scheduler
      securityContext: <span class=o>{}</span>
      terminationGracePeriodSeconds: 30
status: <span class=o>{}</span>

<span class=nv>$ </span>kubectl get nodes
NAME           STATUS   ROLES           AGE   VERSION
controlplane   Ready    control-plane   20h   v1.31.0
node01         Ready    &lt;none&gt;          20h   v1.31.0

<span class=nv>$ </span>kubectl get nodes <span class=nt>-o</span> wide
NAME           STATUS   ROLES           AGE   VERSION   INTERNAL-IP   EXTERNAL-IP   OS-IMAGE             KERNEL-VERSION      CONTAINER-RUNTIME
controlplane   Ready    control-plane   20h   v1.31.0   172.30.1.2    &lt;none&gt;        Ubuntu 20.04.5 LTS   5.4.0-131-generic   containerd://1.7.22
node01         Ready    &lt;none&gt;          20h   v1.31.0   172.30.2.2    &lt;none&gt;        Ubuntu 20.04.5 LTS   5.4.0-131-generic   containerd://1.7.22

<span class=nv>$ </span>kubectl describe nodes | <span class=nb>grep </span>Taints
Taints:             node-role.kubernetes.io/control-plane:NoSchedule
Taints:             &lt;none&gt;

<span class=nv>$ </span>kubectl get componentstatuses
Warning: v1 ComponentStatus is deprecated <span class=k>in </span>v1.19+
NAME                 STATUS      MESSAGE                                                                                        ERROR
controller-manager   Unhealthy   Get <span class=s2>&#34;https://127.0.0.1:10257/healthz&#34;</span>: dial tcp 127.0.0.1:10257: connect: connection refused
scheduler            Healthy     ok
etcd-0               Healthy     ok

<span class=nv>$ </span>kubectl get pods <span class=nt>-n</span> kube-system | <span class=nb>grep </span>kube-controller-manager
kube-controller-manager-controlplane      0/1     CrashLoopBackOff   7 <span class=o>(</span>2m22s ago<span class=o>)</span>   13m

<span class=nv>$ </span>kubectl <span class=nt>-n</span> kube-system describe pod kube-controller-manager-controlplane
Name:                 kube-controller-manager-controlplane
Namespace:            kube-system
Priority:             2000001000
Priority Class Name:  system-node-critical
Node:                 controlplane/172.30.1.2
Start Time:           Wed, 12 Feb 2025 13:31:16 +0000
Labels:               <span class=nv>component</span><span class=o>=</span>kube-controller-manager
                      <span class=nv>tier</span><span class=o>=</span>control-plane
Annotations:          kubernetes.io/config.hash: b0c098fd6896ecf1d8a30f03b739da5f
                      kubernetes.io/config.mirror: b0c098fd6896ecf1d8a30f03b739da5f
                      kubernetes.io/config.seen: 2025-02-12T13:32:02.867060432Z
                      kubernetes.io/config.source: file
Status:               Running
SeccompProfile:       RuntimeDefault
IP:                   172.30.1.2
IPs:
  IP:           172.30.1.2
Controlled By:  Node/controlplane
Containers:
  kube-controller-manager:
    Container ID:  containerd://a5ff61b46d26608c4f4c301161cde11aaf1eb615abfcb57fcd441a6c3ead7b35
    Image:         registry.k8s.io/kube-controller-manager:v1.31.0
    Image ID:      registry.k8s.io/kube-controller-manager@sha256:f6f3c33dda209e8434b83dacf5244c03b59b0018d93325ff21296a142b68497d
    Port:          &lt;none&gt;
    Host Port:     &lt;none&gt;
    Command:
      kube-controller-manegaar
      <span class=nt>--allocate-node-cidrs</span><span class=o>=</span><span class=nb>true</span>
      <span class=nt>--authentication-kubeconfig</span><span class=o>=</span>/etc/kubernetes/controller-manager.conf
      <span class=nt>--authorization-kubeconfig</span><span class=o>=</span>/etc/kubernetes/controller-manager.conf
      <span class=nt>--bind-address</span><span class=o>=</span>127.0.0.1
      <span class=nt>--client-ca-file</span><span class=o>=</span>/etc/kubernetes/pki/ca.crt
      <span class=nt>--cluster-cidr</span><span class=o>=</span>192.168.0.0/16
      <span class=nt>--cluster-name</span><span class=o>=</span>kubernetes
      <span class=nt>--cluster-signing-cert-file</span><span class=o>=</span>/etc/kubernetes/pki/ca.crt
      <span class=nt>--cluster-signing-key-file</span><span class=o>=</span>/etc/kubernetes/pki/ca.key
      <span class=nt>--controllers</span><span class=o>=</span><span class=k>*</span>,bootstrapsigner,tokencleaner
      <span class=nt>--kubeconfig</span><span class=o>=</span>/etc/kubernetes/controller-manager.conf
      <span class=nt>--leader-elect</span><span class=o>=</span><span class=nb>true</span>
      <span class=nt>--requestheader-client-ca-file</span><span class=o>=</span>/etc/kubernetes/pki/front-proxy-ca.crt
      <span class=nt>--root-ca-file</span><span class=o>=</span>/etc/kubernetes/pki/ca.crt
      <span class=nt>--service-account-private-key-file</span><span class=o>=</span>/etc/kubernetes/pki/sa.key
      <span class=nt>--service-cluster-ip-range</span><span class=o>=</span>10.96.0.0/12
      <span class=nt>--use-service-account-credentials</span><span class=o>=</span><span class=nb>true
    </span>State:          Waiting
      Reason:       CrashLoopBackOff
    Last State:     Terminated
      Reason:       StartError
      Message:      failed to create containerd task: failed to create shim task: OCI runtime create failed: runc create failed: unable to start container process: <span class=nb>exec</span>: <span class=s2>&#34;kube-controller-manegaar&#34;</span>: executable file not found <span class=k>in</span> <span class=nv>$PATH</span>: unknown
      Exit Code:    128
      Started:      Thu, 01 Jan 1970 00:00:00 +0000
      Finished:     Wed, 12 Feb 2025 13:43:09 +0000
    Ready:          False
    Restart Count:  7
    Requests:
      cpu:        25m
    Liveness:     http-get https://127.0.0.1:10257/healthz <span class=nv>delay</span><span class=o>=</span>10s <span class=nb>timeout</span><span class=o>=</span>15s <span class=nv>period</span><span class=o>=</span>10s <span class=c>#success=1 #failure=8</span>
    Startup:      http-get https://127.0.0.1:10257/healthz <span class=nv>delay</span><span class=o>=</span>10s <span class=nb>timeout</span><span class=o>=</span>15s <span class=nv>period</span><span class=o>=</span>10s <span class=c>#success=1 #failure=24</span>
    Environment:  &lt;none&gt;
    Mounts:
      /etc/ca-certificates from etc-ca-certificates <span class=o>(</span>ro<span class=o>)</span>
      /etc/kubernetes/controller-manager.conf from kubeconfig <span class=o>(</span>ro<span class=o>)</span>
      /etc/kubernetes/pki from k8s-certs <span class=o>(</span>ro<span class=o>)</span>
      /etc/ssl/certs from ca-certs <span class=o>(</span>ro<span class=o>)</span>
      /usr/libexec/kubernetes/kubelet-plugins/volume/exec from flexvolume-dir <span class=o>(</span>rw<span class=o>)</span>
      /usr/local/share/ca-certificates from usr-local-share-ca-certificates <span class=o>(</span>ro<span class=o>)</span>
      /usr/share/ca-certificates from usr-share-ca-certificates <span class=o>(</span>ro<span class=o>)</span>
Conditions:
  Type                        Status
  PodReadyToStartContainers   True
  Initialized                 True
  Ready                       False
  ContainersReady             False
  PodScheduled                True
Volumes:
  ca-certs:
    Type:          HostPath <span class=o>(</span>bare host directory volume<span class=o>)</span>
    Path:          /etc/ssl/certs
    HostPathType:  DirectoryOrCreate
  etc-ca-certificates:
    Type:          HostPath <span class=o>(</span>bare host directory volume<span class=o>)</span>
    Path:          /etc/ca-certificates
    HostPathType:  DirectoryOrCreate
  flexvolume-dir:
    Type:          HostPath <span class=o>(</span>bare host directory volume<span class=o>)</span>
    Path:          /usr/libexec/kubernetes/kubelet-plugins/volume/exec
    HostPathType:  DirectoryOrCreate
  k8s-certs:
    Type:          HostPath <span class=o>(</span>bare host directory volume<span class=o>)</span>
    Path:          /etc/kubernetes/pki
    HostPathType:  DirectoryOrCreate
  kubeconfig:
    Type:          HostPath <span class=o>(</span>bare host directory volume<span class=o>)</span>
    Path:          /etc/kubernetes/controller-manager.conf
    HostPathType:  FileOrCreate
  usr-local-share-ca-certificates:
    Type:          HostPath <span class=o>(</span>bare host directory volume<span class=o>)</span>
    Path:          /usr/local/share/ca-certificates
    HostPathType:  DirectoryOrCreate
  usr-share-ca-certificates:
    Type:          HostPath <span class=o>(</span>bare host directory volume<span class=o>)</span>
    Path:          /usr/share/ca-certificates
    HostPathType:  DirectoryOrCreate
QoS Class:         Burstable
Node-Selectors:    &lt;none&gt;
Tolerations:       :NoExecute <span class=nv>op</span><span class=o>=</span>Exists
Events:
  Type     Reason   Age                   From     Message
  <span class=nt>----</span>     <span class=nt>------</span>   <span class=nt>----</span>                  <span class=nt>----</span>     <span class=nt>-------</span>
  Normal   Pulled   12m <span class=o>(</span>x4 over 13m<span class=o>)</span>     kubelet  Container image <span class=s2>&#34;registry.k8s.io/kube-controller-manager:v1.31.0&#34;</span> already present on machine
  Normal   Created  12m <span class=o>(</span>x4 over 13m<span class=o>)</span>     kubelet  Created container kube-controller-manager
  Warning  Failed   12m <span class=o>(</span>x4 over 13m<span class=o>)</span>     kubelet  Error: failed to create containerd task: failed to create shim task: OCI runtime create failed: runc create failed: unable to start container process: <span class=nb>exec</span>: <span class=s2>&#34;kube-controller-manegaar&#34;</span>: executable file not found <span class=k>in</span> <span class=nv>$PATH</span>: unknown
  Warning  BackOff  3m36s <span class=o>(</span>x56 over 13m<span class=o>)</span>  kubelet  Back-off restarting failed container kube-controller-manager <span class=k>in </span>pod kube-controller-manager-controlplane_kube-system<span class=o>(</span>b0c098fd6896ecf1d8a30f03b739da5f<span class=o>)</span>
<span class=c># 从这里可以看出：大概率是由于命令写错了。</span>

<span class=nv>$ </span><span class=nb>cat</span> /etc/kubernetes/manifests/kube-controller-manager.yaml
apiVersion: v1
kind: Pod
metadata:
  creationTimestamp: null
  labels:
    component: kube-controller-manager
    tier: control-plane
  name: kube-controller-manager
  namespace: kube-system
spec:
  containers:
  - <span class=nb>command</span>:
    - kube-controller-manegaar
    - <span class=nt>--allocate-node-cidrs</span><span class=o>=</span><span class=nb>true</span>
    - <span class=nt>--authentication-kubeconfig</span><span class=o>=</span>/etc/kubernetes/controller-manager.conf
    - <span class=nt>--authorization-kubeconfig</span><span class=o>=</span>/etc/kubernetes/controller-manager.conf
    - <span class=nt>--bind-address</span><span class=o>=</span>127.0.0.1
    - <span class=nt>--client-ca-file</span><span class=o>=</span>/etc/kubernetes/pki/ca.crt
    - <span class=nt>--cluster-cidr</span><span class=o>=</span>192.168.0.0/16
    - <span class=nt>--cluster-name</span><span class=o>=</span>kubernetes
    - <span class=nt>--cluster-signing-cert-file</span><span class=o>=</span>/etc/kubernetes/pki/ca.crt
    - <span class=nt>--cluster-signing-key-file</span><span class=o>=</span>/etc/kubernetes/pki/ca.key
    - <span class=nt>--controllers</span><span class=o>=</span><span class=k>*</span>,bootstrapsigner,tokencleaner
    - <span class=nt>--kubeconfig</span><span class=o>=</span>/etc/kubernetes/controller-manager.conf
    - <span class=nt>--leader-elect</span><span class=o>=</span><span class=nb>true</span>
    - <span class=nt>--requestheader-client-ca-file</span><span class=o>=</span>/etc/kubernetes/pki/front-proxy-ca.crt
    - <span class=nt>--root-ca-file</span><span class=o>=</span>/etc/kubernetes/pki/ca.crt
    - <span class=nt>--service-account-private-key-file</span><span class=o>=</span>/etc/kubernetes/pki/sa.key
    - <span class=nt>--service-cluster-ip-range</span><span class=o>=</span>10.96.0.0/12
    - <span class=nt>--use-service-account-credentials</span><span class=o>=</span><span class=nb>true
    </span>image: registry.k8s.io/kube-controller-manager:v1.31.0
    imagePullPolicy: IfNotPresent
    livenessProbe:
      failureThreshold: 8
      httpGet:
        host: 127.0.0.1
        path: /healthz
        port: 10257
        scheme: HTTPS
      initialDelaySeconds: 10
      periodSeconds: 10
      timeoutSeconds: 15
    name: kube-controller-manager
    resources:
      requests:
        cpu: 25m
    startupProbe:
      failureThreshold: 24
      httpGet:
        host: 127.0.0.1
        path: /healthz
        port: 10257
        scheme: HTTPS
      initialDelaySeconds: 10
      periodSeconds: 10
      timeoutSeconds: 15
    volumeMounts:
    - mountPath: /etc/ssl/certs
      name: ca-certs
      readOnly: <span class=nb>true</span>
    - mountPath: /etc/ca-certificates
      name: etc-ca-certificates
      readOnly: <span class=nb>true</span>
    - mountPath: /usr/libexec/kubernetes/kubelet-plugins/volume/exec
      name: flexvolume-dir
    - mountPath: /etc/kubernetes/pki
      name: k8s-certs
      readOnly: <span class=nb>true</span>
    - mountPath: /etc/kubernetes/controller-manager.conf
      name: kubeconfig
      readOnly: <span class=nb>true</span>
    - mountPath: /usr/local/share/ca-certificates
      name: usr-local-share-ca-certificates
      readOnly: <span class=nb>true</span>
    - mountPath: /usr/share/ca-certificates
      name: usr-share-ca-certificates
      readOnly: <span class=nb>true
  </span>hostNetwork: <span class=nb>true
  </span>priority: 2000001000
  priorityClassName: system-node-critical
  securityContext:
    seccompProfile:
      <span class=nb>type</span>: RuntimeDefault
  volumes:
  - hostPath:
      path: /etc/ssl/certs
      <span class=nb>type</span>: DirectoryOrCreate
    name: ca-certs
  - hostPath:
      path: /etc/ca-certificates
      <span class=nb>type</span>: DirectoryOrCreate
    name: etc-ca-certificates
  - hostPath:
      path: /usr/libexec/kubernetes/kubelet-plugins/volume/exec
      <span class=nb>type</span>: DirectoryOrCreate
    name: flexvolume-dir
  - hostPath:
      path: /etc/kubernetes/pki
      <span class=nb>type</span>: DirectoryOrCreate
    name: k8s-certs
  - hostPath:
      path: /etc/kubernetes/controller-manager.conf
      <span class=nb>type</span>: FileOrCreate
    name: kubeconfig
  - hostPath:
      path: /usr/local/share/ca-certificates
      <span class=nb>type</span>: DirectoryOrCreate
    name: usr-local-share-ca-certificates
  - hostPath:
      path: /usr/share/ca-certificates
      <span class=nb>type</span>: DirectoryOrCreate
    name: usr-share-ca-certificates
status: <span class=o>{}</span>

<span class=nv>$ </span>vim /etc/kubernetes/manifests/kube-controller-manager.yaml
<span class=c># 将命令由 kube-controller-manegaar 改为 kube-controller-manager</span>

<span class=nv>$ </span>kubectl <span class=nt>-n</span> kube-system get pod kube-controller-manager-controlplane
NAME                                   READY   STATUS    RESTARTS   AGE
kube-controller-manager-controlplane   1/1     Running   0          83s

<span class=nv>$ </span>kubectl get deployments.apps video-app
NAME        READY   UP-TO-DATE   AVAILABLE   AGE
video-app   2/2     2            2           22m

<span class=nv>$ </span>kubectl get pod
NAME                         READY   STATUS    RESTARTS   AGE
video-app-7f4f8696cd-rl7ph   1/1     Running   0          2m18s
video-app-7f4f8696cd-vdfqw   1/1     Running   0          2m18s

<span class=nv>$ </span>kubectl get pods <span class=nt>-l</span> <span class=nv>app</span><span class=o>=</span>video-app
NAME                         READY   STATUS    RESTARTS   AGE
video-app-7f4f8696cd-rl7ph   1/1     Running   0          2m39s
video-app-7f4f8696cd-vdfqw   1/1     Running   0          2m39s</code></pre></div></div><div class=paragraph><p>除此之外，还有其他几个原因也会调度失败：</p></div><div class="olist arabic"><ol class=arabic><li><p>资源不足（CPU/Memory 限制导致调度失败）。</p></li><li><p><code>imagePullPolicy</code> 问题（Redis 镜像拉取失败）。</p></li><li><p>Pod 创建失败，但没有错误日志。</p></li><li><p>检查 <code>kubectl get pods</code> 是否有相关 Pod 存在。</p></li></ol></div></div></div></div><footer class=post__footer><div class="post__tags tags clearfix"><svg class="tags__badge icon icon-tag" width="16" height="16" viewBox="0 0 32 32"><path d="M32 19c0 1-1 2-1 2L21 31s-1 1-2 1-2-1-2-1L2 16c-1-1-1.4-2-1.4-2S0 12.5.0 11V3C0 1.5.8.8.8.8S1.5.0 3 0h8c1.5.0 3 .6 3 .6S15 1 16 2l15 15s1 1 1 2zM7 10a3 3 0 100-6 3 3 0 000 6z"/></svg><ul class=tags__list><li class=tags__item><a class="tags__link btn" href=/tags/kubernetes/ rel=tag>Kubernetes</a></li><li class=tags__item><a class="tags__link btn" href=/tags/linux/ rel=tag>Linux</a></li></ul></div></footer></article></main><div class=clearfix><h3>看在D瓜哥码字的辛苦上，请友情支持一下，D瓜哥感激不尽，😜</h3><table><tr><td><img alt=微信打赏码 src=/images/wxpay.jpg></td><td><img alt=支付宝打赏码 src=/images/alipay.png></td></tr></table></div><br><div class=clearfix><h3>欢迎关注D瓜哥的微信公众号，在公众号可以获取我的微信二维码：</h3><img alt=微信公众号 src=/images/wx-jikerizhi.jpg></div><div class="admonitionblock tip"><table><tbody><tr><td class=icon><i class="fa icon-tip" title=Tip></i></td><td class=content><strong>公众号的微信号是: <code>jikerizhi</code></strong>。如果图片加载不出来，可以直接通过搜索公众号的微信号来查找D瓜哥的公众号。</td></tr></tbody></table></div><div class="authorbox clearfix"><figure class=authorbox__avatar><a target=_blank href=/about/><img alt="D瓜哥 avatar" src=/images/avatar.jpg class=avatar height=110 width=110></a></figure><div class=authorbox__header><span class=authorbox__name>关于 D瓜哥</span></div><div class=authorbox__description>厨艺界最好的码农，挨踢界最棒的厨师。<ul><li><a target=_blank href=https://wordpress.diguage.com/>旧版“地瓜哥”博客网</a></li><li><a target=_blank href=https://notes.diguage.com/mysql/>MySQL 学习笔记<sup>Alpha</sup></a></li><li><a target=_blank href=https://diguage.github.io/jdk-source-analysis/>JDK 源码分析<sup>Alpha</sup></a></li></ul></div></div><nav class="pager flex"><div class="pager__item pager__item--prev"><a class=pager__link href=/post/killercoda-cka-troubleshooting-2/ rel=prev><span class=pager__subtitle>«&#8201;上一篇</span><p class=pager__title>killercoda CKA：Troubleshooting - 2</p></a></div><div class="pager__item pager__item--next"><a class=pager__link href=/post/play-with-kubernetes-01-install-kubernetes-offline/ rel=next><span class=pager__subtitle>下一篇&#8201;»</span><p class=pager__title>玩转 Kubernetes（一）：离线安装 Kubernetes 1</p></a></div></nav><section class=comments><div id=SOHUCS sid=/post/killercoda-cka-troubleshooting-3/></div><script type=text/javascript>(function(){if(window.location.hostname==="localhost")return;var n,e="cyuuTeBp3",t="prod_2906c47c31e735e0ed518282fec489a6",s=window.innerWidth||document.documentElement.clientWidth;s<960?window.document.write('<script id="changyan_mobile_js" charset="utf-8" type="text/javascript" src="https://changyan.sohu.com/upload/mobile/wap-js/changyan_mobile.js?client_id='+e+"&conf="+t+'"><\/script>'):(n=function(e,t){var s=document.getElementsByTagName("head")[0]||document.head||document.documentElement,n=document.createElement("script");n.setAttribute("type","text/javascript"),n.setAttribute("charset","UTF-8"),n.setAttribute("src",e),typeof t=="function"&&(window.attachEvent?n.onreadystatechange=function(){var e=n.readyState;(e==="loaded"||e==="complete")&&(n.onreadystatechange=null,t())}:n.onload=t),s.appendChild(n)},n("https://changyan.sohu.com/upload/changyan.js",function(){window.changyan.api.config({appid:e,conf:t})}))})()</script></section></div><aside class=sidebar><div class="widget-search widget"><form class=widget-search__form role=search method=get action=https://google.com/search><label><input class=widget-search__field type=search placeholder=搜索… name=q aria-label=搜索…>
</label><input class=widget-search__submit type=submit value=Search>
<input type=hidden name=sitesearch value=https://www.diguage.com/></form></div><div class="widget-wechat widget"><h4 class=widget__title>微信公众号</h4><img alt=微信公众号 class=center src=/images/wx-jikerizhi-qrcode.jpg></div><div class="widget-wechat widget"><h4 class=widget__title>知识星球</h4><img alt=微信公众号 class=center src=/images/zhishixingqiu.png></div><div class="widget-recent widget"><h4 class=widget__title>近期文章</h4><div class=widget__content><ul class=widget__list><li class=widget__item><a class=widget__link href=/post/spring-boot-startup-process-overview/>Spring Boot 启动流程概述</a></li><li class=widget__item><a class=widget__link href=/post/redis-core-data-structure-4/>Redis 核心数据结构（四）</a></li><li class=widget__item><a class=widget__link href=/post/redis-core-data-structure-3/>Redis 核心数据结构（三）</a></li><li class=widget__item><a class=widget__link href=/post/algorithm-pattern-subsets/>算法模式：子集</a></li><li class=widget__item><a class=widget__link href=/post/algorithm-pattern-backtracking/>算法模式：回溯</a></li><li class=widget__item><a class=widget__link href=/post/algorithm-pattern-transform-and-conquer/>算法模式：变治法</a></li><li class=widget__item><a class=widget__link href=/post/algorithm-pattern-divide-and-conquer/>算法模式：分治法</a></li><li class=widget__item><a class=widget__link href=/post/algorithm-pattern-decrease-and-conquer/>算法模式：减治法</a></li><li class=widget__item><a class=widget__link href=/post/algorithm-pattern-topological-sort/>算法模式：拓扑排序</a></li><li class=widget__item><a class=widget__link href=/post/algorithm-pattern-union-find/>算法模式：并查集</a></li><li class=widget__item><a class=widget__link href=/post/algorithm-pattern-trie/>算法模式：前缀树</a></li><li class=widget__item><a class=widget__link href=/post/algorithm-pattern-depth-first-search/>算法模式：深度优先搜索</a></li><li class=widget__item><a class=widget__link href=/post/algorithm-pattern-breadth-first-search/>算法模式：广度优先搜索</a></li><li class=widget__item><a class=widget__link href=/post/algorithm-pattern-k-way-merge/>算法模式：多路归并</a></li><li class=widget__item><a class=widget__link href=/post/algorithm-pattern-two-heaps/>算法模式：双堆</a></li><li class=widget__item><a class=widget__link href=/post/algorithm-pattern-cyclic-sort/>算法模式：循环排序</a></li><li class=widget__item><a class=widget__link href=/post/algorithm-pattern-quickselect/>算法模式：快速选择</a></li><li class=widget__item><a class=widget__link href=/post/algorithm-pattern-top-k-elements/>算法模式：Top K 问题</a></li><li class=widget__item><a class=widget__link href=/post/algorithm-pattern-monotonic-stack/>算法模式：单调栈</a></li><li class=widget__item><a class=widget__link href=/post/algorithm-pattern-sliding-window/>算法模式：滑动窗口</a></li></ul></div></div><div class="widget-categories widget"><h4 class=widget__title>分类</h4><div class=widget__content><ul class=widget__list><li class=widget__item><a class=widget__link href=/categories/%E4%B8%AA%E4%BA%BA%E6%88%90%E9%95%BF/>个人成长</a></li><li class=widget__item><a class=widget__link href=/categories/%E5%88%86%E5%B8%83%E5%BC%8F/>分布式</a></li><li class=widget__item><a class=widget__link href=/categories/%E5%BC%80%E5%8F%91%E5%B7%A5%E5%85%B7/>开发工具</a></li><li class=widget__item><a class=widget__link href=/categories/%E6%80%A7%E8%83%BD%E4%BC%98%E5%8C%96/>性能优化</a></li><li class=widget__item><a class=widget__link href=/categories/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/>操作系统</a></li><li class=widget__item><a class=widget__link href=/categories/%E6%95%B0%E6%8D%AE%E5%AD%98%E5%82%A8/>数据存储</a></li><li class=widget__item><a class=widget__link href=/categories/%E6%96%87%E5%AD%A6/>文学</a></li><li class=widget__item><a class=widget__link href=/categories/%E6%96%B9%E6%B3%95%E8%AE%BA/>方法论</a></li><li class=widget__item><a class=widget__link href=/categories/%E7%A8%8B%E5%BA%8F%E8%AE%BE%E8%AE%A1/>程序设计</a></li><li class=widget__item><a class=widget__link href=/categories/%E7%AE%97%E6%B3%95/>算法</a></li><li class=widget__item><a class=widget__link href=/categories/%E7%B3%BB%E7%BB%9F%E6%9E%B6%E6%9E%84/>系统架构</a></li><li class=widget__item><a class=widget__link href=/categories/%E7%BB%8F%E6%B5%8E%E9%87%91%E8%9E%8D/>经济金融</a></li><li class=widget__item><a class=widget__link href=/categories/%E7%BC%96%E7%A8%8B%E8%AF%AD%E8%A8%80/>编程语言</a></li><li class=widget__item><a class=widget__link href=/categories/%E7%BD%91%E7%BB%9C/>网络</a></li><li class=widget__item><a class=widget__link href=/categories/%E8%81%8C%E4%B8%9A%E5%8F%91%E5%B1%95/>职业发展</a></li><li class=widget__item><a class=widget__link href=/categories/%E8%BD%AF%E4%BB%B6%E5%B7%A5%E7%A8%8B/>软件工程</a></li><li class=widget__item><a class=widget__link href=/categories/%E9%80%B8%E9%97%BB%E8%B6%A3%E4%BA%8B/>逸闻趣事</a></li><li class=widget__item><a class=widget__link href=/categories/%E9%98%85%E8%AF%BB%E6%91%98%E8%A6%81/>阅读摘要</a></li></ul></div></div><div class="widget-taglist widget"><h4 class=widget__title>标签</h4><div class=widget__content><a class="widget-taglist__link widget__link btn" href=/tags/gc/ title=GC>GC (9)</a>
<a class="widget-taglist__link widget__link btn" href=/tags/http/ title=HTTP>HTTP (1)</a>
<a class="widget-taglist__link widget__link btn" href=/tags/java/ title=Java>Java (64)</a>
<a class="widget-taglist__link widget__link btn" href=/tags/jvm/ title=JVM>JVM (2)</a>
<a class="widget-taglist__link widget__link btn" href=/tags/kpi/ title=KPI>KPI (4)</a>
<a class="widget-taglist__link widget__link btn" href=/tags/kubernetes/ title=Kubernetes>Kubernetes (10)</a>
<a class="widget-taglist__link widget__link btn" href=/tags/linux/ title=Linux>Linux (12)</a>
<a class="widget-taglist__link widget__link btn" href=/tags/okr/ title=OKR>OKR (5)</a>
<a class="widget-taglist__link widget__link btn" href=/tags/redis/ title=Redis>Redis (2)</a>
<a class="widget-taglist__link widget__link btn" href=/tags/shell/ title=Shell>Shell (1)</a>
<a class="widget-taglist__link widget__link btn" href=/tags/spring/ title=Spring>Spring (26)</a>
<a class="widget-taglist__link widget__link btn" href=/tags/tcp/ title=TCP>TCP (2)</a>
<a class="widget-taglist__link widget__link btn" href=/tags/udp/ title=UDP>UDP (1)</a>
<a class="widget-taglist__link widget__link btn" href=/tags/zookeeper/ title=ZooKeeper>ZooKeeper (2)</a>
<a class="widget-taglist__link widget__link btn" href=/tags/%E4%B8%AA%E4%BA%BA%E6%8F%90%E5%8D%87/ title=个人提升>个人提升 (10)</a>
<a class="widget-taglist__link widget__link btn" href=/tags/%E4%B9%A6%E7%B1%8D/ title=书籍>书籍 (15)</a>
<a class="widget-taglist__link widget__link btn" href=/tags/%E4%BA%A7%E5%93%81/ title=产品>产品 (7)</a>
<a class="widget-taglist__link widget__link btn" href=/tags/%E5%80%BA%E5%88%B8/ title=债券>债券 (1)</a>
<a class="widget-taglist__link widget__link btn" href=/tags/%E5%88%86%E5%B8%83%E5%BC%8F/ title=分布式>分布式 (15)</a>
<a class="widget-taglist__link widget__link btn" href=/tags/%E5%88%86%E6%B2%BB/ title=分治>分治 (1)</a>
<a class="widget-taglist__link widget__link btn" href=/tags/%E5%8A%A8%E6%80%81%E8%A7%84%E5%88%92/ title=动态规划>动态规划 (1)</a>
<a class="widget-taglist__link widget__link btn" href=/tags/%E5%9B%A2%E9%98%9F%E5%BB%BA%E8%AE%BE/ title=团队建设>团队建设 (6)</a>
<a class="widget-taglist__link widget__link btn" href=/tags/%E5%9B%A2%E9%98%9F%E6%96%87%E5%8C%96/ title=团队文化>团队文化 (4)</a>
<a class="widget-taglist__link widget__link btn" href=/tags/%E5%9B%BE/ title=图>图 (7)</a>
<a class="widget-taglist__link widget__link btn" href=/tags/%E5%9F%BA%E9%87%91/ title=基金>基金 (1)</a>
<a class="widget-taglist__link widget__link btn" href=/tags/%E5%A0%86/ title=堆>堆 (3)</a>
<a class="widget-taglist__link widget__link btn" href=/tags/%E5%AD%98%E5%82%A8/ title=存储>存储 (6)</a>
<a class="widget-taglist__link widget__link btn" href=/tags/%E5%B7%A5%E4%BD%9C%E6%96%B9%E6%B3%95/ title=工作方法>工作方法 (5)</a>
<a class="widget-taglist__link widget__link btn" href=/tags/%E5%B9%B6%E5%8F%91/ title=并发>并发 (3)</a>
<a class="widget-taglist__link widget__link btn" href=/tags/%E5%BA%8F%E5%88%97%E5%8C%96/ title=序列化>序列化 (10)</a>
<a class="widget-taglist__link widget__link btn" href=/tags/%E5%BE%AE%E6%9C%8D%E5%8A%A1/ title=微服务>微服务 (17)</a>
<a class="widget-taglist__link widget__link btn" href=/tags/%E5%BF%83%E7%90%86%E5%AD%A6/ title=心理学>心理学 (1)</a>
<a class="widget-taglist__link widget__link btn" href=/tags/%E6%80%A7%E8%83%BD%E6%B5%8B%E8%AF%95/ title=性能测试>性能测试 (4)</a>
<a class="widget-taglist__link widget__link btn" href=/tags/%E6%8A%95%E8%B5%84%E7%90%86%E8%B4%A2/ title=投资理财>投资理财 (5)</a>
<a class="widget-taglist__link widget__link btn" href=/tags/%E6%95%B0%E6%8D%AE%E5%BA%93/ title=数据库>数据库 (10)</a>
<a class="widget-taglist__link widget__link btn" href=/tags/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/ title=数据结构>数据结构 (4)</a>
<a class="widget-taglist__link widget__link btn" href=/tags/%E6%95%B0%E7%BB%84/ title=数组>数组 (13)</a>
<a class="widget-taglist__link widget__link btn" href=/tags/%E6%96%B9%E6%B3%95%E8%AE%BA/ title=方法论>方法论 (13)</a>
<a class="widget-taglist__link widget__link btn" href=/tags/%E6%97%85%E8%A1%8C/ title=旅行>旅行 (1)</a>
<a class="widget-taglist__link widget__link btn" href=/tags/%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5/ title=最佳实践>最佳实践 (10)</a>
<a class="widget-taglist__link widget__link btn" href=/tags/%E6%9E%B6%E6%9E%84/ title=架构>架构 (44)</a>
<a class="widget-taglist__link widget__link btn" href=/tags/%E6%A0%88/ title=栈>栈 (3)</a>
<a class="widget-taglist__link widget__link btn" href=/tags/%E6%A0%91/ title=树>树 (11)</a>
<a class="widget-taglist__link widget__link btn" href=/tags/%E6%B2%9F%E9%80%9A%E6%8A%80%E5%B7%A7/ title=沟通技巧>沟通技巧 (1)</a>
<a class="widget-taglist__link widget__link btn" href=/tags/%E6%BA%90%E7%A0%81%E5%88%86%E6%9E%90/ title=源码分析>源码分析 (8)</a>
<a class="widget-taglist__link widget__link btn" href=/tags/%E7%90%86%E8%B4%A2/ title=理财>理财 (1)</a>
<a class="widget-taglist__link widget__link btn" href=/tags/%E7%94%9F%E6%B4%BB/ title=生活>生活 (1)</a>
<a class="widget-taglist__link widget__link btn" href=/tags/%E7%AC%94%E8%AE%B0/ title=笔记>笔记 (5)</a>
<a class="widget-taglist__link widget__link btn" href=/tags/%E7%AE%97%E6%B3%95/ title=算法>算法 (8)</a>
<a class="widget-taglist__link widget__link btn" href=/tags/%E7%AE%97%E6%B3%95%E6%A8%A1%E5%BC%8F/ title=算法模式>算法模式 (23)</a>
<a class="widget-taglist__link widget__link btn" href=/tags/%E7%BB%8F%E6%B5%8E/ title=经济>经济 (1)</a>
<a class="widget-taglist__link widget__link btn" href=/tags/%E7%BC%96%E7%A0%81/ title=编码>编码 (3)</a>
<a class="widget-taglist__link widget__link btn" href=/tags/%E7%BD%91%E7%BB%9C/ title=网络>网络 (2)</a>
<a class="widget-taglist__link widget__link btn" href=/tags/%E7%BF%BB%E8%AF%91/ title=翻译>翻译 (7)</a>
<a class="widget-taglist__link widget__link btn" href=/tags/%E8%82%A1%E7%A5%A8/ title=股票>股票 (5)</a>
<a class="widget-taglist__link widget__link btn" href=/tags/%E8%84%91%E5%9B%BE/ title=脑图>脑图 (5)</a>
<a class="widget-taglist__link widget__link btn" href=/tags/%E8%8A%82%E6%97%A5/ title=节日>节日 (1)</a>
<a class="widget-taglist__link widget__link btn" href=/tags/%E8%8B%B1%E8%AF%AD/ title=英语>英语 (1)</a>
<a class="widget-taglist__link widget__link btn" href=/tags/%E8%99%9A%E6%8B%9F%E6%9C%BA/ title=虚拟机>虚拟机 (9)</a>
<a class="widget-taglist__link widget__link btn" href=/tags/%E8%AE%BA%E6%96%87/ title=论文>论文 (2)</a>
<a class="widget-taglist__link widget__link btn" href=/tags/%E8%AE%BE%E8%AE%A1/ title=设计>设计 (36)</a>
<a class="widget-taglist__link widget__link btn" href=/tags/%E8%AE%BE%E8%AE%A1%E6%A8%A1%E5%BC%8F/ title=设计模式>设计模式 (3)</a>
<a class="widget-taglist__link widget__link btn" href=/tags/%E8%AF%97%E6%AD%8C/ title=诗歌>诗歌 (2)</a>
<a class="widget-taglist__link widget__link btn" href=/tags/%E8%AF%BB%E4%B9%A6/ title=读书>读书 (5)</a>
<a class="widget-taglist__link widget__link btn" href=/tags/%E9%80%92%E5%BD%92/ title=递归>递归 (1)</a>
<a class="widget-taglist__link widget__link btn" href=/tags/%E9%87%91%E8%9E%8D/ title=金融>金融 (5)</a>
<a class="widget-taglist__link widget__link btn" href=/tags/%E9%93%BE%E8%A1%A8/ title=链表>链表 (5)</a>
<a class="widget-taglist__link widget__link btn" href=/tags/%E9%9D%A2%E5%90%91%E5%AF%B9%E8%B1%A1/ title=面向对象>面向对象 (5)</a>
<a class="widget-taglist__link widget__link btn" href=/tags/%E9%A2%86%E5%9F%9F%E9%A9%B1%E5%8A%A8%E8%AE%BE%E8%AE%A1/ title=领域驱动设计>领域驱动设计 (5)</a></div></div><div class="widget-social widget"><h4 class="widget-social__title widget__title">社交</h4><div class="widget-social__content widget__content"><div class="widget-social__item widget__item"><a class="widget-social__link widget__link btn" title=Twitter rel="noopener noreferrer" href=https://twitter.com/diguage target=_blank><svg class="widget-social__link-icon icon icon-twitter" width="24" height="24" viewBox="0 0 384 312"><path d="m384 36.9c-14.1 6.3-29.3 10.5-45.2 12.4 16.3-9.7 28.8-25.2 34.6-43.6-15.2 9-32.1 15.6-50 19.1-14.4-15.2-34.9-24.8-57.5-24.8-43.5.0-78.8 35.3-78.8 78.8.0 6.2.7 12.2 2 17.9-65.5-3.3-123.5-34.6-162.4-82.3C20 26 16.1 39.6 16.1 54c0 27.3 13.9 51.4 35 65.6-12.9-.4-25.1-4-35.7-9.9v1c0 38.2 27.2 70 63.2 77.2-6.6 1.8-13.6 2.8-20.8 2.8-5.1.0-10-.5-14.8-1.4 10 31.3 39.1 54.1 73.6 54.7-27 21.1-60.9 33.7-97.8 33.7-6.4.0-12.6-.4-18.8-1.1C34.9 299 76.3 312 120.8 312c144.9.0 224.1-120 224.1-224.1.0-3.4-.1-6.8-.2-10.2 15.4-11.1 28.7-25 39.3-40.8z"/></svg>
<span>Twitter</span></a></div><div class="widget-social__item widget__item"><a class="widget-social__link widget__link btn" title=GitHub rel="noopener noreferrer" href=https://github.com/diguage target=_blank><svg class="widget-social__link-icon icon icon-github" width="24" height="24" viewBox="0 0 384 374"><path d="m192 0C85.9.0.0 85.8.0 191.7c0 84.7 55 156.6 131.3 181.9 9.6 1.8 13.1-4.2 13.1-9.2.0-4.6-.2-16.6-.3-32.6-53.4 11.6-64.7-25.7-64.7-25.7-8.7-22.1-21.3-28-21.3-28-17.4-11.9 1.3-11.6 1.3-11.6 19.3 1.4 29.4 19.8 29.4 19.8 17.1 29.3 44.9 20.8 55.9 15.9 1.7-12.4 6.7-20.8 12.2-25.6-42.6-4.8-87.5-21.3-87.5-94.8.0-20.9 7.5-38 19.8-51.4-2-4.9-8.6-24.3 1.9-50.7.0.0 16.1-5.2 52.8 19.7 15.3-4.2 31.7-6.4 48.1-6.5 16.3.1 32.7 2.2 48.1 6.5 36.7-24.8 52.8-19.7 52.8-19.7 10.5 26.4 3.9 45.9 1.9 50.7 12.3 13.4 19.7 30.5 19.7 51.4.0 73.7-44.9 89.9-87.7 94.6 6.9 5.9 13 17.6 13 35.5.0 25.6-.2 46.3-.2 52.6.0 5.1 3.5 11.1 13.2 9.2C329 348.2 384 276.4 384 191.7 384 85.8 298 0 192 0z"/></svg>
<span>GitHub</span></a></div><div class="widget-social__item widget__item"><a class="widget-social__link widget__link btn" title=Email href=mailto:leejun119@gmail.com><svg class="widget-social__link-icon icon icon-mail" width="24" height="24" viewBox="0 0 416 288"><path d="m0 16v256 16h16 384 16v-16V16 0h-16H16 0zm347 16-139 92.5L69 32zM199 157.5l9 5.5 9-5.5L384 46v210H32V46z"/></svg>
<span>leejun119@gmail.com</span></a></div></div></div></aside></div><footer class=footer><div class="container footer__container flex"><div class=footer__copyright>&copy; 2025 "地瓜哥"博客网.
<span class=footer__copyright-credits>基于 <a href=https://gohugo.io/ rel="nofollow noopener" target=_blank>Hugo</a> 引擎和 <a href=https://github.com/Vimux/Mainroad/ rel="nofollow noopener" target=_blank>Mainroad</a> 主题</span>
<span><a href=https://beian.miit.gov.cn/ target=_target>京ICP备14046450号-4</a></span></div></div></footer></div><script async defer src=/js/menu.js></script><script type=text/x-mathjax-config>
	MathJax.Hub.Config({
	  messageStyle: "none",
	  tex2jax: {
		inlineMath: [["\\(", "\\)"]],
		displayMath: [["\\[", "\\]"]],
		ignoreClass: "nostem|nolatexmath"
	  },
	  asciimath2jax: {
		delimiters: [["\\$", "\\$"]],
		ignoreClass: "nostem|noasciimath"
	  },
	  TeX: { equationNumbers: { autoNumber: "none" } }
	})
	MathJax.Hub.Register.StartupHook("AsciiMath Jax Ready", function () {
	  MathJax.InputJax.AsciiMath.postfilterHooks.Add(function (data, node) {
		if ((node = data.script.parentNode) && (node = node.parentNode) && node.classList.contains("stemblock")) {
		  data.math.root.display = "block"
		}
		return data
	  })
	})
</script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/MathJax.js?config=TeX-MML-AM_HTMLorMML" async></script></body></html>